From 53c9961d5b350c96200a3b85c2302a2b569e6fa8 Mon Sep 17 00:00:00 2001
From: Todd Lipcon <todd@cloudera.com>
Date: Wed, 2 Feb 2011 16:59:52 -0800
Subject: [PATCH 0846/1020] HDFS-1298. Add support in HDFS for new statistics added in FileSystem to track the file system operations.

Author: Suresh Srinivas
Ref: CDH-2622
---
 .../apache/hadoop/hdfs/DistributedFileSystem.java  |   20 ++++-
 src/test/org/apache/hadoop/hdfs/DFSTestUtil.java   |    5 +
 .../hadoop/hdfs/TestDistributedFileSystem.java     |  100 +++++++++++++++++++-
 3 files changed, 123 insertions(+), 2 deletions(-)

diff --git a/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java b/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java
index 8c9c323..d948eb4 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/DistributedFileSystem.java
@@ -174,6 +174,7 @@ public class DistributedFileSystem extends FileSystem {
     if (file == null) {
       return null;
     }
+    statistics.incrementReadOps(1);
     return dfs.getBlockLocations(getPathName(file.getPath()), start, len);
   }
 
@@ -182,6 +183,7 @@ public class DistributedFileSystem extends FileSystem {
   }
 
   public FSDataInputStream open(Path f, int bufferSize) throws IOException {
+    statistics.incrementReadOps(1);
     return new DFSClient.DFSDataInputStream(
           dfs.open(getPathName(f), bufferSize, verifyChecksum, statistics));
   }
@@ -190,6 +192,7 @@ public class DistributedFileSystem extends FileSystem {
   public FSDataOutputStream append(Path f, int bufferSize,
       Progressable progress) throws IOException {
 
+    statistics.incrementWriteOps(1);
     DFSOutputStream op = (DFSOutputStream)dfs.append(getPathName(f), bufferSize, progress);
     return new FSDataOutputStream(op, statistics, op.getInitialLen());
   }
@@ -199,6 +202,7 @@ public class DistributedFileSystem extends FileSystem {
     int bufferSize, short replication, long blockSize,
     Progressable progress) throws IOException {
 
+    statistics.incrementWriteOps(1);
     return new FSDataOutputStream
        (dfs.create(getPathName(f), permission,
                    overwrite, replication, blockSize, progress, bufferSize),
@@ -208,6 +212,7 @@ public class DistributedFileSystem extends FileSystem {
   public boolean setReplication(Path src, 
                                 short replication
                                ) throws IOException {
+    statistics.incrementWriteOps(1);
     return dfs.setReplication(getPathName(src), replication);
   }
 
@@ -215,6 +220,7 @@ public class DistributedFileSystem extends FileSystem {
    * Rename files/dirs
    */
   public boolean rename(Path src, Path dst) throws IOException {
+    statistics.incrementWriteOps(1);
     return dfs.rename(getPathName(src), getPathName(dst));
   }
 
@@ -223,6 +229,7 @@ public class DistributedFileSystem extends FileSystem {
    */
   @Deprecated
   public boolean delete(Path f) throws IOException {
+    statistics.incrementWriteOps(1);
     return dfs.delete(getPathName(f));
   }
   
@@ -231,11 +238,13 @@ public class DistributedFileSystem extends FileSystem {
    * empty directory recursively.
    */
   public boolean delete(Path f, boolean recursive) throws IOException {
-   return dfs.delete(getPathName(f), recursive);
+    statistics.incrementWriteOps(1);
+    return dfs.delete(getPathName(f), recursive);
   }
   
   /** {@inheritDoc} */
   public ContentSummary getContentSummary(Path f) throws IOException {
+    statistics.incrementReadOps(1);
     return dfs.getContentSummary(getPathName(f));
   }
 
@@ -281,6 +290,7 @@ public class DistributedFileSystem extends FileSystem {
       for (int i = 0; i < partialListing.length; i++) {
         stats[i] = makeQualified(partialListing[i], p);
       }
+      statistics.incrementReadOps(1);
       return stats;
     }
     
@@ -294,6 +304,7 @@ public class DistributedFileSystem extends FileSystem {
     for (HdfsFileStatus fileStatus : partialListing) {
       listing.add(makeQualified(fileStatus, p));
     }
+    statistics.incrementLargeReadOps(1);
 
     // now fetch more entries
     do {
@@ -307,12 +318,14 @@ public class DistributedFileSystem extends FileSystem {
       for (HdfsFileStatus fileStatus : partialListing) {
         listing.add(makeQualified(fileStatus, p));
       }
+      statistics.incrementLargeReadOps(1);
     } while (thisListing.hasMore());
 
     return listing.toArray(new FileStatus[listing.size()]);
   }
 
   public boolean mkdirs(Path f, FsPermission permission) throws IOException {
+    statistics.incrementWriteOps(1);
     return dfs.mkdirs(getPathName(f), permission);
   }
 
@@ -512,6 +525,7 @@ public class DistributedFileSystem extends FileSystem {
    * @throws FileNotFoundException if the file does not exist.
    */
   public FileStatus getFileStatus(Path f) throws IOException {
+    statistics.incrementReadOps(1);
     HdfsFileStatus fi = dfs.getFileInfo(getPathName(f));
     if (fi != null) {
       return makeQualified(fi, f);
@@ -522,12 +536,14 @@ public class DistributedFileSystem extends FileSystem {
 
   /** {@inheritDoc} */
   public MD5MD5CRC32FileChecksum getFileChecksum(Path f) throws IOException {
+    statistics.incrementReadOps(1);
     return dfs.getFileChecksum(getPathName(f));
   }
 
   /** {@inheritDoc }*/
   public void setPermission(Path p, FsPermission permission
       ) throws IOException {
+    statistics.incrementWriteOps(1);
     dfs.setPermission(getPathName(p), permission);
   }
 
@@ -537,12 +553,14 @@ public class DistributedFileSystem extends FileSystem {
     if (username == null && groupname == null) {
       throw new IOException("username == null && groupname == null");
     }
+    statistics.incrementWriteOps(1);
     dfs.setOwner(getPathName(p), username, groupname);
   }
 
   /** {@inheritDoc }*/
   public void setTimes(Path p, long mtime, long atime
       ) throws IOException {
+    statistics.incrementWriteOps(1);
     dfs.setTimes(getPathName(p), mtime, atime);
   }
 
diff --git a/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java b/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
index 1841980..886022a 100644
--- a/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
+++ b/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
@@ -40,6 +40,7 @@ import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem.Statistics;
 import org.apache.hadoop.fs.BlockLocation;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;
@@ -367,4 +368,8 @@ public class DFSTestUtil {
                     }
                   });
   }  
+
+  public static Statistics getStatistics(FileSystem fs) {
+    return FileSystem.getStatistics(fs.getUri().getScheme(), fs.getClass());
+  }
 }
diff --git a/src/test/org/apache/hadoop/hdfs/TestDistributedFileSystem.java b/src/test/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
index 3a26730..0328ede 100644
--- a/src/test/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
+++ b/src/test/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
@@ -32,15 +32,18 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileChecksum;
+import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.log4j.Level;
+import org.junit.Test;
 
-public class TestDistributedFileSystem extends junit.framework.TestCase {
+public class TestDistributedFileSystem {
   private static final Random RAN = new Random();
 
+  @Test
   public void testFileSystemCloseAll() throws Exception {
     Configuration conf = new Configuration();
     MiniDFSCluster cluster = new MiniDFSCluster(conf, 0, true, null);
@@ -64,6 +67,7 @@ public class TestDistributedFileSystem extends junit.framework.TestCase {
    * Tests DFSClient.close throws no ConcurrentModificationException if 
    * multiple files are open.
    */
+  @Test
   public void testDFSClose() throws Exception {
     Configuration conf = new Configuration();
     MiniDFSCluster cluster = new MiniDFSCluster(conf, 2, true, null);
@@ -81,6 +85,7 @@ public class TestDistributedFileSystem extends junit.framework.TestCase {
     }
   }
 
+  @Test
   public void testDFSClient() throws Exception {
     Configuration conf = new Configuration();
     MiniDFSCluster cluster = null;
@@ -125,6 +130,99 @@ public class TestDistributedFileSystem extends junit.framework.TestCase {
     }
   }
   
+  @Test
+  public void testStatistics() throws Exception {
+    int lsLimit = 2;
+    final Configuration conf = getTestConfiguration();
+    conf.setInt(DFSConfigKeys.DFS_LIST_LIMIT, lsLimit);
+    final MiniDFSCluster cluster = new MiniDFSCluster(conf, 1, true, null);
+    try {
+      final FileSystem fs = cluster.getFileSystem();
+      Path dir = new Path("/test");
+      Path file = new Path(dir, "file");
+      
+      int readOps = DFSTestUtil.getStatistics(fs).getReadOps();
+      int writeOps = DFSTestUtil.getStatistics(fs).getWriteOps();
+      int largeReadOps = DFSTestUtil.getStatistics(fs).getLargeReadOps();
+      fs.mkdirs(dir);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      FSDataOutputStream out = fs.create(file, (short)1);
+      out.close();
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      FileStatus status = fs.getFileStatus(file);
+      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
+      
+      fs.getFileBlockLocations(status, 0, 0);
+      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
+      
+      FSDataInputStream in = fs.open(file);
+      in.close();
+      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
+      
+      fs.setReplication(file, (short)2);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      Path file1 = new Path(dir, "file1");
+      fs.rename(file, file1);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      fs.getContentSummary(file1);
+      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
+      
+      
+      // Iterative ls test
+      for (int i = 0; i < 10; i++) {
+        Path p = new Path(dir, Integer.toString(i));
+        fs.mkdirs(p);
+        FileStatus[] list = fs.listStatus(dir);
+        if (list.length > lsLimit) {
+          // if large directory, then count readOps and largeReadOps by 
+          // number times listStatus iterates
+          int iterations = (int)Math.ceil((double)list.length/lsLimit);
+          largeReadOps += iterations;
+          readOps += iterations;
+        } else {
+          // Single iteration in listStatus - no large read operation done
+          readOps++;
+        }
+        
+        // writeOps incremented by 1 for mkdirs
+        // readOps and largeReadOps incremented by 1 or more
+        checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      }
+      
+      fs.getFileChecksum(file1);
+      checkStatistics(fs, ++readOps, writeOps, largeReadOps);
+      
+      fs.setPermission(file1, new FsPermission((short)0777));
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      fs.setTimes(file1, 0L, 0L);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
+      fs.setOwner(file1, ugi.getUserName(), ugi.getGroupNames()[0]);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+      fs.delete(dir, true);
+      checkStatistics(fs, readOps, ++writeOps, largeReadOps);
+      
+    } finally {
+      if (cluster != null) cluster.shutdown();
+    }
+    
+  }
+  
+  /** Checks statistics. -1 indicates do not check for the operations */
+  private void checkStatistics(FileSystem fs, int readOps, int writeOps, int largeReadOps) {
+    assertEquals(readOps, DFSTestUtil.getStatistics(fs).getReadOps());
+    assertEquals(writeOps, DFSTestUtil.getStatistics(fs).getWriteOps());
+    assertEquals(largeReadOps, DFSTestUtil.getStatistics(fs).getLargeReadOps());
+  }
+
+  @Test
   public void testFileChecksum() throws Exception {
     ((Log4JLogger)HftpFileSystem.LOG).getLogger().setLevel(Level.ALL);
 
-- 
1.7.0.4

