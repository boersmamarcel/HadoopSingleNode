From 1756e97a35451bbc01a493e843f1ec0885c99792 Mon Sep 17 00:00:00 2001
From: Aaron Kimball <aaron@cloudera.com>
Date: Fri, 18 Jun 2010 11:37:22 -0700
Subject: [PATCH 0265/1020] MAPREDUCE-1644. Remove Sqoop from Apache Hadoop (moving to github)

Description: Sqoop is moving to github! All code for sqoop is already live at
http://github.com/cloudera/sqoop - this issue removes the duplicate code from the Apache Hadoop
repository. CDH users should install the separate 'sqoop' package for this functionality.
Reason: Moving to a separate package
Author: Aaron Kimball
Ref: CDH-1404
---
 bin/sqoop                                          |   19 -
 src/contrib/build.xml                              |    1 -
 src/contrib/sqoop/build.xml                        |  162 ---
 src/contrib/sqoop/conf/sqoop-default.xml           |   33 -
 src/contrib/sqoop/doc/.gitignore                   |    3 -
 src/contrib/sqoop/doc/Makefile                     |   43 -
 src/contrib/sqoop/doc/Sqoop-manpage.txt            |  219 -----
 src/contrib/sqoop/doc/SqoopUserGuide.txt           |   67 --
 src/contrib/sqoop/doc/api-reference.txt            |  243 -----
 src/contrib/sqoop/doc/classnames.txt               |   43 -
 src/contrib/sqoop/doc/connecting.txt               |   85 --
 src/contrib/sqoop/doc/controlling-input-format.txt |   42 -
 .../sqoop/doc/controlling-output-format.txt        |   38 -
 src/contrib/sqoop/doc/direct.txt                   |   77 --
 src/contrib/sqoop/doc/export.txt                   |   58 --
 src/contrib/sqoop/doc/full-db-import.txt           |   92 --
 src/contrib/sqoop/doc/hive.txt                     |   66 --
 src/contrib/sqoop/doc/input-formatting-args.txt    |   34 -
 src/contrib/sqoop/doc/input-formatting.txt         |   24 -
 src/contrib/sqoop/doc/intro.txt                    |   34 -
 src/contrib/sqoop/doc/listing-dbs.txt              |   35 -
 src/contrib/sqoop/doc/listing-tables.txt           |   34 -
 src/contrib/sqoop/doc/misc-args.txt                |   40 -
 src/contrib/sqoop/doc/output-formatting-args.txt   |   39 -
 src/contrib/sqoop/doc/output-formatting.txt        |   44 -
 src/contrib/sqoop/doc/supported-dbs.txt            |   55 --
 src/contrib/sqoop/doc/table-import.txt             |   68 --
 src/contrib/sqoop/ivy.xml                          |   88 --
 src/contrib/sqoop/ivy/libraries.properties         |   21 -
 src/contrib/sqoop/readme.txt                       |   15 -
 .../java/org/apache/hadoop/sqoop/ConnFactory.java  |  104 --
 .../src/java/org/apache/hadoop/sqoop/Sqoop.java    |  274 ------
 .../java/org/apache/hadoop/sqoop/SqoopOptions.java | 1024 --------------------
 .../org/apache/hadoop/sqoop/hive/HiveImport.java   |  194 ----
 .../org/apache/hadoop/sqoop/hive/HiveTypes.java    |   95 --
 .../apache/hadoop/sqoop/hive/TableDefWriter.java   |  221 -----
 .../hadoop/sqoop/io/CountingOutputStream.java      |   74 --
 .../hadoop/sqoop/io/HdfsSplitOutputStream.java     |  154 ---
 .../hadoop/sqoop/io/SplittableBufferedWriter.java  |   75 --
 .../hadoop/sqoop/io/SplittingOutputStream.java     |  161 ---
 .../hadoop/sqoop/lib/BigDecimalSerializer.java     |   84 --
 .../apache/hadoop/sqoop/lib/FieldFormatter.java    |  100 --
 .../hadoop/sqoop/lib/JdbcWritableBridge.java       |  203 ----
 .../org/apache/hadoop/sqoop/lib/RecordParser.java  |  353 -------
 .../org/apache/hadoop/sqoop/lib/SqoopRecord.java   |   39 -
 .../apache/hadoop/sqoop/manager/ConnManager.java   |  148 ---
 .../sqoop/manager/DefaultManagerFactory.java       |   99 --
 .../sqoop/manager/DirectPostgresqlManager.java     |  459 ---------
 .../hadoop/sqoop/manager/ExportJobContext.java     |   56 --
 .../hadoop/sqoop/manager/GenericJdbcManager.java   |   72 --
 .../apache/hadoop/sqoop/manager/HsqldbManager.java |   53 -
 .../hadoop/sqoop/manager/ImportJobContext.java     |   56 --
 .../hadoop/sqoop/manager/LocalMySQLManager.java    |  532 ----------
 .../hadoop/sqoop/manager/ManagerFactory.java       |   33 -
 .../apache/hadoop/sqoop/manager/MySQLManager.java  |  229 -----
 .../apache/hadoop/sqoop/manager/OracleManager.java |  272 ------
 .../hadoop/sqoop/manager/PostgresqlManager.java    |  125 ---
 .../apache/hadoop/sqoop/manager/SqlManager.java    |  449 ---------
 .../hadoop/sqoop/mapred/AutoProgressMapRunner.java |  200 ----
 .../org/apache/hadoop/sqoop/mapred/ImportJob.java  |  169 ----
 .../sqoop/mapred/RawKeyTextOutputFormat.java       |  105 --
 .../hadoop/sqoop/mapred/TextImportMapper.java      |   52 -
 .../hadoop/sqoop/mapreduce/AutoProgressMapper.java |  186 ----
 .../sqoop/mapreduce/DataDrivenImportJob.java       |  219 -----
 .../apache/hadoop/sqoop/mapreduce/ExportJob.java   |  218 -----
 .../sqoop/mapreduce/RawKeyTextOutputFormat.java    |  102 --
 .../sqoop/mapreduce/SequenceFileExportMapper.java  |   43 -
 .../hadoop/sqoop/mapreduce/TextExportMapper.java   |   82 --
 .../hadoop/sqoop/mapreduce/TextImportMapper.java   |   46 -
 .../org/apache/hadoop/sqoop/orm/ClassWriter.java   |  854 ----------------
 .../hadoop/sqoop/orm/CompilationManager.java       |  385 --------
 .../apache/hadoop/sqoop/orm/TableClassName.java    |  113 ---
 .../org/apache/hadoop/sqoop/util/AsyncSink.java    |   44 -
 .../apache/hadoop/sqoop/util/ClassLoaderStack.java |   84 --
 .../hadoop/sqoop/util/DirectImportUtils.java       |  111 ---
 .../hadoop/sqoop/util/ErrorableAsyncSink.java      |   40 -
 .../apache/hadoop/sqoop/util/ErrorableThread.java  |   40 -
 .../org/apache/hadoop/sqoop/util/Executor.java     |  117 ---
 .../apache/hadoop/sqoop/util/ExportException.java  |   48 -
 .../org/apache/hadoop/sqoop/util/FileListing.java  |  128 ---
 .../apache/hadoop/sqoop/util/ImportException.java  |   50 -
 .../java/org/apache/hadoop/sqoop/util/JdbcUrl.java |  121 ---
 .../apache/hadoop/sqoop/util/LoggingAsyncSink.java |   96 --
 .../apache/hadoop/sqoop/util/NullAsyncSink.java    |   83 --
 .../org/apache/hadoop/sqoop/util/PerfCounters.java |  131 ---
 .../apache/hadoop/sqoop/util/ResultSetPrinter.java |  152 ---
 .../src/test/org/apache/hadoop/sqoop/AllTests.java |   44 -
 .../test/org/apache/hadoop/sqoop/SmokeTests.java   |   70 --
 .../org/apache/hadoop/sqoop/TestAllTables.java     |  136 ---
 .../org/apache/hadoop/sqoop/TestColumnTypes.java   |  302 ------
 .../org/apache/hadoop/sqoop/TestConnFactory.java   |  156 ---
 .../test/org/apache/hadoop/sqoop/TestExport.java   |  539 ----------
 .../org/apache/hadoop/sqoop/TestMultiCols.java     |  212 ----
 .../org/apache/hadoop/sqoop/TestMultiMaps.java     |  173 ----
 .../test/org/apache/hadoop/sqoop/TestSplitBy.java  |  150 ---
 .../org/apache/hadoop/sqoop/TestSqoopOptions.java  |  228 -----
 .../test/org/apache/hadoop/sqoop/TestWhere.java    |  166 ----
 .../org/apache/hadoop/sqoop/ThirdPartyTests.java   |   51 -
 .../apache/hadoop/sqoop/hive/TestHiveImport.java   |  172 ----
 .../hadoop/sqoop/hive/TestTableDefWriter.java      |   80 --
 .../sqoop/io/TestSplittableBufferedWriter.java     |  257 -----
 .../hadoop/sqoop/lib/TestFieldFormatter.java       |  147 ---
 .../apache/hadoop/sqoop/lib/TestRecordParser.java  |  356 -------
 .../hadoop/sqoop/manager/LocalMySQLTest.java       |  405 --------
 .../apache/hadoop/sqoop/manager/MySQLAuthTest.java |  323 ------
 .../hadoop/sqoop/manager/OracleManagerTest.java    |  295 ------
 .../hadoop/sqoop/manager/PostgresqlTest.java       |  239 -----
 .../hadoop/sqoop/manager/TestHsqldbManager.java    |   83 --
 .../hadoop/sqoop/manager/TestSqlManager.java       |  238 -----
 .../apache/hadoop/sqoop/mapred/MapredTests.java    |   37 -
 .../sqoop/mapred/TestAutoProgressMapRunner.java    |  193 ----
 .../hadoop/sqoop/mapreduce/MapreduceTests.java     |   38 -
 .../hadoop/sqoop/mapreduce/TestImportJob.java      |  117 ---
 .../sqoop/mapreduce/TestTextImportMapper.java      |   79 --
 .../apache/hadoop/sqoop/orm/TestClassWriter.java   |  318 ------
 .../apache/hadoop/sqoop/orm/TestParseMethods.java  |  183 ----
 .../hadoop/sqoop/testutil/BaseSqoopTestCase.java   |  286 ------
 .../apache/hadoop/sqoop/testutil/CommonArgs.java   |   45 -
 .../org/apache/hadoop/sqoop/testutil/DirUtil.java  |   58 --
 .../hadoop/sqoop/testutil/ExportJobTestCase.java   |  196 ----
 .../hadoop/sqoop/testutil/HsqldbTestServer.java    |  239 -----
 .../hadoop/sqoop/testutil/ImportJobTestCase.java   |  170 ----
 .../hadoop/sqoop/testutil/ReparseMapper.java       |  107 --
 .../hadoop/sqoop/testutil/SeqFileReader.java       |   78 --
 src/contrib/sqoop/testdata/hive/bin/hive           |   59 --
 .../sqoop/testdata/hive/scripts/createOnlyImport.q |    1 -
 .../testdata/hive/scripts/createOverwriteImport.q  |    1 -
 .../testdata/hive/scripts/customDelimImport.q      |    2 -
 .../sqoop/testdata/hive/scripts/dateImport.q       |    2 -
 .../sqoop/testdata/hive/scripts/failingImport.q    |    2 -
 .../sqoop/testdata/hive/scripts/normalImport.q     |    2 -
 .../sqoop/testdata/hive/scripts/numericImport.q    |    2 -
 132 files changed, 0 insertions(+), 18616 deletions(-)
 delete mode 100755 bin/sqoop
 delete mode 100644 src/contrib/sqoop/build.xml
 delete mode 100644 src/contrib/sqoop/conf/sqoop-default.xml
 delete mode 100644 src/contrib/sqoop/doc/.gitignore
 delete mode 100644 src/contrib/sqoop/doc/Makefile
 delete mode 100644 src/contrib/sqoop/doc/Sqoop-manpage.txt
 delete mode 100644 src/contrib/sqoop/doc/SqoopUserGuide.txt
 delete mode 100644 src/contrib/sqoop/doc/api-reference.txt
 delete mode 100644 src/contrib/sqoop/doc/classnames.txt
 delete mode 100644 src/contrib/sqoop/doc/connecting.txt
 delete mode 100644 src/contrib/sqoop/doc/controlling-input-format.txt
 delete mode 100644 src/contrib/sqoop/doc/controlling-output-format.txt
 delete mode 100644 src/contrib/sqoop/doc/direct.txt
 delete mode 100644 src/contrib/sqoop/doc/export.txt
 delete mode 100644 src/contrib/sqoop/doc/full-db-import.txt
 delete mode 100644 src/contrib/sqoop/doc/hive.txt
 delete mode 100644 src/contrib/sqoop/doc/input-formatting-args.txt
 delete mode 100644 src/contrib/sqoop/doc/input-formatting.txt
 delete mode 100644 src/contrib/sqoop/doc/intro.txt
 delete mode 100644 src/contrib/sqoop/doc/listing-dbs.txt
 delete mode 100644 src/contrib/sqoop/doc/listing-tables.txt
 delete mode 100644 src/contrib/sqoop/doc/misc-args.txt
 delete mode 100644 src/contrib/sqoop/doc/output-formatting-args.txt
 delete mode 100644 src/contrib/sqoop/doc/output-formatting.txt
 delete mode 100644 src/contrib/sqoop/doc/supported-dbs.txt
 delete mode 100644 src/contrib/sqoop/doc/table-import.txt
 delete mode 100644 src/contrib/sqoop/ivy.xml
 delete mode 100644 src/contrib/sqoop/ivy/libraries.properties
 delete mode 100644 src/contrib/sqoop/readme.txt
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/ConnFactory.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/Sqoop.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/SqoopOptions.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveImport.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveTypes.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/TableDefWriter.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/CountingOutputStream.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/HdfsSplitOutputStream.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittableBufferedWriter.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittingOutputStream.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/BigDecimalSerializer.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/JdbcWritableBridge.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/RecordParser.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/SqoopRecord.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ConnManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DefaultManagerFactory.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DirectPostgresqlManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ExportJobContext.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/GenericJdbcManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/HsqldbManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ImportJobContext.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/LocalMySQLManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ManagerFactory.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/MySQLManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/OracleManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/PostgresqlManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/SqlManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/AutoProgressMapRunner.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/ImportJob.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/RawKeyTextOutputFormat.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/TextImportMapper.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/AutoProgressMapper.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/DataDrivenImportJob.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/ExportJob.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/RawKeyTextOutputFormat.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/SequenceFileExportMapper.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextExportMapper.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextImportMapper.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/ClassWriter.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/CompilationManager.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/TableClassName.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/AsyncSink.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ClassLoaderStack.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/DirectImportUtils.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableAsyncSink.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableThread.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/Executor.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ExportException.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/FileListing.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportException.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/JdbcUrl.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/LoggingAsyncSink.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/NullAsyncSink.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/PerfCounters.java
 delete mode 100644 src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ResultSetPrinter.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/AllTests.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/SmokeTests.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestColumnTypes.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestConnFactory.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestExport.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiCols.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiMaps.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSplitBy.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSqoopOptions.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestWhere.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/ThirdPartyTests.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestHiveImport.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestTableDefWriter.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/io/TestSplittableBufferedWriter.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestRecordParser.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/LocalMySQLTest.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/MySQLAuthTest.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/OracleManagerTest.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/PostgresqlTest.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestHsqldbManager.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestSqlManager.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/MapredTests.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/TestAutoProgressMapRunner.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/MapreduceTests.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestImportJob.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestTextImportMapper.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestClassWriter.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestParseMethods.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/BaseSqoopTestCase.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/CommonArgs.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/DirUtil.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ExportJobTestCase.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/HsqldbTestServer.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ImportJobTestCase.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ReparseMapper.java
 delete mode 100644 src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/SeqFileReader.java
 delete mode 100755 src/contrib/sqoop/testdata/hive/bin/hive
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/createOnlyImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/createOverwriteImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/customDelimImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/dateImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/failingImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/normalImport.q
 delete mode 100644 src/contrib/sqoop/testdata/hive/scripts/numericImport.q

diff --git a/bin/sqoop b/bin/sqoop
deleted file mode 100755
index df7cb49..0000000
--- a/bin/sqoop
+++ /dev/null
@@ -1,19 +0,0 @@
-#!/bin/sh
-
-bin=`dirname $0`
-bin=`cd ${bin} && pwd`
-
-. "$bin"/hadoop-config.sh
-SQOOP_JAR_PATH="$HADOOP_HOME/contrib/sqoop"
-
-# Add sqoop dependencies to classpath.
-SQOOP_CLASSPATH=""
-if [ -d "$SQOOP_JAR_PATH/lib" ]; then
-  for f in $SQOOP_JAR_PATH/lib/*.jar; do
-    SQOOP_CLASSPATH=${SQOOP_CLASSPATH}:$f;
-  done
-fi
-
-SQOOP_JAR=`ls -1 ${SQOOP_JAR_PATH}/hadoop-*-sqoop.jar | head -n 1`
-HADOOP_CLASSPATH="${SQOOP_CLASSPATH}:${HADOOP_CLASSPATH}" \
-    ${bin}/hadoop jar ${SQOOP_JAR} org.apache.hadoop.sqoop.Sqoop "$@"
diff --git a/src/contrib/build.xml b/src/contrib/build.xml
index 81d189c..3455900 100644
--- a/src/contrib/build.xml
+++ b/src/contrib/build.xml
@@ -51,7 +51,6 @@
       <fileset dir="." includes="fairscheduler/build.xml"/>
       <fileset dir="." includes="capacity-scheduler/build.xml"/>
       <fileset dir="." includes="mrunit/build.xml"/>
-      <fileset dir="." includes="sqoop/build.xml"/>
     </subant>
   </target>
   
diff --git a/src/contrib/sqoop/build.xml b/src/contrib/sqoop/build.xml
deleted file mode 100644
index d552dad..0000000
--- a/src/contrib/sqoop/build.xml
+++ /dev/null
@@ -1,162 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<!--
-Before you can run these subtargets directly, you need
-to call at top-level: ant deploy-contrib compile-core-test
--->
-<project name="sqoop" default="jar">
-
-  <import file="../build-contrib.xml"/>
-  <property environment="env"/>
-  <property name="sqoop.thirdparty.lib.dir" value="" />
-  <property name="mrunit.class.dir" value="${build.dir}/../mrunit/classes" />
-
-  <!-- ================================================================== -->
-  <!-- Compile test code                                                  -->
-  <!-- Override with our own version so we can enforce build dependencies -->
-  <!-- on compile-core-test for MiniMRCluster, and MRUnit.                -->
-  <!-- ================================================================== -->
-  <target name="compile-test" depends="compile-examples, ivy-retrieve-test"
-      if="test.available">
-    <echo message="Compiling ${name} dependencies" />
-    <!-- need top-level compile-core-test for MiniMRCluster -->
-    <subant target="compile-core-test">
-      <fileset dir="../../.." includes="build.xml" />
-    </subant>
-
-    <!-- Need MRUnit compiled for some tests -->
-    <subant target="compile">
-      <fileset dir="../mrunit" includes="build.xml" />
-    </subant>
-
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.test}"
-     includes="**/*.java"
-     destdir="${build.test}"
-     debug="${javac.debug}">
-    <classpath>
-      <path refid="test.classpath"/>
-      <pathelement path="${mrunit.class.dir}" />
-    </classpath>
-    </javac>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- Run unit tests                                                     -->
-  <!-- Override with our own version so we can set hadoop.alt.classpath   -->
-  <!-- and Hadoop logger properties                                       -->
-  <!-- ================================================================== -->
-  <target name="test" depends="compile-test, compile" if="test.available">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${hadoop.log.dir}"/>
-    <mkdir dir="${hadoop.log.dir}"/>
-    <delete dir="${build.test}/data"/>
-    <mkdir dir="${build.test}/data" />
-    <junit
-      printsummary="yes" showoutput="${test.output}"
-      haltonfailure="no" fork="yes" maxmemory="256m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}"
-      dir="${build.test}/data">
-
-      <!-- uncomment this if you want to attach a debugger -->
-      <!--
-      <jvmarg line="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=2601" />
-      -->
-
-      <sysproperty key="test.build.data" value="${build.test}/data"/>
-      <sysproperty key="build.test" value="${build.test}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-
-
-      <!-- define this property to force Sqoop to throw better exceptions on errors
-           during testing, instead of printing a short message and exiting with status 1. -->
-      <sysproperty key="sqoop.throwOnError" value="" />
-
-      <!--
-           Added property needed to use the .class files for compilation
-           instead of depending on hadoop-*-core.jar
-      -->
-      <sysproperty key="hadoop.alt.classpath"
-        value="${hadoop.root}/build/classes" />
-
-      <!-- we want more log4j output when running unit tests -->
-      <sysproperty key="hadoop.root.logger"
-        value="DEBUG,console" />
-
-      <!-- requires fork=yes for:
-        relative File paths to use the specified user.dir
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="user.dir" value="${build.test}/data"/>
-
-      <!-- Setting the user.dir property is actually meaningless as it
-          is read-only in the Linux Sun JDK. Provide an alternate sysprop
-          to specify where generated code should go.
-      -->
-      <sysproperty key="sqoop.src.dir" value="${build.test}/data"/>
-
-      <!-- Override standalone Hadoop's working dirs to allow parallel
-           execution of multiple Hudson builders
-      -->
-      <sysproperty key="hadoop.tmp.dir" value="${build.test}/hadoop"/>
-
-      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-      <sysproperty key="hadoop.test.localoutputfile" value="${hadoop.test.localoutputfile}"/>
-      <sysproperty key="hadoop.log.dir" value="${hadoop.log.dir}"/>
-
-      <!-- we have a mock "hive" shell instance in our testdata directory
-           for testing hive integration. Set this property here to ensure
-           that the unit tests pick it up.
-      -->
-      <sysproperty key="hive.home" value="${basedir}/testdata/hive" />
-
-      <classpath>
-        <path refid="test.classpath"/>
-        <path refid="contrib-classpath"/>
-        <!-- need thirdparty JDBC drivers for thirdparty tests -->
-        <fileset dir="${sqoop.thirdparty.lib.dir}"
-            includes="*.jar" />
-        <!-- need MRUnit for some tests -->
-        <pathelement path="${mrunit.class.dir}" />
-      </classpath>
-      <formatter type="${test.junit.output.format}" />
-      <batchtest todir="${build.test}" unless="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/Test*.java" excludes="**/${test.exclude}.java" />
-      </batchtest>
-      <batchtest todir="${build.test}" if="testcase">
-        <fileset dir="${src.test}" includes="**/${testcase}.java"/>
-      </batchtest>
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-  <target name="doc">
-    <exec executable="make" failonerror="true">
-      <arg value="-C" />
-      <arg value="${basedir}/doc" />
-      <arg value="BUILDROOT=${build.dir}" />
-    </exec>
-  </target>
-
-</project>
diff --git a/src/contrib/sqoop/conf/sqoop-default.xml b/src/contrib/sqoop/conf/sqoop-default.xml
deleted file mode 100644
index 07906dc..0000000
--- a/src/contrib/sqoop/conf/sqoop-default.xml
+++ /dev/null
@@ -1,33 +0,0 @@
-<?xml version="1.0"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<!-- Put Sqoop-specific properties in this file. -->
-
-<configuration>
-
-  <property>
-    <name>sqoop.connection.factories</name>
-    <value>org.apache.hadoop.sqoop.manager.DefaultManagerFactory</value>
-    <description>A comma-delimited list of ManagerFactory implementations
-      which are consulted, in order, to instantiate ConnManager instances
-      used to drive connections to databases.
-    </description>
-  </property>
-
-</configuration>
diff --git a/src/contrib/sqoop/doc/.gitignore b/src/contrib/sqoop/doc/.gitignore
deleted file mode 100644
index 1a9a2df..0000000
--- a/src/contrib/sqoop/doc/.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-/Sqoop-manpage.xml
-/sqoop.1
-/Sqoop-web.html
diff --git a/src/contrib/sqoop/doc/Makefile b/src/contrib/sqoop/doc/Makefile
deleted file mode 100644
index 6ccc666..0000000
--- a/src/contrib/sqoop/doc/Makefile
+++ /dev/null
@@ -1,43 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at#
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-BUILDROOT=../../../../build/contrib/sqoop
-BUILD_DIR=$(BUILDROOT)/doc
-
-all: man userguide
-
-man: $(BUILD_DIR)/sqoop.1.gz
-
-userguide: $(BUILD_DIR)/SqoopUserGuide.html
-
-$(BUILD_DIR)/sqoop.1.gz: Sqoop-manpage.txt *formatting*.txt
-	asciidoc -b docbook -d manpage Sqoop-manpage.txt
-	xmlto man Sqoop-manpage.xml
-	gzip sqoop.1
-	rm Sqoop-manpage.xml
-	mkdir -p $(BUILD_DIR)
-	mv sqoop.1.gz $(BUILD_DIR)
-
-$(BUILD_DIR)/SqoopUserGuide.html: SqoopUserGuide.txt *.txt
-	asciidoc SqoopUserGuide.txt
-	mkdir -p $(BUILD_DIR)
-	mv SqoopUserGuide.html $(BUILD_DIR)
-
-clean:
-	-rm $(BUILD_DIR)/sqoop.1.gz
-	-rm $(BUILD_DIR)/SqoopUserGuide.html
-
-.PHONY: all man userguide clean
-
diff --git a/src/contrib/sqoop/doc/Sqoop-manpage.txt b/src/contrib/sqoop/doc/Sqoop-manpage.txt
deleted file mode 100644
index be82c66..0000000
--- a/src/contrib/sqoop/doc/Sqoop-manpage.txt
+++ /dev/null
@@ -1,219 +0,0 @@
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-sqoop(1)
-========
-
-NAME
-----
-sqoop - SQL-to-Hadoop import tool
-
-SYNOPSIS
---------
-'sqoop' <options>
-
-DESCRIPTION
------------
-Sqoop is a tool designed to help users of large data import existing
-relational databases into their Hadoop clusters. Sqoop uses JDBC to
-connect to a database, examine each table's schema, and auto-generate
-the necessary classes to import data into HDFS. It then instantiates
-a MapReduce job to read tables from the database via the DBInputFormat
-(JDBC-based InputFormat). Tables are read into a set of files loaded
-into HDFS. Both SequenceFile and text-based targets are supported. Sqoop
-also supports high-performance imports from select databases including MySQL.
-
-OPTIONS
--------
-
-The +--connect+ option is always required. To perform an import, one of
-+--table+ or +--all-tables+ is required as well. Alternatively, you can
-specify +--generate-only+ or one of the arguments in "Additional commands."
-
-
-Database connection options
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
---connect (jdbc-uri)::
-  Specify JDBC connect string (required)
-
---driver (class-name)::
-  Manually specify JDBC driver class to use
-
---username (username)::
-  Set authentication username
-
---password (password)::
-  Set authentication password
-  (Note: This is very insecure. You should use -P instead.)
-
--P::
-  Prompt for user password
-
---direct::
-  Use direct import fast path (mysql only)
-
-Import control options
-~~~~~~~~~~~~~~~~~~~~~~
-
---all-tables::
-  Import all tables in database
-  (Ignores +--table+, +--columns+, +--order-by+, and +--where+)
-
---columns (col,col,col...)::
-  Columns to export from table
-
---split-by (column-name)::
-  Column of the table used to split the table for parallel import
-
---hadoop-home (dir)::
-  Override $HADOOP_HOME
-
---hive-home (dir)::
-  Override $HIVE_HOME
-
---warehouse-dir (dir)::
-  Tables are uploaded to the HDFS path +/warehouse/dir/(tablename)/+
-
---as-sequencefile::
-  Imports data to SequenceFiles
-
---as-textfile::
-  Imports data as plain text (default)
-
---hive-import::
-  If set, then import the table into Hive
-
---hive-create-only::
-  Creates table in hive and skips the data import step
-
---hive-overwrite::
-  Overwrites existing table in hive.
-  By default it does not overwrite existing table.
-
---table (table-name)::
-  The table to import
-
---hive-table (table-name)::
-  When used with --hive-import, overrides the destination table name
-
---where (clause)::
-  Import only the rows for which _clause_ is true.
-  e.g.: `--where "user_id > 400 AND hidden == 0"`
-
---compress::
--z::
-  Uses gzip to compress data as it is written to HDFS
-
---direct-split-size (size)::
-  When using direct mode, write to multiple files of
-  approximately _size_ bytes each.
-
-Export control options
-~~~~~~~~~~~~~~~~~~~~~~
-
---export-dir (dir)::
-  Export from an HDFS path into a table (set with
-  --table)
-
-Output line formatting options
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-include::output-formatting.txt[]
-include::output-formatting-args.txt[]
-
-Input line parsing options
-~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-include::input-formatting.txt[]
-include::input-formatting-args.txt[]
-
-Code generation options
-~~~~~~~~~~~~~~~~~~~~~~~
-
---bindir (dir)::
-  Output directory for compiled objects
-
---class-name (name)::
-  Sets the name of the class to generate. By default, classes are
-  named after the table they represent. Using this parameters
-  ignores +--package-name+.
-
---generate-only::
-  Stop after code generation; do not import
-
---outdir (dir)::
-  Output directory for generated code
-
---package-name (package)::
-  Puts auto-generated classes in the named Java package
-
-Library loading options
-~~~~~~~~~~~~~~~~~~~~~~~
---jar-file (file)::
-  Disable code generation; use specified jar
-
---class-name (name)::
-  The class within the jar that represents the table to import/export
-
-Additional commands
-~~~~~~~~~~~~~~~~~~~
-
-These commands cause Sqoop to report information and exit;
-no import or code generation is performed.
-
---debug-sql (statement)::
-  Execute 'statement' in SQL and display the results
-
---help::
-  Display usage information and exit
-
---list-databases::
-  List all databases available and exit
-
---list-tables::
-  List tables in database and exit
-
---verbose::
-  Print more information while working
-
-Database-specific options
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Additional arguments may be passed to the database manager
-after a lone '-' on the command-line.
-
-In MySQL direct mode, additional arguments are passed directly to
-mysqldump.
-
-ENVIRONMENT
------------
-
-JAVA_HOME::
-  As part of its import process, Sqoop generates and compiles Java code
-  by invoking the Java compiler *javac*(1). As a result, JAVA_HOME must
-  be set to the location of your JDK (note: This cannot just be a JRE).
-  e.g., +/usr/java/default+. Hadoop (and Sqoop) requires Sun Java 1.6 which
-  can be downloaded from http://java.sun.com.
-
-HADOOP_HOME::
-  The location of the Hadoop jar files. If you installed Hadoop via RPM
-  or DEB, these are in +/usr/lib/hadoop-20+.
-
-HIVE_HOME::
-  If you are performing a Hive import, you must identify the location of
-  Hive's jars and configuration. If you installed Hive via RPM or DEB,
-  these are in +/usr/lib/hive+.
-
diff --git a/src/contrib/sqoop/doc/SqoopUserGuide.txt b/src/contrib/sqoop/doc/SqoopUserGuide.txt
deleted file mode 100644
index e777774..0000000
--- a/src/contrib/sqoop/doc/SqoopUserGuide.txt
+++ /dev/null
@@ -1,67 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-include::intro.txt[]
-
-
-The Sqoop Command Line
-----------------------
-
-To execute Sqoop, run with Hadoop:
-----
-$ bin/hadoop jar contrib/sqoop/hadoop-$(version)-sqoop.jar (arguments)
-----
-
-NOTE:Throughput this document, we will use `sqoop` as shorthand for the
-above. i.e., `$ sqoop (arguments)`
-
-You pass this program options describing the
-import job you want to perform. If you need a hint, running Sqoop with
-`--help` will print out a list of all the command line
-options available. The +sqoop(1)+ manual page will also describe
-Sqoop's available arguments in greater detail. The manual page is built
-in `$HADOOP_HOME/build/contrib/sqoop/doc/sqoop.1.gz`.
-The following subsections will describe the most common modes of operation.
-
-include::connecting.txt[]
-
-include::listing-dbs.txt[]
-
-include::listing-tables.txt[]
-
-include::full-db-import.txt[]
-
-include::table-import.txt[]
-
-include::controlling-output-format.txt[]
-
-include::classnames.txt[]
-
-include::misc-args.txt[]
-
-include::direct.txt[]
-
-include::hive.txt[]
-
-include::export.txt[]
-
-include::supported-dbs.txt[]
-
-include::api-reference.txt[]
-
diff --git a/src/contrib/sqoop/doc/api-reference.txt b/src/contrib/sqoop/doc/api-reference.txt
deleted file mode 100644
index 2b22d0a..0000000
--- a/src/contrib/sqoop/doc/api-reference.txt
+++ /dev/null
@@ -1,243 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-Developer API Reference
------------------------
-
-This section is intended to specify the APIs available to application writers
-integrating with Sqoop, and those modifying Sqoop. The next three subsections
-are written from the following three perspectives: those using classes generated
-by Sqoop, and its public library; those writing Sqoop extensions (i.e.,
-additional ConnManager implementations that interact with more databases); and
-those modifying Sqoop's internals. Each section describes the system in
-successively greater depth.
-
-
-The External API
-~~~~~~~~~~~~~~~~
-
-Sqoop auto-generates classes that represent the tables imported into HDFS. The
-class contains member fields for each column of the imported table; an instance
-of the class holds one row of the table. The generated classes implement the
-serialization APIs used in Hadoop, namely the _Writable_ and _DBWritable_
-interfaces.  They also contain other convenience methods: a +parse()+ method
-that interprets delimited text fields, and a +toString()+ method that preserves
-the user's chosen delimiters. The full set of methods guaranteed to exist in an
-auto-generated class are specified in the interface
-+org.apache.hadoop.sqoop.lib.SqoopRecord+.
-
-Instances of _SqoopRecord_ may depend on Sqoop's public API. This is all classes
-in the +org.apache.hadoop.sqoop.lib+ package. These are briefly described below.
-Clients of Sqoop should not need to directly interact with any of these classes,
-although classes generated by Sqoop will depend on them. Therefore, these APIs
-are considered public and care will be taken when forward-evolving them.
-
-* The +RecordParser+ class will parse a line of text into a list of fields,
-  using controllable delimiters and quote characters.
-* The static +FieldFormatter+ class provides a method which handles quoting and
-  escaping of characters in a field which will be used in
-  +SqoopRecord.toString()+ implementations.
-* Marshaling data between _ResultSet_ and _PreparedStatement_ objects and
-  _SqoopRecords_ is done via +JdbcWritableBridge+.
-* +BigDecimalSerializer+ contains a pair of methods that facilitate
-  serialization of +BigDecimal+ objects over the _Writable_ interface.
-
-The Extension API
-~~~~~~~~~~~~~~~~~
-
-This section covers the API and primary classes used by extensions for Sqoop
-which allow Sqoop to interface with more database vendors.
-
-While Sqoop uses JDBC and +DBInputFormat+ (and +DataDrivenDBInputFormat+) to
-read from databases, differences in the SQL supported by different vendors as
-well as JDBC metadata necessitates vendor-specific codepaths for most databases.
-Sqoop's solution to this problem is by introducing the ConnManager API
-(+org.apache.hadoop.sqoop.manager.ConnMananger+).
-
-+ConnManager+ is an abstract class defining all methods that interact with the
-database itself. Most implementations of +ConnManager+ will extend the
-+org.apache.hadoop.sqoop.manager.SqlManager+ abstract class, which uses standard
-SQL to perform most actions. Subclasses are required to implement the
-+getConnection()+ method which returns the actual JDBC connection to the
-database. Subclasses are free to override all other methods as well. The
-+SqlManager+ class itself exposes a protected API that allows developers to
-selectively override behavior. For example, the +getColNamesQuery()+ method
-allows the SQL query used by +getColNames()+ to be modified without needing to
-rewrite the majority of +getColNames()+.
-
-+ConnManager+ implementations receive a lot of their configuration data from a
-Sqoop-specific class, +SqoopOptions+. While +SqoopOptions+ does not currently
-contain many setter methods, clients should not assume +SqoopOptions+ are
-immutable. More setter methods may be added in the future.  +SqoopOptions+ does
-not directly store specific per-manager options. Instead, it contains a
-reference to the +Configuration+ returned by +Tool.getConf()+ after parsing
-command-line arguments with the +GenericOptionsParser+. This allows extension
-arguments via "+-D any.specific.param=any.value+" without requiring any layering
-of options parsing or modification of +SqoopOptions+.
-
-All existing +ConnManager+ implementations are stateless. Thus, the system which
-instantiates +ConnManagers+ may implement multiple instances of the same
-+ConnMananger+ class over Sqoop's lifetime. If a caching layer is required, we
-can add one later, but it is not currently available.
-
-+ConnManagers+ are currently created by instances of the abstract class +ManagerFactory+ (See
-MAPREDUCE-750). One +ManagerFactory+ implementation currently serves all of
-Sqoop: +org.apache.hadoop.sqoop.manager.DefaultManagerFactory+.  Extensions
-should not modify +DefaultManagerFactory+. Instead, an extension-specific
-+ManagerFactory+ implementation should be provided with the new ConnManager.
-+ManagerFactory+ has a single method of note, named +accept()+. This method will
-determine whether it can instantiate a +ConnManager+ for the user's
-+SqoopOptions+. If so, it returns the +ConnManager+ instance. Otherwise, it
-returns +null+.
-
-The +ManagerFactory+ implementations used are governed by the
-+sqoop.connection.factories+ setting in sqoop-site.xml. Users of extension
-libraries can install the 3rd-party library containing a new +ManagerFactory+
-and +ConnManager+(s), and configure sqoop-site.xml to use the new
-+ManagerFactory+.  The +DefaultManagerFactory+ principly discriminates between
-databases by parsing the connect string stored in +SqoopOptions+.
-
-Extension authors may make use of classes in the +org.apache.hadoop.sqoop.io+,
-+mapred+, +mapreduce+, and +util+ packages to facilitate their implementations.
-These packages and classes are described in more detail in the following
-section.
-
-
-Sqoop Internals
-~~~~~~~~~~~~~~~
-
-This section describes the internal architecture of Sqoop.
-
-The Sqoop program is driven by the +org.apache.hadoop.sqoop.Sqoop+ main class.
-A limited number of additional classes are in the same package; +SqoopOptions+
-(described earlier) and +ConnFactory+ (which manipulates +ManagerFactory+
-instances).
-
-General program flow
-^^^^^^^^^^^^^^^^^^^^
-
-The general program flow is as follows:
-
-+org.apache.hadoop.sqoop.Sqoop+ is the main class and implements _Tool_. A new
-instance is launched with +ToolRunner+. It parses its arguments using the
-+SqoopOptions+ class.  Within the +SqoopOptions+, an +ImportAction+ will be
-chosen by the user. This may be import all tables, import one specific table,
-execute a SQL statement, or others.
-
-A +ConnManager+ is then instantiated based on the data in the +SqoopOptions+.
-The +ConnFactory+ is used to get a +ConnManager+ from a +ManagerFactory+; the
-mechanics of this were described in an earlier section.
-
-Then in the +run()+ method, using a case statement, it determines which actions
-the user needs performed based on the +ImportAction+ enum. Usually this involves
-determining a list of tables to import, generating user code for them, and
-running a MapReduce job per table to read the data.  The import itself does not
-specifically need to be run via a MapReduce job; the +ConnManager.importTable()+
-method is left to determine how best to run the import. Each of these actions is
-controlled by the +ConnMananger+, except for the generating of code, which is
-done by the +CompilationManager+ and +ClassWriter+. (Both in the
-+org.apache.hadoop.sqoop.orm+ package.) Importing into Hive is also taken care
-of via the +org.apache.hadoop.sqoop.hive.HiveImport+ class after the
-+importTable()+ has completed. This is done without concern for the
-+ConnManager+ implementation used.
-
-A ConnManager's +importTable()+ method receives a single argument of type
-+ImportJobContext+ which contains parameters to the method. This class may be
-extended with additional parameters in the future, which optionally further
-direct the import operation. Similarly, the +exportTable()+ method receives an
-argument of type +ExportJobContext+. These classes contain the name of the table
-to import/export, a reference to the +SqoopOptions+ object, and other related
-data.
-
-Subpackages
-^^^^^^^^^^^
-
-The following subpackages under +org.apache.hadoop.sqoop+ exist:
-
-* +hive+ - Facilitates importing data to Hive.
-* +io+ - Implementations of +java.io.*+ interfaces (namely, _OutputStream_ and
-  _Writer_).
-* +lib+ - The external public API (described earlier).
-* +manager+ - The +ConnManager+ and +ManagerFactory+ interface and their
-  implementations.
-* +mapred+ - Classes interfacing with the old (pre-0.20) MapReduce API.
-* +mapreduce+ - Classes interfacing with the new (0.20+) MapReduce API....
-* +orm+ - Code auto-generation.
-* +util+ - Miscellaneous utility classes.
-
-The +io+ package contains _OutputStream_ and _BufferedWriter_ implementations
-used by direct writers to HDFS. The +SplittableBufferedWriter+ allows a single
-BufferedWriter to be opened to a client which will, under the hood, write to
-multiple files in series as they reach a target threshold size. This allows
-unsplittable compression libraries (e.g., gzip) to be used in conjunction with
-Sqoop import while still allowing subsequent MapReduce jobs to use multiple
-input splits per dataset.
-
-Code in the +mapred+ package should be considered deprecated. The +mapreduce+
-package contains +DataDrivenImportJob+, which uses the +DataDrivenDBInputFormat+
-introduced in 0.21. The mapred package contains +ImportJob+, which uses the
-older +DBInputFormat+. Most +ConnManager+ implementations use
-+DataDrivenImportJob+; +DataDrivenDBInputFormat+ does not currently work with
-Oracle in all circumstances, so it remains on the old code-path.
-
-The +orm+ package contains code used for class generation. It depends on the
-JDK's tools.jar which provides the com.sun.tools.javac package.
-
-The +util+ package contains various utilities used throughout Sqoop:
-
-* +ClassLoaderStack+ manages a stack of +ClassLoader+ instances used by the
-  current thread. This is principly used to load auto-generated code into the
-  current thread when running MapReduce in local (standalone) mode.
-* +DirectImportUtils+ contains convenience methods used by direct HDFS
-  importers.
-* +Executor+ launches external processes and connects these to stream handlers
-  generated by an AsyncSink (see more detail below).
-* +ExportException+ is thrown by +ConnManagers+ when exports fail.
-* +ImportException+ is thrown by +ConnManagers+ when imports fail.
-* +JdbcUrl+ handles parsing of connect strings, which are URL-like but not
-  specification-conforming. (In particular, JDBC connect strings may have
-  +multi:part:scheme://+ components.)
-* +PerfCounters+ are used to estimate transfer rates for display to the user.
-* +ResultSetPrinter+ will pretty-print a _ResultSet_.
-
-In several places, Sqoop reads the stdout from external processes. The most
-straightforward cases are direct-mode imports as performed by the
-+LocalMySQLManager+ and +DirectPostgresqlManager+. After a process is spawned by
-+Runtime.exec()+, its stdout (+Process.getInputStream()+) and potentially stderr
-(+Process.getErrorStream()+) must be handled. Failure to read enough data from
-both of these streams will cause the external process to block before writing
-more. Consequently, these must both be handled, and preferably asynchronously.
-
-In Sqoop parlance, an "async sink" is a thread that takes an +InputStream+ and
-reads it to completion. These are realized by +AsyncSink+ implementations. The
-+org.apache.hadoop.sqoop.util.AsyncSink+ abstract class defines the operations
-this factory must perform. +processStream()+ will spawn another thread to
-immediately begin handling the data read from the +InputStream+ argument; it
-must read this stream to completion. The +join()+ method allows external threads
-to wait until this processing is complete.
-
-Some "stock" +AsyncSink+ implementations are provided: the +LoggingAsyncSink+ will
-repeat everything on the +InputStream+ as log4j INFO statements. The
-+NullAsyncSink+ consumes all its input and does nothing.
-
-The various +ConnManagers+ that make use of external processes have their own
-+AsyncSink+ implementations as inner classes, which read from the database tools
-and forward the data along to HDFS, possibly performing formatting conversions
-in the meantime.
-
-
diff --git a/src/contrib/sqoop/doc/classnames.txt b/src/contrib/sqoop/doc/classnames.txt
deleted file mode 100644
index ab84bcf..0000000
--- a/src/contrib/sqoop/doc/classnames.txt
+++ /dev/null
@@ -1,43 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Generated Class Names
-~~~~~~~~~~~~~~~~~~~~~
-
-By default, classes are named after the table they represent. e.g.,
-+sqoop --table foo+ will generate a file named +foo.java+. You can
-override the generated class name with the +--class-name+ argument.
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-  --table employee_names --class-name com.example.EmployeeNames
-----
-_This generates a file named +com/example/EmployeeNames.java+_
-
-If you want to specify a package name for generated classes, but
-still want them to be named after the table they represent, you
-can instead use the argument +--package-name+:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-  --table employee_names --package-name com.example
-----
-_This generates a file named +com/example/employee_names.java+_
-
-
diff --git a/src/contrib/sqoop/doc/connecting.txt b/src/contrib/sqoop/doc/connecting.txt
deleted file mode 100644
index 8d60057..0000000
--- a/src/contrib/sqoop/doc/connecting.txt
+++ /dev/null
@@ -1,85 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Connecting to a Database Server
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Sqoop is designed to import tables from a database into HDFS. As such,
-it requires a _connect string_ that describes how to connect to the
-database. The _connect string_ looks like a URL, and is communicated to
-Sqoop with the +--connect+ argument. This describes the server and
-database to connect to; it may also specify the port. e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees
-----
-
-This string will connect to a MySQL database named +employees+ on the
-host +database.example.com+. It's important that you *do not* use the URL
-+localhost+ if you intend to use Sqoop with a distributed Hadoop
-cluster. The connect string you supply will be used on TaskTracker nodes
-throughout your MapReduce cluster; if they're told to connect to the
-literal name +localhost+, they'll each reach a different
-database (or more likely, no database at all)! Instead, you should use
-the full hostname or IP address of the database host that can be seen
-by all your remote nodes.
-
-You may need to authenticate against the database before you can
-access it. The +--username+ and +--password+ or +-P+ parameters can
-be used to supply a username and a password to the database. e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-    --username aaron --password 12345
-----
-
-.Password security
-WARNING: The +--password+ parameter is insecure, as other users may
-be able to read your password from the command-line arguments via
-the output of programs such as `ps`. The *+-P+* argument will read
-a password from a console prompt, and is the preferred method of
-entering credentials. Credentials may still be transferred between
-nodes of the MapReduce cluster using insecure means.
-
-Sqoop automatically supports several databases, including MySQL. Connect strings beginning
-with +jdbc:mysql://+ are handled automatically Sqoop, though you may need
-to install the driver yourself. (A full list of databases with
-built-in support is provided in the "Supported Databases" section, below.)
-
-You can use Sqoop with any other
-JDBC-compliant database as well. First, download the appropriate JDBC
-driver for the database you want to import from, and install the .jar
-file in the +/usr/hadoop/lib+ directory on all machines in your Hadoop
-cluster, or some other directory which is in the classpath
-on all nodes. Each driver jar also has a specific driver class which defines
-the entry-point to the driver. For example, MySQL's Connector/J library has
-a driver class of +com.mysql.jdbc.Driver+. Refer to your database
-vendor-specific documentation to determine the main driver class.
-This class must be provided as an argument to Sqoop with +--driver+.
-
-For example, to connect to a postgres database, first download the driver from
-link:http://jdbc.postgresql.org[http://jdbc.postgresql.org] and
-install it in your Hadoop lib path.
-Then run Sqoop with something like:
-
-----
-$ sqoop --connect jdbc:postgresql://postgres-server.example.com/employees \
-    --driver org.postgresql.Driver
-----
-
diff --git a/src/contrib/sqoop/doc/controlling-input-format.txt b/src/contrib/sqoop/doc/controlling-input-format.txt
deleted file mode 100644
index 4a5268c..0000000
--- a/src/contrib/sqoop/doc/controlling-input-format.txt
+++ /dev/null
@@ -1,42 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Controlling the Input Format
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-include::input-formatting.txt[]
-
-The following arguments allow you to control the input format of
-records:
-
-include::input-formatting-args.txt[]
-
-If you have already imported data into HDFS in a text-based
-representation and want to change the delimiters being used, you
-should regenerate the class via `sqoop --generate-only`, specifying
-the new delimiters with +--fields-terminated-by+, etc., and the old
-delimiters with +--input-fields-terminated-by+, etc. Then run a
-MapReduce job where your mapper creates an instance of your record
-class, uses its +parse()+ method to read the fields using the old
-delimiters, and emits a new +Text+ output value via the record's
-+toString()+ method, which will use the new delimiters. You'll then
-want to regenerate the class another time without the
-+--input-fields-terminated-by+ specified so that the new delimiters
-are used for both input and output.
-
diff --git a/src/contrib/sqoop/doc/controlling-output-format.txt b/src/contrib/sqoop/doc/controlling-output-format.txt
deleted file mode 100644
index f871611..0000000
--- a/src/contrib/sqoop/doc/controlling-output-format.txt
+++ /dev/null
@@ -1,38 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Controlling the Output Format
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-include::output-formatting.txt[]
-
-The following arguments allow you to control the output format of
-records:
-
-include::output-formatting-args.txt[]
-
-For example, we may want to separate records by tab characters, with
-every record surrounded by "double quotes", and internal quote marks
-escaped by a backslash (+\+) character:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-  --table employee_names --fields-terminated-by \t \
-  --lines-terminated-by \n --enclosed-by '\"' --escaped-by '\\'
-----
-
diff --git a/src/contrib/sqoop/doc/direct.txt b/src/contrib/sqoop/doc/direct.txt
deleted file mode 100644
index 4c02068..0000000
--- a/src/contrib/sqoop/doc/direct.txt
+++ /dev/null
@@ -1,77 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Direct-mode Imports
--------------------
-
-While the JDBC-based import method used by Sqoop provides it with the
-ability to read from a variety of databases using a generic driver, it
-is not the most high-performance method available. Sqoop can read from
-certain database systems faster by using their built-in export tools.
-
-For example, Sqoop can read from a MySQL database by using the +mysqldump+
-tool distributed with MySQL. You can take advantage of this faster
-import method by running Sqoop with the +--direct+ argument. This
-combined with a connect string that begins with +jdbc:mysql://+ will
-inform Sqoop that it should select the faster access method.
-
-If your delimiters exactly match the delimiters used by +mysqldump+,
-then Sqoop will use a fast-path that copies the data directly from
-+mysqldump+'s output into HDFS. Otherwise, Sqoop will parse +mysqldump+'s
-output into fields and transcode them into the user-specified delimiter set.
-This incurs additional processing, so performance may suffer.
-For convenience, the +--mysql-delimiters+
-argument will set all the output delimiters to be consistent with
-+mysqldump+'s format.
-
-Sqoop also provides a direct-mode backend for PostgreSQL that uses the
-+COPY TO STDOUT+ protocol from +psql+. No specific delimiter set provides
-better performance; Sqoop will forward delimiter control arguments to
-+psql+.
-
-The "Supported Databases" section provides a full list of database vendors
-which have direct-mode support from Sqoop.
-
-When writing to HDFS, direct mode will open a single output file to receive
-the results of the import. You can instruct Sqoop to use multiple output
-files by using the +--direct-split-size+ argument which takes a size in
-bytes. Sqoop will generate files of approximately this size. e.g.,
-+--direct-split-size 1000000+ will generate files of approximately 1 MB
-each. If compressing the HDFS files with +--compress+, this will allow
-subsequent MapReduce programs to use multiple mappers across your data
-in parallel.
-
-Tool-specific arguments
-~~~~~~~~~~~~~~~~~~~~~~~
-
-Sqoop will generate a set of command-line arguments with which it invokes
-the underlying direct-mode tool (e.g., mysqldump). You can specify additional
-arguments which should be passed to the tool by passing them to Sqoop
-after a single '+-+' argument. e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://localhost/db --table foo --direct - --lock-tables
-----
-
-The +--lock-tables+ argument (and anything else to the right of the +-+ argument)
-will be passed directly to mysqldump.
-
-
-
-
diff --git a/src/contrib/sqoop/doc/export.txt b/src/contrib/sqoop/doc/export.txt
deleted file mode 100644
index 84f286b..0000000
--- a/src/contrib/sqoop/doc/export.txt
+++ /dev/null
@@ -1,58 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Exporting to a Database
------------------------
-
-In addition to importing database tables into HDFS, Sqoop can also
-work in "reverse," reading the contents of a file or directory in
-HDFS, interpreting the data as database rows, and inserting them
-into a specified database table.
-
-To run an export, invoke Sqoop with the +--export-dir+ and
-+--table+ options. e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://db.example.com/foo --table bar  \
-    --export-dir /results/bar_data
-----
-
-This will take the files in +/results/bar_data+ and inject their
-contents in to the +bar+ table in the +foo+ database on +db.example.com+.
-The target table must already exist in the database. Sqoop will perform
-a set of +INSERT INTO+ operations, without regard for existing content. If
-Sqoop attempts to insert rows which violate constraints in the database
-(e.g., a particular primary key value already exists), then the export
-will fail.
-
-As in import mode, Sqoop will auto-generate an interoperability class
-to use with the particular table in question. This will be used to parse
-the records in HDFS files before loading their contents into the database.
-You must specify the same delimiters (e.g., with +--fields-terminated-by+,
-etc.) as are used in the files to export in order to parse the data
-correctly. If your data is stored in SequenceFiles (created with an import
-in the +--as-sequencefile+ format), then you do not need to specify
-delimiters.
-
-If you have an existing auto-generated jar and class that you intend to use
-with Sqoop, you can specify these with the +--jar-file+ and +--class-name+
-parameters. Providing these options will disable autogeneration of a new
-class based on the target table.
-
-
diff --git a/src/contrib/sqoop/doc/full-db-import.txt b/src/contrib/sqoop/doc/full-db-import.txt
deleted file mode 100644
index 1f7ec15..0000000
--- a/src/contrib/sqoop/doc/full-db-import.txt
+++ /dev/null
@@ -1,92 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Automatic Full-database Import
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-If you want to import all the tables in a database, you can use the
-+--all-tables+ command to do so:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees --all-tables
-----
-
-This will query the database for the available tables, generate an ORM
-class for each table, and run a MapReduce job to import each one.
-Hadoop uses the DBInputFormat to read from a database into a Mapper
-instance. To read a table into a MapReduce program requires creating a
-class to hold the fields of one row of the table. One of the benefits
-of Sqoop is that it generates this class definition for you, based on
-the table definition in the database.
-
-The generated +.java+ files are, by default, placed in the current
-directory. You can supply a different directory with the +--outdir+
-parameter. These are then compiled into +.class+ and +.jar+ files for use
-by the MapReduce job that it launches. These files are created in a
-temporary directory. You can redirect this target with +--bindir+.
-
-Each table will be imported into a separate directory in HDFS, with
-the same name as the table. For instance, if my Hadoop username is
-aaron, the above command would have generated the following
-directories in HDFS:
-
-----
-/user/aaron/employee_names
-/user/aaron/payroll_checks
-/user/aaron/job_descriptions
-/user/aaron/office_supplies
-----
-
-You can change the base directory under which the tables are loaded
-with the +--warehouse-dir+ parameter. For example:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees --all-tables \
-    --warehouse-dir /common/warehouse
-----
-
-This would create the following directories instead:
-
-----
-/common/warehouse/employee_names
-/common/warehouse/payroll_checks
-/common/warehouse/job_descriptions
-/common/warehouse/office_supplies
-----
-
-By default the data will be read into text files in HDFS. Each of the
-columns will be represented as comma-delimited text. Each row is
-terminated by a newline. See the section on "Controlling the Output
-Format" below for information on how to change these delimiters.
-
-If you want to leverage compression and binary file formats, the
-+--as-sequencefile+ argument to Sqoop will import the table
-to a set of SequenceFiles instead. This stores each field of each
-database record in a separate object in a SequenceFile.
-This representation is also likely to be higher performance when used
-as an input to subsequent MapReduce programs as it does not require
-parsing. For completeness, Sqoop provides an +--as-textfile+ option, which is
-implied by default. An +--as-textfile+ on the command-line will override
-a previous +--as-sequencefile+ argument.
-
-The SequenceFile format will embed the records from the database as
-objects using the code generated by Sqoop. It is important that you
-retain the +.java+ file for this class, as you will need to be able to
-instantiate the same type to read the objects back later, in other
-user-defined applications.
-
diff --git a/src/contrib/sqoop/doc/hive.txt b/src/contrib/sqoop/doc/hive.txt
deleted file mode 100644
index 4947500..0000000
--- a/src/contrib/sqoop/doc/hive.txt
+++ /dev/null
@@ -1,66 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Importing Data Into Hive
-------------------------
-
-Sqoop's primary function is to upload your data into files in HDFS. If
-you have a Hive metastore associated with your HDFS cluster, Sqoop can
-also import the data into Hive by generating and executing a +CREATE
-TABLE+ statement to define the data's layout in Hive. Importing data
-into Hive is as simple as adding the *+--hive-import+* option to your
-Sqoop command line.
-
-By default the data is imported into HDFS, but you can skip this operation
-by using the *+--hive-create+* option. Optionally, you can specify the
-*+--hive-overwrite+* option to indicate that existing table in hive must
-be replaced. After your data is imported into HDFS or this step is
-omitted, Sqoop will generate a Hive script containing a +CREATE TABLE+
-operation defining your columns using Hive's types, and a +LOAD DATA INPATH+
-statement to move the data files into Hive's warehouse directory if
-*+--hive-create+* option is not added. The script will be executed by calling
-the installed copy of hive on the machine where Sqoop is run. If you have
-multiple Hive installations, or +hive+ is not in your +$PATH+ use the
-*+--hive-home+* option to identify the Hive installation directory.
-Sqoop will use +$HIVE_HOME/bin/hive+ from here.
-
-NOTE: This function is incompatible with +--as-sequencefile+.
-
-Hive's text parser does not know how to support escaping or enclosing
-characters. Sqoop will print a warning if you use +--escaped-by+,
-+--enclosed-by+, or +--optionally-enclosed-by+ since Hive does not know
-how to parse these. It will pass the field and record terminators through
-to Hive. If you do not set any delimiters and do use +--hive-import+,
-the field delimiter will be set to +^A+ and the record delimiter will
-be set to +\n+ to be consistent with Hive's defaults.
-
-The table name used in Hive is, by default, the same as that of the
-source table. You can control the output table name with the +--hive-table+
-option.
-
-Hive's Type System
-~~~~~~~~~~~~~~~~~~
-
-Hive users will note that there is not a one-to-one mapping between
-SQL types and Hive types. In general, SQL types that do not have a
-direct mapping (e.g., +DATE+, +TIME+, and +TIMESTAMP+) will be coerced to
-+STRING+ in Hive. The +NUMERIC+ and +DECIMAL+ SQL types will be coerced to
-+DOUBLE+. In these cases, Sqoop will emit a warning in its log messages
-informing you of the loss of precision.
-
diff --git a/src/contrib/sqoop/doc/input-formatting-args.txt b/src/contrib/sqoop/doc/input-formatting-args.txt
deleted file mode 100644
index 0a41ccd..0000000
--- a/src/contrib/sqoop/doc/input-formatting-args.txt
+++ /dev/null
@@ -1,34 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
---input-fields-terminated-by (char)::
-  Sets the input field separator
-
---input-lines-terminated-by (char)::
-  Sets the input end-of-line char
-
---input-optionally-enclosed-by (char)::
-  Sets an input field-enclosing character
-
---input-enclosed-by (char)::
-  Sets a required input field encloser
-
---input-escaped-by (char)::
-  Sets the input escape character
-
diff --git a/src/contrib/sqoop/doc/input-formatting.txt b/src/contrib/sqoop/doc/input-formatting.txt
deleted file mode 100644
index d3d82f1..0000000
--- a/src/contrib/sqoop/doc/input-formatting.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-Record classes generated by Sqoop include both a +toString()+ method
-that formats output records, and a +parse()+ method that interprets
-text based on an input delimiter set. The input delimiters default to
-the same ones chosen for output delimiters, but you can override these
-settings to support converting from one set of delimiters to another.
-
diff --git a/src/contrib/sqoop/doc/intro.txt b/src/contrib/sqoop/doc/intro.txt
deleted file mode 100644
index 0a7b338..0000000
--- a/src/contrib/sqoop/doc/intro.txt
+++ /dev/null
@@ -1,34 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Introduction
-------------
-
-Sqoop is a tool designed to help users of large data import
-existing relational databases into their Hadoop clusters. Sqoop uses
-JDBC to connect to a database, examine each table's schema, and
-auto-generate the necessary classes to import data into HDFS. It
-then instantiates a MapReduce job to read tables from the database
-via the DBInputFormat (JDBC-based InputFormat). Tables are read
-into a set of files loaded into HDFS. Both SequenceFile and
-text-based targets are supported. Sqoop also supports high-performance
-imports from select databases including MySQL.
-
-This document describes how to get started using Sqoop to import
-your data into Hadoop.
diff --git a/src/contrib/sqoop/doc/listing-dbs.txt b/src/contrib/sqoop/doc/listing-dbs.txt
deleted file mode 100644
index c66ee8e..0000000
--- a/src/contrib/sqoop/doc/listing-dbs.txt
+++ /dev/null
@@ -1,35 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Listing Available Databases
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Once connected to a database server, you can list the available
-databases with the +--list-databases+ parameter. This currently is supported
-only by HSQLDB and MySQL. Note that in this case, the connect string does
-not include a database name, just a server address.
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/ --list-databases
-information_schema
-employees
-----
-_This only works with HSQLDB and MySQL. A vendor-agnostic implementation of
-this function has not yet been implemented._
-
diff --git a/src/contrib/sqoop/doc/listing-tables.txt b/src/contrib/sqoop/doc/listing-tables.txt
deleted file mode 100644
index 83933e0..0000000
--- a/src/contrib/sqoop/doc/listing-tables.txt
+++ /dev/null
@@ -1,34 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Listing Available Tables
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-Within a database, you can list the tables available for import with
-the +--list-tables+ command. The following example shows four tables available
-within the "employees" example database:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees --list-tables
-employee_names
-payroll_checks
-job_descriptions
-office_supplies
-----
-
diff --git a/src/contrib/sqoop/doc/misc-args.txt b/src/contrib/sqoop/doc/misc-args.txt
deleted file mode 100644
index 2143942..0000000
--- a/src/contrib/sqoop/doc/misc-args.txt
+++ /dev/null
@@ -1,40 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Miscellaneous Additional Arguments
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you want to generate the Java classes to represent tables without
-actually performing an import, supply a connect string and
-(optionally) credentials as above, as well as +--all-tables+ or
-+--table+, but also use the +--generate-only+ argument. This will
-generate the classes and cease further operation.
-
-You can override the +$HADOOP_HOME+ environment variable within Sqoop
-with the +--hadoop-home+ argument. You can override the +$HIVE_HOME+
-environment variable with +--hive-home+.
-
-Data emitted to HDFS is by default uncompressed. You can instruct
-Sqoop to use gzip to compress your data by providing either the
-+--compress+ or +-z+ argument (both are equivalent).
-
-Using +--verbose+ will instruct Sqoop to print more details about its
-operation; this is particularly handy if Sqoop appears to be misbehaving.
-
-
diff --git a/src/contrib/sqoop/doc/output-formatting-args.txt b/src/contrib/sqoop/doc/output-formatting-args.txt
deleted file mode 100644
index 4d9130f..0000000
--- a/src/contrib/sqoop/doc/output-formatting-args.txt
+++ /dev/null
@@ -1,39 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
---fields-terminated-by (char)::
-  Sets the field separator character
-
---lines-terminated-by (char)::
-  Sets the end-of-line character
-
---optionally-enclosed-by (char)::
-  Sets a field-enclosing character which may be used if a
-  value contains delimiter characters.
-
---enclosed-by (char)::
-  Sets a field-enclosing character which will be used for all fields.
-
---escaped-by (char)::
-  Sets the escape character
-
---mysql-delimiters::
-Uses MySQL's default delimiter set:
-+
-fields: ,  lines: \n  escaped-by: \  optionally-enclosed-by: '
-
diff --git a/src/contrib/sqoop/doc/output-formatting.txt b/src/contrib/sqoop/doc/output-formatting.txt
deleted file mode 100644
index e4fe9ff..0000000
--- a/src/contrib/sqoop/doc/output-formatting.txt
+++ /dev/null
@@ -1,44 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-The delimiters used to separate fields and records can be specified
-on the command line, as can a quoting character and an escape character
-(for quoting delimiters inside a values). Data imported with
-+--as-textfile+ will be formatted according to these parameters. Classes
-generated by Sqoop will encode this information, so using +toString()+
-from a data record stored +--as-sequencefile+ will reproduce your
-specified formatting.
-
-The +(char)+ argument for each argument in this section can be specified
-either as a normal character (e.g., +--fields-terminated-by ,+) or via
-an escape sequence. Arguments of the form +\0xhhh+ will be interpreted
-as a hexidecimal representation of a character with hex number _hhh_.
-Arguments of the form +\0ooo+ will be treated as an octal representation
-of a character represented by octal number _ooo_. The special escapes
-+\n+, +\r+, +\"+, +\b+, +\t+, and +\\+ act as they do inside Java strings. +\0+ will be
-treated as NUL. This will insert NUL characters between fields or lines
-(if used for +--fields-terminated-by+ or +--lines-terminated-by+), or will
-disable enclosing/escaping if used for one of the +--enclosed-by+,
-+--optionally-enclosed-by+, or +--escaped-by+ arguments.
-
-The default delimiters are +,+ for fields, +\n+ for records, no quote
-character, and no escape character. Note that this can lead to
-ambiguous/unparsible records if you import database records containing
-commas or newlines in the field data. For unambiguous parsing, both must
-be enabled, e.g., via +--mysql-delimiters+.
-
diff --git a/src/contrib/sqoop/doc/supported-dbs.txt b/src/contrib/sqoop/doc/supported-dbs.txt
deleted file mode 100644
index aebd9c3..0000000
--- a/src/contrib/sqoop/doc/supported-dbs.txt
+++ /dev/null
@@ -1,55 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Supported Databases
--------------------
-
-Sqoop uses JDBC to connect to databases. JDBC is a compatibility layer
-that allows a program to access many different databases through a common
-API. Slight differences in the SQL language spoken by each database, however,
-may mean that Sqoop can't use every database out of the box, or that some
-databases may be used in an inefficient manner.
-
-When you provide a connect string to Sqoop, it inspects the protocol scheme to
-determine appropriate vendor-specific logic to use. If Sqoop knows about
-a given database, it will work automatically. If not, you may need to
-specify the driver class to load via +--driver+. This will use a generic
-code path which will use standard SQL to access the database. Sqoop provides
-some databases with faster, non-JDBC-based access mechanisms. These can be
-enabled by specfying the +--direct+ parameter.
-
-Sqoop includes vendor-specific code paths for the following databases:
-
-[grid="all"]
-`-----------`--------`--------------------`---------------------
-Database    version  +--direct+ support?  connect string matches
-----------------------------------------------------------------
-HSQLDB      1.8.0+   No                   +jdbc:hsqldb:*//+
-MySQL       5.0+     Yes                  +jdbc:mysql://+
-Oracle      10.2.0+  No                   +jdbc:oracle:*//+
-PostgreSQL  8.3+     Yes                  +jdbc:postgresql://+
-----------------------------------------------------------------
-
-Sqoop may work with older versions of the databases listed, but we have
-only tested it with the versions specified above.
-
-Even if Sqoop supports a database internally, you may still need to
-install the database vendor's JDBC driver in your +$HADOOP_HOME/lib+
-path.
-
diff --git a/src/contrib/sqoop/doc/table-import.txt b/src/contrib/sqoop/doc/table-import.txt
deleted file mode 100644
index 8ad2ccf..0000000
--- a/src/contrib/sqoop/doc/table-import.txt
+++ /dev/null
@@ -1,68 +0,0 @@
-
-////
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-////
-
-
-Importing Individual Tables
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-In addition to full-database imports, Sqoop will allow you to import
-individual tables. Instead of using +--all-tables+, specify the name of
-a particular table with the +--table+ argument:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-    --table employee_names
-----
-
-You can further specify a subset of the columns in a table by using
-the +--columns+ argument. This takes a list of column names, delimited
-by commas, with no spaces in between. e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-    --table employee_names --columns employee_id,first_name,last_name,dept_id
-----
-
-Sqoop will use a MapReduce job to read sections of the table in
-parallel. For the MapReduce tasks to divide the table space, the
-results returned by the database must be orderable. Sqoop will
-automatically detect the primary key for a table and use that to order
-the results. If no primary key is available, or (less likely) you want
-to order the results along a different column, you can specify the
-column name with +--split-by+.
-
-.Row ordering
-IMPORTANT:  To guarantee correctness of your input, you must select an
-ordering column for which each row has a unique value. If duplicate
-values appear in the ordering column, the results of the import are
-undefined, and Sqoop will not be able to detect the error.
-
-Finally, you can control which rows of a table are imported via the
-+--where+ argument. With this argument, you may specify a clause to be
-appended to the SQL statement used to select rows from the table,
-e.g.:
-
-----
-$ sqoop --connect jdbc:mysql://database.example.com/employees \
-  --table employee_names --where "employee_id > 40 AND active = 1"
-----
-
-The +--columns+, +--split-by+, and +--where+ arguments are incompatible with
-+--all-tables+. If you require special handling for some of the tables,
-then you must manually run a separate import job for each table.
-
diff --git a/src/contrib/sqoop/ivy.xml b/src/contrib/sqoop/ivy.xml
deleted file mode 100644
index 63f5649..0000000
--- a/src/contrib/sqoop/ivy.xml
+++ /dev/null
@@ -1,88 +0,0 @@
-<?xml version="1.0" ?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-  
-       http://www.apache.org/licenses/LICENSE-2.0
-    
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-<ivy-module version="1.0">
-  <info organisation="org.apache.hadoop" module="${ant.project.name}">
-    <license name="Apache 2.0"/>
-    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
-    <description>
-        Apache Hadoop
-    </description>
-  </info>
-  <configurations defaultconfmapping="default">
-    <!--these match the Maven configurations-->
-    <conf name="default" extends="master,runtime"/>
-    <conf name="master" description="contains the artifact but no dependencies"/>
-    <conf name="runtime" description="runtime but not the artifact" />
-
-    <conf name="common" visibility="private"
-      extends="runtime"
-      description="artifacts needed to compile/test the application"/>
-    <conf name="test" visibility="private" extends="runtime"/>
-  </configurations>
-
-  <publications>
-    <!--get the artifact from our module name-->
-    <artifact conf="master"/>
-  </publications>
-  <dependencies>
-    <dependency org="commons-cli"
-      name="commons-cli"
-      rev="${commons-cli.version}"
-      conf="common->default"/>
-    <dependency org="commons-logging"
-      name="commons-logging"
-      rev="${commons-logging.version}"
-      conf="common->default"/>
-    <dependency org="commons-httpclient"
-      name="commons-httpclient"
-      rev="${commons-httpclient.version}"
-      conf="common->default"/>
-    <dependency org="commons-io"
-      name="commons-io"
-      rev="${commons-io.version}"
-      conf="common->default"/>
-    <dependency org="commons-codec"
-      name="commons-codec"
-      rev="${commons-codec.version}"
-      conf="common->default"/>
-    <dependency org="junit"
-      name="junit"
-      rev="${junit.version}"
-      conf="common->default"/>
-    <dependency org="log4j"
-      name="log4j"
-      rev="${log4j.version}"
-      conf="common->master"/>
-    <dependency org="hsqldb"
-      name="hsqldb"
-      rev="${hsqldb.version}"
-      conf="common->default"/>
-    <dependency org="org.mortbay.jetty"
-      name="servlet-api-2.5"
-      rev="${servlet-api-2.5.version}"
-      conf="common->master"/>
-    <dependency org="org.mortbay.jetty"
-      name="jetty"
-      rev="${jetty.version}"
-      conf="common->master"/>
-    <dependency org="org.mortbay.jetty"
-      name="jetty-util"
-      rev="${jetty-util.version}"
-      conf="common->master"/>
-    </dependencies>
-</ivy-module>
diff --git a/src/contrib/sqoop/ivy/libraries.properties b/src/contrib/sqoop/ivy/libraries.properties
deleted file mode 100644
index 4f65a81..0000000
--- a/src/contrib/sqoop/ivy/libraries.properties
+++ /dev/null
@@ -1,21 +0,0 @@
-#   Licensed to the Apache Software Foundation (ASF) under one or more
-#   contributor license agreements.  See the NOTICE file distributed with
-#   this work for additional information regarding copyright ownership.
-#   The ASF licenses this file to You under the Apache License, Version 2.0
-#   (the "License"); you may not use this file except in compliance with
-#   the License.  You may obtain a copy of the License at
-#  
-#       http://www.apache.org/licenses/LICENSE-2.0
-#    
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-
-#This properties file lists the versions of the various artifacts used by streaming.
-#It drives ivy and the generation of a maven POM
-
-#Please list the dependencies name with version if they are different from the ones
-#listed in the global libraries.properties file (in alphabetical order)
-
diff --git a/src/contrib/sqoop/readme.txt b/src/contrib/sqoop/readme.txt
deleted file mode 100644
index 40f5fbf..0000000
--- a/src/contrib/sqoop/readme.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-Sqoop documentation is in the doc/ directory in asciidoc format.
-
-Run 'ant doc' to build the documentation. It will be created in
-$HADOOP_HOME/build/contrib/sqoop/doc.
-
-There will be a manpage (sqoop.1.gz) and a User Guide formatted in HTML.
-
-This process requires the following programs:
-  asciidoc
-  gzip
-  make
-  python 2.5+
-  xmlto
-
-For more information about asciidoc, see http://www.methods.co.nz/asciidoc/
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/ConnFactory.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/ConnFactory.java
deleted file mode 100644
index 90683dc..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/ConnFactory.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.DefaultManagerFactory;
-import org.apache.hadoop.sqoop.manager.ManagerFactory;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import java.io.IOException;
-import java.util.LinkedList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Factory class to create the ConnManager type required
- * for the current import job.
- *
- * This class delegates the actual responsibility for instantiating
- * ConnManagers to one or more instances of ManagerFactory. ManagerFactories
- * are consulted in the order specified in sqoop-site.xml (sqoop.connection.factories).
- */
-public class ConnFactory {
-
-  public static final Log LOG = LogFactory.getLog(ConnFactory.class.getName());
-
-  public ConnFactory(Configuration conf) {
-    factories = new LinkedList<ManagerFactory>();
-    instantiateFactories(conf);
-  }
-
-  /** The sqoop-site.xml configuration property used to set the list of 
-   * available ManagerFactories.
-   */
-  public final static String FACTORY_CLASS_NAMES_KEY = "sqoop.connection.factories";
-
-  // The default value for sqoop.connection.factories is the name of the DefaultManagerFactory.
-  final static String DEFAULT_FACTORY_CLASS_NAMES = DefaultManagerFactory.class.getName(); 
-
-  /** The list of ManagerFactory instances consulted by getManager().
-   */
-  private List<ManagerFactory> factories;
-
-  /**
-   * Create the ManagerFactory instances that should populate
-   * the factories list.
-   */
-  private void instantiateFactories(Configuration conf) {
-    String [] classNameArray =
-        conf.getStrings(FACTORY_CLASS_NAMES_KEY, DEFAULT_FACTORY_CLASS_NAMES);
-
-    for (String className : classNameArray) {
-      try {
-        className = className.trim(); // Ignore leading/trailing whitespace.
-        ManagerFactory factory = ReflectionUtils.newInstance(
-            (Class<ManagerFactory>) conf.getClassByName(className), conf);
-        LOG.debug("Loaded manager factory: " + className);
-        factories.add(factory);
-      } catch (ClassNotFoundException cnfe) {
-        LOG.error("Could not load ManagerFactory " + className + " (not found)");
-      }
-    }
-  }
-
-  /**
-   * Factory method to get a ConnManager for the given JDBC connect string.
-   * @param opts The parsed command-line options
-   * @return a ConnManager instance for the appropriate database
-   * @throws IOException if it cannot find a ConnManager for this schema
-   */
-  public ConnManager getManager(SqoopOptions opts) throws IOException {
-    // Try all the available manager factories.
-    for (ManagerFactory factory : factories) {
-      LOG.debug("Trying ManagerFactory: " + factory.getClass().getName());
-      ConnManager mgr = factory.accept(opts);
-      if (null != mgr) {
-        LOG.debug("Instantiated ConnManager.");
-        return mgr;
-      }
-    }
-
-    throw new IOException("No manager for connect string: " + opts.getConnectString());
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/Sqoop.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/Sqoop.java
deleted file mode 100644
index bfbf4f9..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/Sqoop.java
+++ /dev/null
@@ -1,274 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-
-import org.apache.hadoop.sqoop.hive.HiveImport;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.ExportJobContext;
-import org.apache.hadoop.sqoop.manager.ImportJobContext;
-import org.apache.hadoop.sqoop.orm.ClassWriter;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.util.ExportException;
-import org.apache.hadoop.sqoop.util.ImportException;
-
-/**
- * Main entry-point for Sqoop
- * Usage: hadoop jar (this_jar_name) org.apache.hadoop.sqoop.Sqoop (options)
- * See the SqoopOptions class for options.
- */
-public class Sqoop extends Configured implements Tool {
-
-  public static final Log LOG = LogFactory.getLog(Sqoop.class.getName());
-
-  /** If this System property is set, always throw an exception, do not just
-      exit with status 1.
-    */
-  public static final String SQOOP_RETHROW_PROPERTY = "sqoop.throwOnError";
-
-  static {
-    Configuration.addDefaultResource("sqoop-default.xml");
-    Configuration.addDefaultResource("sqoop-site.xml");
-  }
-
-  private SqoopOptions options;
-  private ConnManager manager;
-  private HiveImport hiveImport;
-  private List<String> generatedJarFiles;
-
-  public Sqoop() {
-    init();
-  }
-
-  public Sqoop(Configuration conf) {
-    init();
-    setConf(conf);
-  }
-
-  private void init() {
-    generatedJarFiles = new ArrayList<String>();
-  }
-
-  public SqoopOptions getOptions() {
-    return options;
-  }
-
-  /**
-   * @return a list of jar files generated as part of this im/export process
-   */
-  public List<String> getGeneratedJarFiles() {
-    ArrayList<String> out = new ArrayList<String>(generatedJarFiles);
-    return out;
-  }
-
-  /**
-   * Generate the .class and .jar files
-   * @return the filename of the emitted jar file.
-   * @throws IOException
-   */
-  private String generateORM(String tableName) throws IOException {
-    String existingJar = options.getExistingJarName();
-    if (existingJar != null) {
-      // The user has pre-specified a jar and class to use. Don't generate.
-      LOG.info("Using existing jar: " + existingJar);
-      return existingJar;
-    }
-
-    LOG.info("Beginning code generation");
-    CompilationManager compileMgr = new CompilationManager(options);
-    ClassWriter classWriter = new ClassWriter(options, manager, tableName, compileMgr);
-    classWriter.generate();
-    compileMgr.compile();
-    compileMgr.jar();
-    String jarFile = compileMgr.getJarFilename();
-    this.generatedJarFiles.add(jarFile);
-    return jarFile;
-  }
-
-  private void importTable(String tableName) throws IOException, ImportException {
-    String jarFile = null;
-
-    // Generate the ORM code for the tables.
-    jarFile = generateORM(tableName);
-
-    if (options.getAction() == SqoopOptions.ControlAction.FullImport) {
-      // check if data import is to be performed
-      if (!options.doCreateHiveTableOnly()) {
-        // Proceed onward to do the import.
-        ImportJobContext context = new ImportJobContext(tableName, jarFile, options);
-        manager.importTable(context);
-      }
-
-      // If the user wants this table to be in Hive, perform that post-load.
-      if (options.doHiveImport()) {
-        hiveImport.importTable(tableName, options.getHiveTableName());
-      }
-    }
-  }
-
-  private void exportTable(String tableName) throws ExportException, IOException {
-    String jarFile = null;
-
-    // Generate the ORM code for the tables.
-    jarFile = generateORM(tableName);
-
-    ExportJobContext context = new ExportJobContext(tableName, jarFile, options);
-    manager.exportTable(context);
-  }
-
-  /**
-   * Actual main entry-point for the program
-   */
-  public int run(String [] args) {
-    options = new SqoopOptions();
-    options.setConf(getConf());
-    try {
-      options.parse(args);
-      options.validate();
-    } catch (SqoopOptions.InvalidOptionsException e) {
-      // display the error msg
-      System.err.println(e.getMessage());
-      return 1; // exit on exception here
-    }
-
-    // Get the connection to the database
-    try {
-      manager = new ConnFactory(getConf()).getManager(options);
-    } catch (Exception e) {
-      LOG.error("Got error creating database manager: " + e.toString());
-      if (System.getProperty(SQOOP_RETHROW_PROPERTY) != null) {
-        throw new RuntimeException(e);
-      } else {
-        return 1;
-      }
-    }
-
-    if (options.doHiveImport()) {
-      hiveImport = new HiveImport(options, manager, getConf());
-    }
-
-    SqoopOptions.ControlAction action = options.getAction();
-    if (action == SqoopOptions.ControlAction.ListTables) {
-      String [] tables = manager.listTables();
-      if (null == tables) {
-        System.err.println("Could not retrieve tables list from server");
-        LOG.error("manager.listTables() returned null");
-        return 1;
-      } else {
-        for (String tbl : tables) {
-          System.out.println(tbl);
-        }
-      }
-    } else if (action == SqoopOptions.ControlAction.ListDatabases) {
-      String [] databases = manager.listDatabases();
-      if (null == databases) {
-        System.err.println("Could not retrieve database list from server");
-        LOG.error("manager.listDatabases() returned null");
-        return 1;
-      } else {
-        for (String db : databases) {
-          System.out.println(db);
-        }
-      }
-    } else if (action == SqoopOptions.ControlAction.DebugExec) {
-      // just run a SQL statement for debugging purposes.
-      manager.execAndPrint(options.getDebugSqlCmd());
-      return 0;
-    } else if (action == SqoopOptions.ControlAction.Export) {
-      // Export a table.
-      try {
-        exportTable(options.getTableName());
-      } catch (IOException ioe) {
-        LOG.error("Encountered IOException running export job: " + ioe.toString());
-        if (System.getProperty(SQOOP_RETHROW_PROPERTY) != null) {
-          throw new RuntimeException(ioe);
-        } else {
-          return 1;
-        }
-      } catch (ExportException ee) {
-        LOG.error("Error during export: " + ee.toString());
-        if (System.getProperty(SQOOP_RETHROW_PROPERTY) != null) {
-          throw new RuntimeException(ee);
-        } else {
-          return 1;
-        }
-      }
-    } else {
-      // This is either FullImport or GenerateOnly.
-
-      try {
-        if (options.isAllTables()) {
-          String [] tables = manager.listTables();
-          if (null == tables) {
-            System.err.println("Could not retrieve tables list from server");
-            LOG.error("manager.listTables() returned null");
-            return 1;
-          } else {
-            for (String tableName : tables) {
-              importTable(tableName);
-            }
-          }
-        } else {
-          // just import a single table the user specified.
-          importTable(options.getTableName());
-        }
-      } catch (IOException ioe) {
-        LOG.error("Encountered IOException running import job: " + ioe.toString());
-        if (System.getProperty(SQOOP_RETHROW_PROPERTY) != null) {
-          throw new RuntimeException(ioe);
-        } else {
-          return 1;
-        }
-      } catch (ImportException ie) {
-        LOG.error("Error during import: " + ie.toString());
-        if (System.getProperty(SQOOP_RETHROW_PROPERTY) != null) {
-          throw new RuntimeException(ie);
-        } else {
-          return 1;
-        }
-      }
-    }
-
-    return 0;
-  }
-
-  public static void main(String [] args) {
-    int ret;
-    try {
-      Sqoop importer = new Sqoop();
-      ret = ToolRunner.run(importer, args);
-    } catch (Exception e) {
-      LOG.error("Got exception running Sqoop: " + e.toString());
-      e.printStackTrace();
-      ret = 1;
-    }
-
-    System.exit(ret);
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/SqoopOptions.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/SqoopOptions.java
deleted file mode 100644
index c09ea0c..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/SqoopOptions.java
+++ /dev/null
@@ -1,1024 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.hadoop.sqoop;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.Properties;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.log4j.Category;
-import org.apache.log4j.Level;
-import org.apache.log4j.Logger;
-
-/**
- * Command-line arguments used by Sqoop
- */
-public class SqoopOptions {
-
-  public static final Log LOG = LogFactory.getLog(SqoopOptions.class.getName());
-
-  /**
-   * Thrown when invalid cmdline options are given
-   */
-  @SuppressWarnings("serial")
-  public static class InvalidOptionsException extends Exception {
-
-    private String message;
-
-    public InvalidOptionsException(final String msg) {
-      this.message = msg;
-    }
-
-    public String getMessage() {
-      return message;
-    }
-
-    public String toString() {
-      return getMessage();
-    }
-  }
-
-  // control-flow selector based on command-line switches.
-  public enum ControlAction {
-    ListDatabases,  // list available databases and exit.
-    ListTables,     // list available tables and exit.
-    GenerateOnly,   // generate ORM code but do not import.
-    FullImport,     // generate code (as needed) and import.
-    DebugExec,      // just execute a single sql command and print its results.
-    Export          // export a table from HDFS to a database.
-  }
-
-  // selects in-HDFS destination file format
-  public enum FileLayout {
-    TextFile,
-    SequenceFile
-  }
-
-
-  // TODO(aaron): Adding something here? Add a getter, a cmdline switch, and a properties file
-  // entry in loadFromProperties(). Add a default value in initDefaults() if you need one.
-  // Make sure you add the stub to the testdata/sqoop.properties.template file.
-  private String connectString;
-  private String tableName;
-  private String [] columns;
-  private boolean allTables;
-  private String username;
-  private String password;
-  private String codeOutputDir;
-  private String jarOutputDir;
-  private ControlAction action;
-  private String hadoopHome;
-  private String splitByCol;
-  private String whereClause;
-  private String debugSqlCmd;
-  private String driverClassName;
-  private String warehouseDir;
-  private FileLayout layout;
-  private boolean direct; // if true and conn is mysql, use mysqldump.
-  private String tmpDir; // where temp data goes; usually /tmp
-  private String hiveHome;
-  private boolean hiveImport;
-  private boolean createHiveTableOnly;
-  private boolean overwriteHiveTable;
-  private String hiveTableName;
-  private String packageName; // package to prepend to auto-named classes.
-  private String className; // package+class to apply to individual table import.
-                            // also used as an *input* class with existingJarFile.
-  private String existingJarFile; // Name of a jar containing existing table definition
-                                  // class to use.
-  private int numMappers;
-  private boolean useCompression;
-  private long directSplitSize; // In direct mode, open a new stream every X bytes.
-
-  private String exportDir; // HDFS path to read from when performing an export
-
-  private char inputFieldDelim;
-  private char inputRecordDelim;
-  private char inputEnclosedBy;
-  private char inputEscapedBy;
-  private boolean inputMustBeEnclosed;
-
-  private char outputFieldDelim;
-  private char outputRecordDelim;
-  private char outputEnclosedBy;
-  private char outputEscapedBy;
-  private boolean outputMustBeEnclosed;
-
-  private boolean areDelimsManuallySet;
-
-  private Configuration conf;
-
-  public static final int DEFAULT_NUM_MAPPERS = 4;
-
-  private static final String DEFAULT_CONFIG_FILE = "sqoop.properties";
-
-  private String [] extraArgs;
-
-  public SqoopOptions() {
-    initDefaults(null);
-  }
-
-  public SqoopOptions(Configuration conf) {
-    initDefaults(conf);
-  }
-
-  /**
-   * Alternate SqoopOptions interface used mostly for unit testing
-   * @param connect JDBC connect string to use
-   * @param database Database to read
-   * @param table Table to read
-   */
-  public SqoopOptions(final String connect, final String table) {
-    initDefaults(null);
-
-    this.connectString = connect;
-    this.tableName = table;
-  }
-
-  private boolean getBooleanProperty(Properties props, String propName, boolean defaultValue) {
-    String str = props.getProperty(propName,
-        Boolean.toString(defaultValue)).toLowerCase();
-    return "true".equals(str) || "yes".equals(str) || "1".equals(str);
-  }
-
-  private long getLongProperty(Properties props, String propName, long defaultValue) {
-    String str = props.getProperty(propName,
-        Long.toString(defaultValue)).toLowerCase();
-    try {
-      return Long.parseLong(str);
-    } catch (NumberFormatException nfe) {
-      LOG.warn("Could not parse integer value for config parameter " + propName);
-      return defaultValue;
-    }
-  }
-
-  private void loadFromProperties() {
-    File configFile = new File(DEFAULT_CONFIG_FILE);
-    if (!configFile.canRead()) {
-      return; //can't do this.
-    }
-
-    Properties props = new Properties();
-    InputStream istream = null;
-    try {
-      LOG.info("Loading properties from " + configFile.getAbsolutePath());
-      istream = new FileInputStream(configFile);
-      props.load(istream);
-
-      this.hadoopHome = props.getProperty("hadoop.home", this.hadoopHome);
-      this.codeOutputDir = props.getProperty("out.dir", this.codeOutputDir);
-      this.jarOutputDir = props.getProperty("bin.dir", this.jarOutputDir);
-      this.username = props.getProperty("db.username", this.username);
-      this.password = props.getProperty("db.password", this.password);
-      this.tableName = props.getProperty("db.table", this.tableName);
-      this.connectString = props.getProperty("db.connect.url", this.connectString);
-      this.splitByCol = props.getProperty("db.split.column", this.splitByCol);
-      this.whereClause = props.getProperty("db.where.clause", this.whereClause);
-      this.driverClassName = props.getProperty("jdbc.driver", this.driverClassName);
-      this.warehouseDir = props.getProperty("hdfs.warehouse.dir", this.warehouseDir);
-      this.hiveHome = props.getProperty("hive.home", this.hiveHome);
-      this.className = props.getProperty("java.classname", this.className);
-      this.packageName = props.getProperty("java.packagename", this.packageName);
-      this.existingJarFile = props.getProperty("java.jar.file", this.existingJarFile);
-      this.exportDir = props.getProperty("export.dir", this.exportDir);
-
-      this.direct = getBooleanProperty(props, "direct.import", this.direct);
-      this.hiveImport = getBooleanProperty(props, "hive.import", this.hiveImport);
-      this.createHiveTableOnly = getBooleanProperty(props, "hive.create.table.only", this.createHiveTableOnly);
-      this.overwriteHiveTable = getBooleanProperty(props, "hive.overwrite.table", this.overwriteHiveTable);
-      this.useCompression = getBooleanProperty(props, "compression", this.useCompression);
-      this.directSplitSize = getLongProperty(props, "direct.split.size",
-          this.directSplitSize);
-    } catch (IOException ioe) {
-      LOG.error("Could not read properties file " + DEFAULT_CONFIG_FILE + ": " + ioe.toString());
-    } finally {
-      if (null != istream) {
-        try {
-          istream.close();
-        } catch (IOException ioe) {
-          // ignore this; we're closing.
-        }
-      }
-    }
-  }
-
-  /**
-   * @return the temp directory to use; this is guaranteed to end with
-   * the file separator character (e.g., '/')
-   */
-  public String getTempDir() {
-    return this.tmpDir;
-  }
-
-  private void initDefaults(Configuration baseConfiguration) {
-    // first, set the true defaults if nothing else happens.
-    // default action is to run the full pipeline.
-    this.action = ControlAction.FullImport;
-    this.hadoopHome = System.getenv("HADOOP_HOME");
-
-    // Set this with $HIVE_HOME, but -Dhive.home can override.
-    this.hiveHome = System.getenv("HIVE_HOME");
-    this.hiveHome = System.getProperty("hive.home", this.hiveHome);
-
-    // Set this to cwd, but -Dsqoop.src.dir can override.
-    this.codeOutputDir = System.getProperty("sqoop.src.dir", ".");
-
-    String myTmpDir = System.getProperty("test.build.data", "/tmp/");
-    if (!myTmpDir.endsWith(File.separator)) {
-      myTmpDir = myTmpDir + File.separator;
-    }
-
-    this.tmpDir = myTmpDir;
-    this.jarOutputDir = tmpDir + "sqoop/compile";
-    this.layout = FileLayout.TextFile;
-
-    this.inputFieldDelim = '\000';
-    this.inputRecordDelim = '\000';
-    this.inputEnclosedBy = '\000';
-    this.inputEscapedBy = '\000';
-    this.inputMustBeEnclosed = false;
-
-    this.outputFieldDelim = ',';
-    this.outputRecordDelim = '\n';
-    this.outputEnclosedBy = '\000';
-    this.outputEscapedBy = '\000';
-    this.outputMustBeEnclosed = false;
-
-    this.areDelimsManuallySet = false;
-
-    this.numMappers = DEFAULT_NUM_MAPPERS;
-    this.useCompression = false;
-    this.directSplitSize = 0;
-
-    if (null == baseConfiguration) {
-      this.conf = new Configuration();
-    } else {
-      this.conf = new Configuration(baseConfiguration);
-    }
-
-    this.extraArgs = null;
-
-    loadFromProperties();
-  }
-
-  /**
-   * Allow the user to enter his password on the console without printing characters.
-   * @return the password as a string
-   */
-  private String securePasswordEntry() {
-    return new String(System.console().readPassword("Enter password: "));
-  }
-
-  /**
-   * Print usage strings for the program's arguments.
-   */
-  public static void printUsage() {
-    System.out.println("Usage: hadoop sqoop.jar org.apache.hadoop.sqoop.Sqoop (options)");
-    System.out.println("");
-    System.out.println("Database connection options:");
-    System.out.println("--connect (jdbc-uri)         Specify JDBC connect string");
-    System.out.println("--driver (class-name)        Manually specify JDBC driver class to use");
-    System.out.println("--username (username)        Set authentication username");
-    System.out.println("--password (password)        Set authentication password");
-    System.out.println("-P                           Read password from console");
-    System.out.println("--direct                     Use direct import fast path (mysql only)");
-    System.out.println("");
-    System.out.println("Import control options:");
-    System.out.println("--table (tablename)          Table to read");
-    System.out.println("--columns (col,col,col...)   Columns to export from table");
-    System.out.println("--split-by (column-name)     Column of the table used to split work units");
-    System.out.println("--where (where clause)       Where clause to use during export");
-    System.out.println("--hadoop-home (dir)          Override $HADOOP_HOME");
-    System.out.println("--hive-home (dir)            Override $HIVE_HOME");
-    System.out.println("--warehouse-dir (dir)        HDFS path for table destination");
-    System.out.println("--as-sequencefile            Imports data to SequenceFiles");
-    System.out.println("--as-textfile                Imports data as plain text (default)");
-    System.out.println("--all-tables                 Import all tables in database");
-    System.out.println("                             (Ignores --table, --columns and --split-by)");
-    System.out.println("--hive-import                If set, then import the table into Hive.");
-    System.out.println("                    (Uses Hive's default delimiters if none are set.)");
-    System.out.println("--hive-table (tablename)     Sets the table name to use when importing");
-    System.out.println("                             to hive.");
-    System.out.println("-m, --num-mappers (n)        Use 'n' map tasks to import in parallel");
-    System.out.println("-z, --compress               Enable compression");
-    System.out.println("--direct-split-size (n)      Split the input stream every 'n' bytes");
-    System.out.println("                             when importing in direct mode.");
-    System.out.println("");
-    System.out.println("Export options:");
-    System.out.println("--export-dir (dir)           Export from an HDFS path into a table");
-    System.out.println("                             (set with --table)");
-    System.out.println("");
-    System.out.println("Output line formatting options:");
-    System.out.println("--fields-terminated-by (char)    Sets the field separator character");
-    System.out.println("--lines-terminated-by (char)     Sets the end-of-line character");
-    System.out.println("--optionally-enclosed-by (char)  Sets a field enclosing character");
-    System.out.println("--enclosed-by (char)             Sets a required field enclosing char");
-    System.out.println("--escaped-by (char)              Sets the escape character");
-    System.out.println("--mysql-delimiters               Uses MySQL's default delimiter set");
-    System.out.println("  fields: ,  lines: \\n  escaped-by: \\  optionally-enclosed-by: '");
-    System.out.println("");
-    System.out.println("Input parsing options:");
-    System.out.println("--input-fields-terminated-by (char)    Sets the input field separator");
-    System.out.println("--input-lines-terminated-by (char)     Sets the input end-of-line char");
-    System.out.println("--input-optionally-enclosed-by (char)  Sets a field enclosing character");
-    System.out.println("--input-enclosed-by (char)             Sets a required field encloser");
-    System.out.println("--input-escaped-by (char)              Sets the input escape character");
-    System.out.println("");
-    System.out.println("Code generation options:");
-    System.out.println("--outdir (dir)               Output directory for generated code");
-    System.out.println("--bindir (dir)               Output directory for compiled objects");
-    System.out.println("--generate-only              Stop after code generation; do not import");
-    System.out.println("--package-name (name)        Put auto-generated classes in this package");
-    System.out.println("--class-name (name)          When generating one class, use this name.");
-    System.out.println("                             This overrides --package-name.");
-    System.out.println("");
-    System.out.println("Library loading options:");
-    System.out.println("--jar-file (file)            Disable code generation; use specified jar");
-    System.out.println("--class-name (name)          The class within the jar that represents");
-    System.out.println("                             the table to import/export");
-    System.out.println("");
-    System.out.println("Additional commands:");
-    System.out.println("--list-tables                List tables in database and exit");
-    System.out.println("--list-databases             List all databases available and exit");
-    System.out.println("--debug-sql (statement)      Execute 'statement' in SQL and exit");
-    System.out.println("--verbose                    Print more information while working");
-    System.out.println("");
-    System.out.println("Database-specific options:");
-    System.out.println("Arguments may be passed to the database manager after a lone '-':");
-    System.out.println("  MySQL direct mode: arguments passed directly to mysqldump");
-    System.out.println("");
-    System.out.println("Generic Hadoop command-line options:");
-    ToolRunner.printGenericCommandUsage(System.out);
-    System.out.println("");
-    System.out.println("At minimum, you must specify --connect "
-        + "and either --table or --all-tables.");
-    System.out.println("Alternatively, you can specify --generate-only or one of the additional");
-    System.out.println("commands.");
-  }
-
-  /**
-   * Given a string containing a single character or an escape sequence representing
-   * a char, return that char itself.
-   *
-   * Normal literal characters return themselves: "x" -&gt; 'x', etc.
-   * Strings containing a '\' followed by one of t, r, n, or b escape to the usual
-   * character as seen in Java: "\n" -&gt; (newline), etc.
-   *
-   * Strings like "\0ooo" return the character specified by the octal sequence 'ooo'
-   * Strings like "\0xhhh" or "\0Xhhh" return the character specified by the hex sequence 'hhh'
-   */
-  static char toChar(String charish) throws InvalidOptionsException {
-    if (null == charish) {
-      throw new InvalidOptionsException("Character argument expected." 
-          + "\nTry --help for usage instructions.");
-    } else if (charish.startsWith("\\0x") || charish.startsWith("\\0X")) {
-      if (charish.length() == 3) {
-        throw new InvalidOptionsException("Base-16 value expected for character argument."
-          + "\nTry --help for usage instructions.");
-      } else {
-        String valStr = charish.substring(3);
-        int val = Integer.parseInt(valStr, 16);
-        return (char) val;
-      }
-    } else if (charish.startsWith("\\0")) {
-      if (charish.equals("\\0")) {
-        // it's just '\0', which we can take as shorthand for nul.
-        return '\000';
-      } else {
-        // it's an octal value.
-        String valStr = charish.substring(2);
-        int val = Integer.parseInt(valStr, 8);
-        return (char) val;
-      }
-    } else if (charish.startsWith("\\")) {
-      if (charish.length() == 1) {
-        // it's just a '\'. Keep it literal.
-        return '\\';
-      } else if (charish.length() > 2) {
-        // we don't have any 3+ char escape strings. 
-        throw new InvalidOptionsException("Cannot understand character argument: " + charish
-            + "\nTry --help for usage instructions.");
-      } else {
-        // this is some sort of normal 1-character escape sequence.
-        char escapeWhat = charish.charAt(1);
-        switch(escapeWhat) {
-        case 'b':
-          return '\b';
-        case 'n':
-          return '\n';
-        case 'r':
-          return '\r';
-        case 't':
-          return '\t';
-        case '\"':
-          return '\"';
-        case '\'':
-          return '\'';
-        case '\\':
-          return '\\';
-        default:
-          throw new InvalidOptionsException("Cannot understand character argument: " + charish
-              + "\nTry --help for usage instructions.");
-        }
-      }
-    } else if (charish.length() == 0) {
-      throw new InvalidOptionsException("Character argument expected." 
-          + "\nTry --help for usage instructions.");
-    } else {
-      // it's a normal character.
-      if (charish.length() > 1) {
-        LOG.warn("Character argument " + charish + " has multiple characters; "
-            + "only the first will be used.");
-      }
-
-      return charish.charAt(0);
-    }
-  }
-
-  /**
-   * Read args from the command-line into member fields.
-   * @throws Exception if there's a problem parsing arguments.
-   */
-  public void parse(String [] args) throws InvalidOptionsException {
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Parsing sqoop arguments:");
-      for (String arg : args) {
-        LOG.debug("  " + arg);
-      }
-    }
-
-    int i = 0;
-    try {
-      for (i = 0; i < args.length; i++) {
-        if (args[i].equals("--connect")) {
-          this.connectString = args[++i];
-        } else if (args[i].equals("--driver")) {
-          this.driverClassName = args[++i];
-        } else if (args[i].equals("--table")) {
-          this.tableName = args[++i];
-        } else if (args[i].equals("--columns")) {
-          String columnString = args[++i];
-          this.columns = columnString.split(",");
-        } else if (args[i].equals("--split-by")) {
-          this.splitByCol = args[++i];
-        } else if (args[i].equals("--where")) {
-          this.whereClause = args[++i];
-        } else if (args[i].equals("--list-tables")) {
-          this.action = ControlAction.ListTables;
-        } else if (args[i].equals("--all-tables")) {
-          this.allTables = true;
-        } else if (args[i].equals("--export-dir")) {
-          this.exportDir = args[++i];
-          this.action = ControlAction.Export;
-        } else if (args[i].equals("--local")) {
-          // TODO(aaron): Remove this after suitable deprecation time period.
-          LOG.warn("--local is deprecated; use --direct instead.");
-          this.direct = true;
-        } else if (args[i].equals("--direct")) {
-          this.direct = true;
-        } else if (args[i].equals("--username")) {
-          this.username = args[++i];
-          if (null == this.password) {
-            // Set password to empty if the username is set first,
-            // to ensure that they're either both null or neither.
-            this.password = "";
-          }
-        } else if (args[i].equals("--password")) {
-          LOG.warn("Setting your password on the command-line is insecure. "
-              + "Consider using -P instead.");
-          this.password = args[++i];
-        } else if (args[i].equals("-P")) {
-          this.password = securePasswordEntry();
-        } else if (args[i].equals("--hadoop-home")) {
-          this.hadoopHome = args[++i];
-        } else if (args[i].equals("--hive-home")) {
-          this.hiveHome = args[++i];
-        } else if (args[i].equals("--hive-import")) {
-          this.hiveImport = true;
-        } else if (args[i].equals("--hive-create-only")) {
-          this.createHiveTableOnly = true;
-        } else if (args[i].equals("--hive-overwrite")) {
-          this.overwriteHiveTable = true;
-        } else if (args[i].equals("--hive-table")) {
-          this.hiveTableName = args[++i];
-        } else if (args[i].equals("--num-mappers") || args[i].equals("-m")) {
-          String numMappersStr = args[++i];
-          this.numMappers = Integer.valueOf(numMappersStr);
-        } else if (args[i].equals("--fields-terminated-by")) {
-          this.outputFieldDelim = SqoopOptions.toChar(args[++i]);
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--lines-terminated-by")) {
-          this.outputRecordDelim = SqoopOptions.toChar(args[++i]);
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--optionally-enclosed-by")) {
-          this.outputEnclosedBy = SqoopOptions.toChar(args[++i]);
-          this.outputMustBeEnclosed = false;
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--enclosed-by")) {
-          this.outputEnclosedBy = SqoopOptions.toChar(args[++i]);
-          this.outputMustBeEnclosed = true;
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--escaped-by")) {
-          this.outputEscapedBy = SqoopOptions.toChar(args[++i]);
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--mysql-delimiters")) {
-          this.outputFieldDelim = ',';
-          this.outputRecordDelim = '\n';
-          this.outputEnclosedBy = '\'';
-          this.outputEscapedBy = '\\';
-          this.outputMustBeEnclosed = false;
-          this.areDelimsManuallySet = true;
-        } else if (args[i].equals("--input-fields-terminated-by")) {
-          this.inputFieldDelim = SqoopOptions.toChar(args[++i]);
-        } else if (args[i].equals("--input-lines-terminated-by")) {
-          this.inputRecordDelim = SqoopOptions.toChar(args[++i]);
-        } else if (args[i].equals("--input-optionally-enclosed-by")) {
-          this.inputEnclosedBy = SqoopOptions.toChar(args[++i]);
-          this.inputMustBeEnclosed = false;
-        } else if (args[i].equals("--input-enclosed-by")) {
-          this.inputEnclosedBy = SqoopOptions.toChar(args[++i]);
-          this.inputMustBeEnclosed = true;
-        } else if (args[i].equals("--input-escaped-by")) {
-          this.inputEscapedBy = SqoopOptions.toChar(args[++i]);
-        } else if (args[i].equals("--outdir")) {
-          this.codeOutputDir = args[++i];
-        } else if (args[i].equals("--as-sequencefile")) {
-          this.layout = FileLayout.SequenceFile;
-        } else if (args[i].equals("--as-textfile")) {
-          this.layout = FileLayout.TextFile;
-        } else if (args[i].equals("--bindir")) {
-          this.jarOutputDir = args[++i];
-        } else if (args[i].equals("--warehouse-dir")) {
-          this.warehouseDir = args[++i];
-        } else if (args[i].equals("--package-name")) {
-          this.packageName = args[++i];
-        } else if (args[i].equals("--class-name")) {
-          this.className = args[++i];
-        } else if (args[i].equals("-z") || args[i].equals("--compress")) {
-          this.useCompression = true;
-        } else if (args[i].equals("--direct-split-size")) {
-          this.directSplitSize = Long.parseLong(args[++i]);
-        } else if (args[i].equals("--jar-file")) {
-          this.existingJarFile = args[++i];
-        } else if (args[i].equals("--list-databases")) {
-          this.action = ControlAction.ListDatabases;
-        } else if (args[i].equals("--generate-only")) {
-          this.action = ControlAction.GenerateOnly;
-        } else if (args[i].equals("--debug-sql")) {
-          this.action = ControlAction.DebugExec;
-          // read the entire remainder of the commandline into the debug sql statement.
-          if (null == this.debugSqlCmd) {
-            this.debugSqlCmd = "";
-          }
-          for (i++; i < args.length; i++) {
-            this.debugSqlCmd = this.debugSqlCmd + args[i] + " ";
-          }
-        } else if (args[i].equals("--verbose")) {
-          // Immediately switch into DEBUG logging.
-          Category sqoopLogger =
-              Logger.getLogger(SqoopOptions.class.getName()).getParent();
-          sqoopLogger.setLevel(Level.DEBUG);
-
-        } else if (args[i].equals("--help")) {
-          printUsage();
-          throw new InvalidOptionsException("");
-        } else if (args[i].equals("-")) {
-          // Everything after a '--' goes into extraArgs.
-          ArrayList<String> extra = new ArrayList<String>();
-          for (i++; i < args.length; i++) {
-            extra.add(args[i]);
-          }
-          this.extraArgs = extra.toArray(new String[0]);
-        } else {
-          throw new InvalidOptionsException("Invalid argument: " + args[i] + ".\n"
-              + "Try --help for usage.");
-        }
-      }
-    } catch (ArrayIndexOutOfBoundsException oob) {
-      throw new InvalidOptionsException("Error: " + args[--i] + " expected argument.\n"
-          + "Try --help for usage.");
-    } catch (NumberFormatException nfe) {
-      throw new InvalidOptionsException("Error: " + args[--i] + " expected numeric argument.\n"
-          + "Try --help for usage.");
-    }
-  }
-
-  private static final String HELP_STR = "\nTry --help for usage instructions.";
-
-  /**
-   * Validates options and ensures that any required options are
-   * present and that any mutually-exclusive options are not selected.
-   * @throws Exception if there's a problem.
-   */
-  public void validate() throws InvalidOptionsException {
-    if (this.allTables && this.columns != null) {
-      // If we're reading all tables in a database, can't filter column names.
-      throw new InvalidOptionsException("--columns and --all-tables are incompatible options."
-          + HELP_STR);
-    } else if (this.allTables && this.splitByCol != null) {
-      // If we're reading all tables in a database, can't set pkey
-      throw new InvalidOptionsException("--split-by and --all-tables are incompatible options."
-          + HELP_STR);
-    } else if (this.allTables && this.className != null) {
-      // If we're reading all tables, can't set individual class name
-      throw new InvalidOptionsException("--class-name and --all-tables are incompatible options."
-          + HELP_STR);
-    } else if (this.allTables && this.hiveTableName != null) {
-      // If we're reading all tables, can't set hive target table name
-      throw new InvalidOptionsException(
-          "--hive-table and --all-tables are incompatible options."
-          + HELP_STR);
-    } else if (this.hiveTableName != null && !this.hiveImport) {
-      throw new InvalidOptionsException(
-          "--hive-table is invalid without --hive-import"
-          + HELP_STR);
-    } else if (this.connectString == null) {
-      throw new InvalidOptionsException("Error: Required argument --connect is missing."
-          + HELP_STR);
-    } else if (this.className != null && this.packageName != null) {
-      throw new InvalidOptionsException(
-          "--class-name overrides --package-name. You cannot use both." + HELP_STR);
-    } else if (this.action == ControlAction.FullImport && !this.allTables
-        && this.tableName == null) {
-      throw new InvalidOptionsException(
-          "One of --table or --all-tables is required for import." + HELP_STR);
-    } else if (this.action == ControlAction.Export && this.allTables) {
-      throw new InvalidOptionsException("You cannot export with --all-tables." + HELP_STR);
-    } else if (this.action == ControlAction.Export && this.tableName == null) {
-      throw new InvalidOptionsException("Export requires a --table argument." + HELP_STR);
-    } else if (this.existingJarFile != null && this.className == null) {
-      throw new InvalidOptionsException("Jar specified with --jar-file, but no "
-          + "class specified with --class-name." + HELP_STR);
-    } else if (this.existingJarFile != null && this.action == ControlAction.GenerateOnly) {
-      throw new InvalidOptionsException("Cannot generate code using existing jar." + HELP_STR);
-    }
-
-    if (this.hiveImport) {
-      if (!areDelimsManuallySet) {
-        // user hasn't manually specified delimiters, and wants to import straight to Hive.
-        // Use Hive-style delimiters.
-        LOG.info("Using Hive-specific delimiters for output. You can override");
-        LOG.info("delimiters with --fields-terminated-by, etc.");
-        this.outputFieldDelim = (char)0x1; // ^A
-        this.outputRecordDelim = '\n';
-        this.outputEnclosedBy = '\000'; // no enclosing in Hive.
-        this.outputEscapedBy = '\000'; // no escaping in Hive
-        this.outputMustBeEnclosed = false;
-      }
-
-      if (this.getOutputEscapedBy() != '\000') {
-        LOG.warn("Hive does not support escape characters in fields;");
-        LOG.warn("parse errors in Hive may result from using --escaped-by.");
-      }
-
-      if (this.getOutputEnclosedBy() != '\000') {
-        LOG.warn("Hive does not support quoted strings; parse errors");
-        LOG.warn("in Hive may result from using --enclosed-by.");
-      }
-    }
-  }
-
-  /** get the temporary directory; guaranteed to end in File.separator
-   * (e.g., '/')
-   */
-  public String getTmpDir() {
-    return tmpDir;
-  }
-
-  public String getConnectString() {
-    return connectString;
-  }
-
-  public void setConnectString(String connectStr) {
-    this.connectString = connectStr;
-  }
-
-  public String getTableName() {
-    return tableName;
-  }
-
-  public String getExportDir() {
-    return exportDir;
-  }
-
-  public String getExistingJarName() {
-    return existingJarFile;
-  }
-
-  public String[] getColumns() {
-    if (null == columns) {
-      return null;
-    } else {
-      return Arrays.copyOf(columns, columns.length);
-    }
-  }
-
-  public String getSplitByCol() {
-    return splitByCol;
-  }
-  
-  public String getWhereClause() {
-    return whereClause;
-  }
-
-  public ControlAction getAction() {
-    return action;
-  }
-
-  public boolean isAllTables() {
-    return allTables;
-  }
-
-  public String getUsername() {
-    return username;
-  }
-
-  public String getPassword() {
-    return password;
-  }
-
-  public boolean isDirect() {
-    return direct;
-  }
-
-  /**
-   * @return the number of map tasks to use for import
-   */
-  public int getNumMappers() {
-    return this.numMappers;
-  }
-
-  /**
-   * @return the user-specified absolute class name for the table
-   */
-  public String getClassName() {
-    return className;
-  }
-
-  /**
-   * @return the user-specified package to prepend to table names via --package-name.
-   */
-  public String getPackageName() {
-    return packageName;
-  }
-
-  public String getHiveHome() {
-    return hiveHome;
-  }
-
-  /** @return true if we should import the table into Hive */
-  public boolean doHiveImport() {
-    return hiveImport;
-  }
-
-  /**
-   * @return the user-specified option to create tables in hive with no loading
-   */
-  public boolean doCreateHiveTableOnly() {
-    return createHiveTableOnly;
-  }
-
-  /**
-   * @return the user-specified option to overwrite existing table in hive
-   */
-  public boolean doOverwriteHiveTable() {
-    return overwriteHiveTable;
-  }
-
-  /**
-   * @return location where .java files go; guaranteed to end with '/'
-   */
-  public String getCodeOutputDir() {
-    if (codeOutputDir.endsWith(File.separator)) {
-      return codeOutputDir;
-    } else {
-      return codeOutputDir + File.separator;
-    }
-  }
-
-  /**
-   * @return location where .jar and .class files go; guaranteed to end with '/'
-   */
-  public String getJarOutputDir() {
-    if (jarOutputDir.endsWith(File.separator)) {
-      return jarOutputDir;
-    } else {
-      return jarOutputDir + File.separator;
-    }
-  }
-
-  /**
-   * Return the value of $HADOOP_HOME
-   * @return $HADOOP_HOME, or null if it's not set.
-   */
-  public String getHadoopHome() {
-    return hadoopHome;
-  }
-
-  /**
-   * @return a sql command to execute and exit with.
-   */
-  public String getDebugSqlCmd() {
-    return debugSqlCmd;
-  }
-
-  /**
-   * @return The JDBC driver class name specified with --driver
-   */
-  public String getDriverClassName() {
-    return driverClassName;
-  }
-
-  /**
-   * @return the base destination path for table uploads.
-   */
-  public String getWarehouseDir() {
-    return warehouseDir;
-  }
-
-  /**
-   * @return the destination file format
-   */
-  public FileLayout getFileLayout() {
-    return this.layout;
-  }
-
-  public void setUsername(String name) {
-    this.username = name;
-  }
-
-  public void setPassword(String pass) {
-    this.password = pass;
-  }
-
-  /**
-   * @return the field delimiter to use when parsing lines. Defaults to the field delim
-   * to use when printing lines
-   */
-  public char getInputFieldDelim() {
-    if (inputFieldDelim == '\000') {
-      return this.outputFieldDelim;
-    } else {
-      return this.inputFieldDelim;
-    }
-  }
-
-  /**
-   * @return the record delimiter to use when parsing lines. Defaults to the record delim
-   * to use when printing lines.
-   */
-  public char getInputRecordDelim() {
-    if (inputRecordDelim == '\000') {
-      return this.outputRecordDelim;
-    } else {
-      return this.inputRecordDelim;
-    }
-  }
-
-  /**
-   * @return the character that may enclose fields when parsing lines. Defaults to the
-   * enclosing-char to use when printing lines.
-   */
-  public char getInputEnclosedBy() {
-    if (inputEnclosedBy == '\000') {
-      return this.outputEnclosedBy;
-    } else {
-      return this.inputEnclosedBy;
-    }
-  }
-
-  /**
-   * @return the escape character to use when parsing lines. Defaults to the escape
-   * character used when printing lines.
-   */
-  public char getInputEscapedBy() {
-    if (inputEscapedBy == '\000') {
-      return this.outputEscapedBy;
-    } else {
-      return this.inputEscapedBy;
-    }
-  }
-
-  /**
-   * @return true if fields must be enclosed by the --enclosed-by character when parsing.
-   * Defaults to false. Set true when --input-enclosed-by is used.
-   */
-  public boolean isInputEncloseRequired() {
-    if (inputEnclosedBy == '\000') {
-      return this.outputMustBeEnclosed;
-    } else {
-      return this.inputMustBeEnclosed;
-    }
-  }
-
-  /**
-   * @return the character to print between fields when importing them to text.
-   */
-  public char getOutputFieldDelim() {
-    return this.outputFieldDelim;
-  }
-
-
-  /**
-   * @return the character to print between records when importing them to text.
-   */
-  public char getOutputRecordDelim() {
-    return this.outputRecordDelim;
-  }
-
-  /**
-   * @return a character which may enclose the contents of fields when imported to text.
-   */
-  public char getOutputEnclosedBy() {
-    return this.outputEnclosedBy;
-  }
-
-  /**
-   * @return a character which signifies an escape sequence when importing to text.
-   */
-  public char getOutputEscapedBy() {
-    return this.outputEscapedBy;
-  }
-
-  /**
-   * @return true if fields imported to text must be enclosed by the EnclosedBy char.
-   * default is false; set to true if --enclosed-by is used instead of --optionally-enclosed-by.
-   */
-  public boolean isOutputEncloseRequired() {
-    return this.outputMustBeEnclosed;
-  }
-
-  /**
-   * @return true if the user wants imported results to be compressed.
-   */
-  public boolean shouldUseCompression() {
-    return this.useCompression;
-  }
-
-  /**
-   * @return the name of the destination table when importing to Hive
-   */
-  public String getHiveTableName( ) {
-    if (null != this.hiveTableName) {
-      return this.hiveTableName;
-    } else {
-      return this.tableName;
-    }
-  }
-
-  /**
-   * @return the file size to split by when using --direct mode.
-   */
-  public long getDirectSplitSize() {
-    return this.directSplitSize;
-  }
-
-  public Configuration getConf() {
-    return conf;
-  }
-
-  public void setConf(Configuration config) {
-    this.conf = config;
-  }
-
-  /**
-   * @return command-line arguments after a '-'
-   */
-  public String [] getExtraArgs() {
-    if (extraArgs == null) {
-      return null;
-    }
-
-    String [] out = new String[extraArgs.length];
-    for (int i = 0; i < extraArgs.length; i++) {
-      out[i] = extraArgs[i];
-    }
-    return out;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveImport.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveImport.java
deleted file mode 100644
index 836448a..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveImport.java
+++ /dev/null
@@ -1,194 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.hive;
-
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.OutputStreamWriter;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.util.Executor;
-import org.apache.hadoop.sqoop.util.LoggingAsyncSink;
-
-/**
- * Utility to import a table into the Hive metastore. Manages the connection
- * to Hive itself as well as orchestrating the use of the other classes in this
- * package.
- */
-public class HiveImport {
-
-  public static final Log LOG = LogFactory.getLog(HiveImport.class.getName());
-
-  private SqoopOptions options;
-  private ConnManager connManager;
-  private Configuration configuration;
-
-  public HiveImport(final SqoopOptions opts, final ConnManager connMgr, final Configuration conf) {
-    this.options = opts;
-    this.connManager = connMgr;
-    this.configuration = conf;
-  }
-
-
-  /** 
-   * @return the filename of the hive executable to run to do the import
-   */
-  private String getHiveBinPath() {
-    // If the user has $HIVE_HOME set, then use $HIVE_HOME/bin/hive if it
-    // exists.
-    // Fall back to just plain 'hive' and hope it's in the path.
-
-    String hiveHome = options.getHiveHome();
-    if (null == hiveHome) {
-      return "hive";
-    }
-
-    Path p = new Path(hiveHome);
-    p = new Path(p, "bin");
-    p = new Path(p, "hive");
-    String hiveBinStr = p.toString();
-    if (new File(hiveBinStr).exists()) {
-      return hiveBinStr;
-    } else {
-      return "hive";
-    }
-  }
-
-  /**
-   * If we used a MapReduce-based upload of the data, remove the _logs dir
-   * from where we put it, before running Hive LOAD DATA INPATH
-   */
-  private void removeTempLogs(String tableName) throws IOException {
-    FileSystem fs = FileSystem.get(configuration);
-    String warehouseDir = options.getWarehouseDir();
-    Path tablePath; 
-    if (warehouseDir != null) {
-      tablePath = new Path(new Path(warehouseDir), tableName);
-    } else {
-      tablePath = new Path(tableName);
-    }
-
-    Path logsPath = new Path(tablePath, "_logs");
-    if (fs.exists(logsPath)) {
-      LOG.info("Removing temporary files from import process: " + logsPath);
-      if (!fs.delete(logsPath, true)) {
-        LOG.warn("Could not delete temporary files; continuing with import, but it may fail.");
-      }
-    }
-  }
-
-  /**
-   * Perform the import of data from an HDFS path to a Hive table.
-   *
-   * @param inputTableName the name of the table as loaded into HDFS
-   * @param outputTableName the name of the table to create in Hive.
-   */
-  public void importTable(String inputTableName, String outputTableName)
-      throws IOException {
-    removeTempLogs(inputTableName);
-
-    LOG.info("Loading uploaded data into Hive");
-
-    if (null == outputTableName) {
-      outputTableName = inputTableName;
-    }
-    LOG.debug("Hive.inputTable: " + inputTableName);
-    LOG.debug("Hive.outputTable: " + outputTableName);
-
-    // For testing purposes against our mock hive implementation, 
-    // if the sysproperty "expected.script" is set, we set the EXPECTED_SCRIPT
-    // environment variable for the child hive process. We also disable
-    // timestamp comments so that we have deterministic table creation scripts.
-    String expectedScript = System.getProperty("expected.script");
-    List<String> env = Executor.getCurEnvpStrings();
-    boolean debugMode = expectedScript != null;
-    if (debugMode) {
-      env.add("EXPECTED_SCRIPT=" + expectedScript);
-      env.add("TMPDIR=" + options.getTempDir());
-    }
-
-    // generate the HQL statements to run.
-    TableDefWriter tableWriter = new TableDefWriter(options, connManager,
-        inputTableName, outputTableName,
-        configuration, !debugMode);
-    String createTableStr = tableWriter.getCreateTableStmt() + ";\n";
-    String loadDataStmtStr = tableWriter.getLoadDataStmt() + ";\n";
-
-    // write them to a script file.
-    File tempFile = File.createTempFile("hive-script-",".txt", new File(options.getTempDir()));
-    try {
-      String tmpFilename = tempFile.toString();
-      BufferedWriter w = null;
-      try {
-        FileOutputStream fos = new FileOutputStream(tempFile);
-        w = new BufferedWriter(new OutputStreamWriter(fos));
-        w.write(createTableStr, 0, createTableStr.length());
-        if (!options.doCreateHiveTableOnly()) {
-          w.write(loadDataStmtStr, 0, loadDataStmtStr.length());
-        }
-      } catch (IOException ioe) {
-        LOG.error("Error writing Hive load-in script: " + ioe.toString());
-        ioe.printStackTrace();
-        throw ioe;
-      } finally {
-        if (null != w) {
-          try {
-            w.close();
-          } catch (IOException ioe) {
-            LOG.warn("IOException closing stream to Hive script: " + ioe.toString());
-          }
-        }
-      }
-
-      // run Hive on the script and note the return code.
-      String hiveExec = getHiveBinPath();
-      ArrayList<String> args = new ArrayList<String>();
-      args.add(hiveExec);
-      args.add("-f");
-      args.add(tmpFilename);
-
-      LoggingAsyncSink logSink = new LoggingAsyncSink(LOG);
-      int ret = Executor.exec(args.toArray(new String[0]),
-          env.toArray(new String[0]), logSink, logSink);
-      if (0 != ret) {
-        throw new IOException("Hive exited with status " + ret);
-      }
-
-      LOG.info("Hive import complete.");
-    } finally {
-      if (!tempFile.delete()) {
-        LOG.warn("Could not remove temporary file: " + tempFile.toString());
-        // try to delete the file later.
-        tempFile.deleteOnExit();
-      }
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveTypes.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveTypes.java
deleted file mode 100644
index 59c6fda..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/HiveTypes.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.hive;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import java.sql.Types;
-
-/**
- * Defines conversion between SQL types and Hive types.
- */
-public class HiveTypes {
-
-  public static final Log LOG = LogFactory.getLog(HiveTypes.class.getName());
-
-  /**
-   * Given JDBC SQL types coming from another database, what is the best
-   * mapping to a Hive-specific type?
-   */
-  public static String toHiveType(int sqlType) {
-    if (sqlType == Types.INTEGER) {
-      return "INT";
-    } else if (sqlType == Types.VARCHAR) {
-      return "STRING";
-    } else if (sqlType == Types.CHAR) {
-      return "STRING";
-    } else if (sqlType == Types.LONGVARCHAR) {
-      return "STRING";
-    } else if (sqlType == Types.NUMERIC) {
-      // Per suggestion on hive-user, this is converted to DOUBLE for now.
-      return "DOUBLE";
-    } else if (sqlType == Types.DECIMAL) {
-      // Per suggestion on hive-user, this is converted to DOUBLE for now.
-      return "DOUBLE";
-    } else if (sqlType == Types.BIT) {
-      return "BOOLEAN";
-    } else if (sqlType == Types.BOOLEAN) {
-      return "BOOLEAN";
-    } else if (sqlType == Types.TINYINT) {
-      return "TINYINT";
-    } else if (sqlType == Types.SMALLINT) {
-      return "INTEGER";
-    } else if (sqlType == Types.BIGINT) {
-      return "BIGINT";
-    } else if (sqlType == Types.REAL) {
-      return "DOUBLE";
-    } else if (sqlType == Types.FLOAT) {
-      return "DOUBLE";
-    } else if (sqlType == Types.DOUBLE) {
-      return "DOUBLE";
-    } else if (sqlType == Types.DATE) {
-      // unfortunate type coercion
-      return "STRING";
-    } else if (sqlType == Types.TIME) {
-      // unfortunate type coercion
-      return "STRING";
-    } else if (sqlType == Types.TIMESTAMP) {
-      // unfortunate type coercion
-      return "STRING";
-    } else {
-      // TODO(aaron): Support BINARY, VARBINARY, LONGVARBINARY, DISTINCT, CLOB,
-      // BLOB, ARRAY, STRUCT, REF, JAVA_OBJECT.
-      return null;
-    }
-  }
-
-  /** 
-   * @return true if a sql type can't be translated to a precise match
-   * in Hive, and we have to cast it to something more generic.
-   */
-  public static boolean isHiveTypeImprovised(int sqlType) {
-    return sqlType == Types.DATE || sqlType == Types.TIME
-        || sqlType == Types.TIMESTAMP
-        || sqlType == Types.DECIMAL
-        || sqlType == Types.NUMERIC;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/TableDefWriter.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/TableDefWriter.java
deleted file mode 100644
index 1fd907d..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/hive/TableDefWriter.java
+++ /dev/null
@@ -1,221 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.hive;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Map;
-import java.util.Date;
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Creates (Hive-specific) SQL DDL statements to create tables to hold data
- * we're importing from another source.
- *
- * After we import the database into HDFS, we can inject it into Hive using
- * the CREATE TABLE and LOAD DATA INPATH statements generated by this object.
- */
-public class TableDefWriter {
-
-  public static final Log LOG = LogFactory.getLog(TableDefWriter.class.getName());
-
-  private SqoopOptions options;
-  private ConnManager connManager;
-  private Configuration configuration;
-  private String inputTableName;
-  private String outputTableName;
-  private boolean commentsEnabled;
-
-  /**
-   * Creates a new TableDefWriter to generate a Hive CREATE TABLE statement.
-   * @param opts program-wide options
-   * @param connMgr the connection manager used to describe the table.
-   * @param table the name of the table to read.
-   * @param config the Hadoop configuration to use to connect to the dfs
-   * @param withComments if true, then tables will be created with a
-   *        timestamp comment.
-   */
-  public TableDefWriter(final SqoopOptions opts, final ConnManager connMgr,
-      final String inputTable, final String outputTable,
-      final Configuration config, final boolean withComments) {
-    this.options = opts;
-    this.connManager = connMgr;
-    this.inputTableName = inputTable;
-    this.outputTableName = outputTable;
-    this.configuration = config;
-    this.commentsEnabled = withComments;
-  }
-
-  private Map<String, Integer> externalColTypes;
-
-  /**
-   * Set the column type map to be used.
-   * (dependency injection for testing; not used in production.)
-   */
-  void setColumnTypes(Map<String, Integer> colTypes) {
-    this.externalColTypes = colTypes;
-    LOG.debug("Using test-controlled type map");
-  }
-
-  /**
-   * Get the column names to import.
-   */
-  private String [] getColumnNames() {
-    String [] colNames = options.getColumns();
-    if (null != colNames) {
-      return colNames; // user-specified column names.
-    } else if (null != externalColTypes) {
-      // Test-injection column mapping. Extract the col names from this.
-      ArrayList<String> keyList = new ArrayList<String>();
-      for (String key : externalColTypes.keySet()) {
-        keyList.add(key);
-      }
-
-      return keyList.toArray(new String[keyList.size()]);
-    } else {
-      return connManager.getColumnNames(inputTableName);
-    }
-  }
-
-  /**
-   * @return the CREATE TABLE statement for the table to load into hive.
-   */
-  public String getCreateTableStmt() throws IOException {
-    Map<String, Integer> columnTypes;
-
-    if (externalColTypes != null) {
-      // Use pre-defined column types.
-      columnTypes = externalColTypes;
-    } else {
-      // Get these from the database.
-      columnTypes = connManager.getColumnTypes(inputTableName);
-    }
-
-    String [] colNames = getColumnNames();
-    StringBuilder sb = new StringBuilder();
-    if (options.doOverwriteHiveTable()) {
-      sb.append("CREATE TABLE " + outputTableName + " ( ");
-    } else {
-      sb.append("CREATE TABLE IF NOT EXISTS " + outputTableName + " ( ");
-    }
-
-    boolean first = true;
-    for (String col : colNames) {
-      if (!first) {
-        sb.append(", ");
-      }
-
-      first = false;
-
-      Integer colType = columnTypes.get(col);
-      String hiveColType = connManager.toHiveType(colType);
-      if (null == hiveColType) {
-        throw new IOException("Hive does not support the SQL type for column " + col);
-      }
-
-      sb.append(col + " " + hiveColType);
-
-      if (HiveTypes.isHiveTypeImprovised(colType)) {
-        LOG.warn("Column " + col + " had to be cast to a less precise type in Hive");
-      }
-    }
-
-    sb.append(") ");
-
-    if (commentsEnabled) {
-      DateFormat dateFormat = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss");
-      String curDateStr = dateFormat.format(new Date());
-      sb.append("COMMENT 'Imported by sqoop on " + curDateStr + "' ");
-    }
-
-    sb.append("ROW FORMAT DELIMITED FIELDS TERMINATED BY '");
-    sb.append(getHiveOctalCharCode((int) options.getOutputFieldDelim()));
-    sb.append("' LINES TERMINATED BY '");
-    sb.append(getHiveOctalCharCode((int) options.getOutputRecordDelim()));
-    sb.append("' STORED AS TEXTFILE");
-
-    LOG.debug("Create statement: " + sb.toString());
-    return sb.toString();
-  }
-
-  private static final int DEFAULT_HDFS_PORT =
-      org.apache.hadoop.hdfs.server.namenode.NameNode.DEFAULT_PORT;
-
-  /**
-   * @return the LOAD DATA statement to import the data in HDFS into hive
-   */
-  public String getLoadDataStmt() throws IOException { 
-    String warehouseDir = options.getWarehouseDir();
-    if (null == warehouseDir) {
-      warehouseDir = "";
-    } else if (!warehouseDir.endsWith(File.separator)) {
-      warehouseDir = warehouseDir + File.separator;
-    }
-
-    String tablePath = warehouseDir + inputTableName;
-    FileSystem fs = FileSystem.get(configuration);
-    Path finalPath = new Path(tablePath).makeQualified(fs);
-    String finalPathStr = finalPath.toString();
-
-    StringBuilder sb = new StringBuilder();
-    sb.append("LOAD DATA INPATH '");
-    sb.append(finalPathStr);
-    sb.append("' INTO TABLE ");
-    sb.append(outputTableName);
-
-    LOG.debug("Load statement: " + sb.toString());
-    return sb.toString();
-  }
-
-  /**
-   * Return a string identifying the character to use as a delimiter
-   * in Hive, in octal representation.
-   * Hive can specify delimiter characters in the form '\ooo' where
-   * ooo is a three-digit octal number between 000 and 177. Values
-   * may not be truncated ('\12' is wrong; '\012' is ok) nor may they
-   * be zero-prefixed (e.g., '\0177' is wrong).
-   *
-   * @param charNum the character to use as a delimiter
-   * @return a string of the form "\ooo" where ooo is an octal number
-   * in [000, 177].
-   * @throws IllegalArgumentException if charNum &gt;> 0177.
-   */
-  static String getHiveOctalCharCode(int charNum)
-      throws IllegalArgumentException {
-    if (charNum > 0177) {
-      throw new IllegalArgumentException(
-          "Character " + charNum + " is an out-of-range delimiter");
-    }
-
-    return String.format("\\%03o", charNum);
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/CountingOutputStream.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/CountingOutputStream.java
deleted file mode 100644
index e223628..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/CountingOutputStream.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.io;
-
-import java.io.OutputStream;
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * An output stream that counts how many bytes its written.
- */
-public class CountingOutputStream extends OutputStream {
-
-  public static final Log LOG = LogFactory.getLog(CountingOutputStream.class.getName());
-
-  private final OutputStream stream;
-  private long bytesWritten;
-
-  public CountingOutputStream(final OutputStream outputStream) {
-    this.stream = outputStream;
-    this.bytesWritten = 0;
-  }
-
-  /** @return the number of bytes written thus far to the stream. */
-  public long getBytesWritten() {
-    return bytesWritten;
-  }
-
-  /** Reset the counter of bytes written to zero. */
-  public void resetCount() {
-    this.bytesWritten = 0;
-  }
-
-  public void close() throws IOException {
-    this.stream.close();
-  }
-
-  public void flush() throws IOException {
-    this.stream.flush();
-  }
-
-  public void write(byte [] b) throws IOException {
-    this.stream.write(b);
-    bytesWritten += b.length;
-  }
-
-  public void write(byte [] b, int off, int len) throws IOException {
-    this.stream.write(b, off, len);
-    bytesWritten += len;
-  }
-
-  public void write(int b) throws IOException {
-    this.stream.write(b);
-    bytesWritten++;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/HdfsSplitOutputStream.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/HdfsSplitOutputStream.java
deleted file mode 100644
index cbb33ea..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/HdfsSplitOutputStream.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.io;
-
-import java.io.OutputStream;
-import java.io.IOException;
-import java.util.zip.GZIPOutputStream;
-import java.util.Formatter;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-/**
- * An output stream that writes to HDFS, opening a new file after
- * a specified number of bytes have been written to the current one.
- */
-public class HdfsSplitOutputStream extends OutputStream {
-
-  public static final Log LOG = LogFactory.getLog(HdfsSplitOutputStream.class.getName());
-
-  private OutputStream writeStream;
-  private CountingOutputStream countingFilterStream;
-  private Configuration conf;
-  private Path destDir;
-  private String filePrefix;
-  private long cutoffBytes;
-  private boolean doGzip;
-  private int fileNum;
-
-  /**
-   * Create a new HdfsSplitOutputStream.
-   * @param conf the Configuration to use to interface with HDFS
-   * @param destDir the directory where the files will go (should already exist).
-   * @param filePrefix the first part of the filename, which will be appended by a number.
-   *    This file will be placed inside destDir.
-   * @param cutoff the approximate number of bytes to use per file
-   * @param doGzip if true, then output files will be gzipped and have a .gz suffix.
-   */
-  public HdfsSplitOutputStream(final Configuration conf, final Path destDir,
-      final String filePrefix, final long cutoff, final boolean doGzip) throws IOException {
-
-    this.conf = conf;
-    this.destDir = destDir;
-    this.filePrefix = filePrefix;
-    this.cutoffBytes = cutoff;
-    if (this.cutoffBytes < 0) {
-      this.cutoffBytes = 0; // splitting disabled.
-    }
-    this.doGzip = doGzip;
-    this.fileNum = 0;
-
-    openNextFile();
-  }
-
-  /** Initialize the OutputStream to the next file to write to.
-   */
-  private void openNextFile() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-
-    StringBuffer sb = new StringBuffer();
-    Formatter fmt = new Formatter(sb);
-    fmt.format("%05d", this.fileNum++);
-    String filename = filePrefix + fmt.toString();
-    if (this.doGzip) {
-      filename = filename + ".gz";
-    }
-    Path destFile = new Path(destDir, filename);
-    LOG.debug("Opening next output file: " + destFile);
-    if (fs.exists(destFile)) {
-       Path canonicalDest = destFile.makeQualified(fs);
-       throw new IOException("Destination file " + canonicalDest + " already exists");
-    }
-
-    OutputStream fsOut = fs.create(destFile);
-
-    // Count how many actual bytes hit HDFS.
-    this.countingFilterStream = new CountingOutputStream(fsOut);
-
-    if (this.doGzip) {
-      // Wrap that in a Gzip stream.
-      this.writeStream = new GZIPOutputStream(this.countingFilterStream);
-    } else {
-      // Write to the counting stream directly.
-      this.writeStream = this.countingFilterStream;
-    }
-  }
-
-  /**
-   * @return true if allowSplit() would actually cause a split.
-   */
-  public boolean wouldSplit() {
-    return this.cutoffBytes > 0
-        && this.countingFilterStream.getBytesWritten() >= this.cutoffBytes;
-  }
-
-  /** If we've written more to the disk than the user's split size,
-   * open the next file.
-   */
-  private void checkForNextFile() throws IOException {
-    if (wouldSplit()) {
-      LOG.debug("Starting new split");
-      this.writeStream.flush();
-      this.writeStream.close();
-      openNextFile();
-    }
-  }
-
-  /** Defines a point in the stream when it is acceptable to split to a new file;
-      e.g., the end of a record.
-    */
-  public void allowSplit() throws IOException {
-    checkForNextFile();
-  }
-
-  public void close() throws IOException {
-    this.writeStream.close();
-  }
-
-  public void flush() throws IOException {
-    this.writeStream.flush();
-  }
-
-  public void write(byte [] b) throws IOException {
-    this.writeStream.write(b);
-  }
-
-  public void write(byte [] b, int off, int len) throws IOException {
-    this.writeStream.write(b, off, len);
-  }
-
-  public void write(int b) throws IOException {
-    this.writeStream.write(b);
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittableBufferedWriter.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittableBufferedWriter.java
deleted file mode 100644
index dd7c5a2..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittableBufferedWriter.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.io;
-
-import java.io.BufferedWriter;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.io.IOException;
-import java.util.Formatter;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * A BufferedWriter implementation that wraps around a SplittingOutputStream
- * and allows splitting of the underlying stream.
- * Splits occur at allowSplit() calls, or newLine() calls.
- */
-public class SplittableBufferedWriter extends BufferedWriter {
-
-  public static final Log LOG = LogFactory.getLog(
-      SplittableBufferedWriter.class.getName());
-
-  private SplittingOutputStream splitOutputStream;
-  private boolean alwaysFlush;
-
-  public SplittableBufferedWriter(
-      final SplittingOutputStream splitOutputStream) {
-    super(new OutputStreamWriter(splitOutputStream));
-
-    this.splitOutputStream = splitOutputStream;
-    this.alwaysFlush = false;
-  }
-
-  /** For testing */
-  SplittableBufferedWriter(final SplittingOutputStream splitOutputStream,
-      final boolean alwaysFlush) {
-    super(new OutputStreamWriter(splitOutputStream));
-
-    this.splitOutputStream = splitOutputStream;
-    this.alwaysFlush = alwaysFlush;
-  }
-
-  public void newLine() throws IOException {
-    super.newLine();
-    this.allowSplit();
-  }
-
-  public void allowSplit() throws IOException {
-    if (alwaysFlush) {
-      this.flush();
-    }
-    if (this.splitOutputStream.wouldSplit()) {
-      LOG.debug("Starting new split");
-      this.flush();
-      this.splitOutputStream.allowSplit();
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittingOutputStream.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittingOutputStream.java
deleted file mode 100644
index 2202086..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/io/SplittingOutputStream.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.io;
-
-import java.io.OutputStream;
-import java.io.IOException;
-import java.util.zip.GZIPOutputStream;
-import java.util.Formatter;
-
-import org.apache.commons.io.output.CountingOutputStream;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-/**
- * An output stream that writes to an underlying filesystem, opening
- * a new file after a specified number of bytes have been written to the
- * current one.
- */
-public class SplittingOutputStream extends OutputStream {
-
-  public static final Log LOG = LogFactory.getLog(
-      SplittingOutputStream.class.getName());
-
-  private OutputStream writeStream;
-  private CountingOutputStream countingFilterStream;
-  private Configuration conf;
-  private Path destDir;
-  private String filePrefix;
-  private long cutoffBytes;
-  private boolean doGzip;
-  private int fileNum;
-
-  /**
-   * Create a new SplittingOutputStream.
-   * @param conf the Configuration to use to interface with HDFS
-   * @param destDir the directory where the files will go (should already
-   *     exist).
-   * @param filePrefix the first part of the filename, which will be appended
-   *    by a number. This file will be placed inside destDir.
-   * @param cutoff the approximate number of bytes to use per file
-   * @param doGzip if true, then output files will be gzipped and have a .gz
-   *   suffix.
-   */
-  public SplittingOutputStream(final Configuration conf, final Path destDir,
-      final String filePrefix, final long cutoff, final boolean doGzip)
-      throws IOException {
-
-    this.conf = conf;
-    this.destDir = destDir;
-    this.filePrefix = filePrefix;
-    this.cutoffBytes = cutoff;
-    if (this.cutoffBytes < 0) {
-      this.cutoffBytes = 0; // splitting disabled.
-    }
-    this.doGzip = doGzip;
-    this.fileNum = 0;
-
-    openNextFile();
-  }
-
-  /** Initialize the OutputStream to the next file to write to.
-   */
-  private void openNextFile() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-
-    StringBuffer sb = new StringBuffer();
-    Formatter fmt = new Formatter(sb);
-    fmt.format("%05d", this.fileNum++);
-    String filename = filePrefix + fmt.toString();
-    if (this.doGzip) {
-      filename = filename + ".gz";
-    }
-    Path destFile = new Path(destDir, filename);
-    LOG.debug("Opening next output file: " + destFile);
-    if (fs.exists(destFile)) {
-      Path canonicalDest = destFile.makeQualified(fs);
-      throw new IOException("Destination file " + canonicalDest
-          + " already exists");
-    }
-
-    OutputStream fsOut = fs.create(destFile);
-
-    // Count how many actual bytes hit HDFS.
-    this.countingFilterStream = new CountingOutputStream(fsOut);
-
-    if (this.doGzip) {
-      // Wrap that in a Gzip stream.
-      this.writeStream = new GZIPOutputStream(this.countingFilterStream);
-    } else {
-      // Write to the counting stream directly.
-      this.writeStream = this.countingFilterStream;
-    }
-  }
-
-  /**
-   * @return true if allowSplit() would actually cause a split.
-   */
-  public boolean wouldSplit() {
-    return this.cutoffBytes > 0
-        && this.countingFilterStream.getByteCount() >= this.cutoffBytes;
-  }
-
-  /** If we've written more to the disk than the user's split size,
-   * open the next file.
-   */
-  private void checkForNextFile() throws IOException {
-    if (wouldSplit()) {
-      LOG.debug("Starting new split");
-      this.writeStream.flush();
-      this.writeStream.close();
-      openNextFile();
-    }
-  }
-
-  /** Defines a point in the stream when it is acceptable to split to a new
-      file; e.g., the end of a record.
-    */
-  public void allowSplit() throws IOException {
-    checkForNextFile();
-  }
-
-  public void close() throws IOException {
-    this.writeStream.close();
-  }
-
-  public void flush() throws IOException {
-    this.writeStream.flush();
-  }
-
-  public void write(byte [] b) throws IOException {
-    this.writeStream.write(b);
-  }
-
-  public void write(byte [] b, int off, int len) throws IOException {
-    this.writeStream.write(b, off, len);
-  }
-
-  public void write(int b) throws IOException {
-    this.writeStream.write(b);
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/BigDecimalSerializer.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/BigDecimalSerializer.java
deleted file mode 100644
index ea1df7b..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/BigDecimalSerializer.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.math.BigDecimal;
-import java.math.BigInteger;
-
-import org.apache.hadoop.io.Text;
-
-/**
- * Serialize BigDecimal classes to/from DataInput and DataOutput objects.
- *
- * BigDecimal is comprised of a BigInteger with an integer 'scale' field.
- * The BigDecimal/BigInteger can also return itself as a 'long' value.
- *
- * We serialize in one of two formats:
- *
- *  First, check whether the BigInt can fit in a long:
- *  boolean b = BigIntegerPart > LONG_MAX || BigIntegerPart < LONG_MIN
- *
- *  [int: scale][boolean: b == false][long: BigInt-part]
- *  [int: scale][boolean: b == true][string: BigInt-part.toString()]
- *
- *
- * 
- *
- * TODO(aaron): Get this to work with Hadoop's Serializations framework.
- */
-public final class BigDecimalSerializer {
-
-  private BigDecimalSerializer() { }
-
-  static final BigInteger LONG_MAX_AS_BIGINT = BigInteger.valueOf(Long.MAX_VALUE);
-  static final BigInteger LONG_MIN_AS_BIGINT = BigInteger.valueOf(Long.MIN_VALUE);
-
-  public static void write(BigDecimal d, DataOutput out) throws IOException {
-    int scale = d.scale();
-    BigInteger bigIntPart = d.unscaledValue();
-    boolean fastpath = bigIntPart.compareTo(LONG_MAX_AS_BIGINT) < 0
-        && bigIntPart .compareTo(LONG_MIN_AS_BIGINT) > 0;
-
-    out.writeInt(scale);
-    out.writeBoolean(fastpath);
-    if (fastpath) {
-      out.writeLong(bigIntPart.longValue());
-    } else {
-      Text.writeString(out, bigIntPart.toString());
-    }
-  }
-
-  public static BigDecimal readFields(DataInput in) throws IOException {
-    int scale = in.readInt();
-    boolean fastpath = in.readBoolean();
-    BigInteger unscaledIntPart;
-    if (fastpath) {
-      long unscaledValue = in.readLong();
-      unscaledIntPart = BigInteger.valueOf(unscaledValue);
-    } else {
-      String unscaledValueStr = Text.readString(in);
-      unscaledIntPart = new BigInteger(unscaledValueStr);
-    }
-
-    return new BigDecimal(unscaledIntPart, scale);
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java
deleted file mode 100644
index a1b6742..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-
-/**
- * Abstract base class for all DBWritable types generated by Sqoop.
- * Contains methods required by all such types, to help with parsing,
- * stringification, etc.
- */
-public final class FieldFormatter {
-
-  private FieldFormatter() { }
-
-  /** 
-   * Takes an input string representing the value of a field, encloses it in
-   * enclosing chars, and escapes any occurrences of such characters in the middle.
-   * The escape character itself is also escaped if it appears in the text of the
-   * field.
-   *
-   * The field is enclosed only if:
-   *   enclose != '\000', and:
-   *     encloseRequired is true, or
-   *     one of the characters in the mustEscapeFor list is present in the string.
-   *
-   * Escaping is not performed if the escape char is '\000'.
-   *
-   * @param str - The user's string to escape and enclose
-   * @param escape - What string to use as the escape sequence. If "" or null, then don't escape.
-   * @param enclose - The string to use to enclose str e.g. "quoted". If "" or null, then don't
-   *     enclose.
-   * @param mustEncloseFor - A list of characters; if one is present in 'str', then str must be
-   *     enclosed
-   * @param encloseRequired - If true, then always enclose, regardless of mustEscapeFor
-   * @return the escaped, enclosed version of 'str'
-   */
-  public static final String escapeAndEnclose(String str, String escape, String enclose,
-      char [] mustEncloseFor, boolean encloseRequired) {
-
-    // true if we can use an escape character.
-    boolean escapingLegal = (null != escape && escape.length() > 0 && !escape.equals("\000"));
-    String withEscapes;
-
-    if (null == str) {
-      return null;
-    }
-
-    if (escapingLegal) {
-      // escaping is legal. Escape any instances of the escape char itself
-      withEscapes = str.replace(escape, escape + escape);
-    } else {
-      // no need to double-escape
-      withEscapes = str;
-    }
-
-    if (null == enclose || enclose.length() == 0 || enclose.equals("\000")) {
-      // The enclose-with character was left unset, so we can't enclose items. We're done.
-      return withEscapes;
-    }
-
-    // if we have an enclosing character, and escaping is legal, then the encloser must
-    // always be escaped.
-    if (escapingLegal) {
-      withEscapes = withEscapes.replace(enclose, escape + enclose);
-    }
-
-    boolean actuallyDoEnclose = encloseRequired;
-    if (!actuallyDoEnclose && mustEncloseFor != null) {
-      // check if the string requires enclosing
-      for (char reason : mustEncloseFor) {
-        if (str.indexOf(reason) != -1) {
-          actuallyDoEnclose = true;
-          break;
-        }
-      }
-    }
-
-    if (actuallyDoEnclose) {
-      return enclose + withEscapes + enclose;
-    } else {
-      return withEscapes;
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/JdbcWritableBridge.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/JdbcWritableBridge.java
deleted file mode 100644
index bf350aa..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/JdbcWritableBridge.java
+++ /dev/null
@@ -1,203 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import java.math.BigDecimal;
-import java.sql.Date;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.sql.Time;
-import java.sql.Timestamp;
-
-/**
- * Contains a set of methods which can read db columns from a ResultSet into
- * Java types, and do serialization of these types to/from DataInput/DataOutput
- * for use with Hadoop's Writable implementation. This supports null values
- * for all types.
- *
- * 
- *
- */
-public final class JdbcWritableBridge {
-
-  private JdbcWritableBridge() {
-  }
-
-  public static Integer readInteger(int colNum, ResultSet r) throws SQLException {
-    int val;
-    val = r.getInt(colNum);
-    if (r.wasNull()) {
-      return null;
-    } else {
-      return Integer.valueOf(val);
-    }
-  }
-
-  public static Long readLong(int colNum, ResultSet r) throws SQLException {
-    long val;
-    val = r.getLong(colNum);
-    if (r.wasNull()) {
-      return null;
-    } else {
-      return Long.valueOf(val);
-    }
-  }
-
-  public static String readString(int colNum, ResultSet r) throws SQLException {
-    return r.getString(colNum);
-  }
-
-  public static Float readFloat(int colNum, ResultSet r) throws SQLException {
-    float val;
-    val = r.getFloat(colNum);
-    if (r.wasNull()) {
-      return null;
-    } else {
-      return Float.valueOf(val);
-    }
-  }
-
-  public static Double readDouble(int colNum, ResultSet r) throws SQLException {
-    double val;
-    val = r.getDouble(colNum);
-    if (r.wasNull()) {
-      return null;
-    } else {
-      return Double.valueOf(val);
-    }
-  }
-
-  public static Boolean readBoolean(int colNum, ResultSet r) throws SQLException {
-    boolean val;
-    val = r.getBoolean(colNum);
-    if (r.wasNull()) {
-      return null;
-    } else {
-      return Boolean.valueOf(val);
-    }
-  }
-
-  public static Time readTime(int colNum, ResultSet r) throws SQLException {
-    return r.getTime(colNum);
-  }
-
-  public static Timestamp readTimestamp(int colNum, ResultSet r) throws SQLException {
-    return r.getTimestamp(colNum);
-  }
-
-  public static Date readDate(int colNum, ResultSet r) throws SQLException {
-    return r.getDate(colNum);
-  }
-
-  public static BigDecimal readBigDecimal(int colNum, ResultSet r) throws SQLException {
-    return r.getBigDecimal(colNum);
-  }
-
-  public static void writeInteger(Integer val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setInt(paramIdx, val);
-    }
-  }
-
-  public static void writeLong(Long val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setLong(paramIdx, val);
-    }
-  }
-
-  public static void writeDouble(Double val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setDouble(paramIdx, val);
-    }
-  }
-
-  public static void writeBoolean(Boolean val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setBoolean(paramIdx, val);
-    }
-  }
-
-  public static void writeFloat(Float val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setFloat(paramIdx, val);
-    }
-  }
-
-  public static void writeString(String val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setString(paramIdx, val);
-    }
-  }
-
-  public static void writeTimestamp(Timestamp val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setTimestamp(paramIdx, val);
-    }
-  }
-
-  public static void writeTime(Time val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setTime(paramIdx, val);
-    }
-  }
-
-  public static void writeDate(Date val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setDate(paramIdx, val);
-    }
-  }
-
-  public static void writeBigDecimal(BigDecimal val, int paramIdx, int sqlType, PreparedStatement s)
-      throws SQLException {
-    if (null == val) {
-      s.setNull(paramIdx, sqlType);
-    } else {
-      s.setBigDecimal(paramIdx, val);
-    }
-  }
-
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/RecordParser.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/RecordParser.java
deleted file mode 100644
index feb01cb..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/RecordParser.java
+++ /dev/null
@@ -1,353 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.io.Text;
-
-import java.nio.ByteBuffer;
-import java.nio.CharBuffer;
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * Parses a record containing one or more fields. Fields are separated
- * by some FIELD_DELIMITER character, e.g. a comma or a ^A character.
- * Records are terminated by a RECORD_DELIMITER character, e.g., a newline.
- *
- * Fields may be (optionally or mandatorily) enclosed by a quoting char
- * e.g., '\"'
- *
- * Fields may contain escaped characters. An escape character may be, e.g.,
- * the '\\' character. Any character following an escape character
- * is treated literally. e.g., '\n' is recorded as an 'n' character, not a
- * newline.
- *
- * Unexpected results may occur if the enclosing character escapes itself.
- * e.g., this cannot parse SQL SELECT statements where the single character
- * ['] escapes to [''].
- *
- * This class is not synchronized. Multiple threads must use separate
- * instances of RecordParser.
- *
- * The fields parsed by RecordParser are backed by an internal buffer
- * which is cleared when the next call to parseRecord() is made. If
- * the buffer is required to be preserved, you must copy it yourself.
- */
-public final class RecordParser {
-
-  public static final Log LOG = LogFactory.getLog(RecordParser.class.getName());
-
-  private enum ParseState {
-    FIELD_START,
-    ENCLOSED_FIELD,
-    UNENCLOSED_FIELD,
-    ENCLOSED_ESCAPE,
-    ENCLOSED_EXPECT_DELIMITER,
-    UNENCLOSED_ESCAPE
-  }
-
-  public static class ParseError extends Exception {
-    public ParseError() {
-      super("ParseError");
-    }
-
-    public ParseError(final String msg) {
-      super(msg);
-    }
-
-    public ParseError(final String msg, final Throwable cause) {
-      super(msg, cause);
-    }
-
-    public ParseError(final Throwable cause) {
-      super(cause);
-    }
-  }
-
-  private char fieldDelim;
-  private char recordDelim;
-  private char enclosingChar;
-  private char escapeChar;
-  private boolean enclosingRequired;
-  private ArrayList<String> outputs;
-
-  public RecordParser(final char field, final char record, final char enclose,
-      final char escape, final boolean mustEnclose) {
-    this.fieldDelim = field;
-    this.recordDelim = record;
-    this.enclosingChar = enclose;
-    this.escapeChar = escape;
-    this.enclosingRequired = mustEnclose;
-
-    this.outputs = new ArrayList<String>();
-  }
-
-  /**
-   * Return a list of strings representing the fields of the input line.
-   * This list is backed by an internal buffer which is cleared by the
-   * next call to parseRecord().
-   */
-  public List<String> parseRecord(CharSequence input) throws ParseError {
-    if (null == input) {
-      throw new ParseError("null input string");
-    }
-
-    return parseRecord(CharBuffer.wrap(input));
-  }
-
-  /**
-   * Return a list of strings representing the fields of the input line.
-   * This list is backed by an internal buffer which is cleared by the
-   * next call to parseRecord().
-   */
-  public List<String> parseRecord(Text input) throws ParseError { 
-    if (null == input) { 
-      throw new ParseError("null input string");
-    }
-
-    // TODO(aaron): The parser should be able to handle UTF-8 strings
-    // as well, to avoid this transcode operation.
-    return parseRecord(input.toString());
-  }
-
-  /**
-   * Return a list of strings representing the fields of the input line.
-   * This list is backed by an internal buffer which is cleared by the
-   * next call to parseRecord().
-   */
-  public List<String> parseRecord(byte [] input) throws ParseError {
-    if (null == input) {
-      throw new ParseError("null input string");
-    }
-
-    return parseRecord(ByteBuffer.wrap(input).asCharBuffer());
-  }
-
-  /**
-   * Return a list of strings representing the fields of the input line.
-   * This list is backed by an internal buffer which is cleared by the
-   * next call to parseRecord().
-   */
-  public List<String> parseRecord(char [] input) throws ParseError {
-    if (null == input) {
-      throw new ParseError("null input string");
-    }
-
-    return parseRecord(CharBuffer.wrap(input));
-  }
-
-  public List<String> parseRecord(ByteBuffer input) throws ParseError {
-    if (null == input) {
-      throw new ParseError("null input string");
-    }
-
-    return parseRecord(input.asCharBuffer());
-  }
-
-  /**
-   * Return a list of strings representing the fields of the input line.
-   * This list is backed by an internal buffer which is cleared by the
-   * next call to parseRecord().
-   */
-  public List<String> parseRecord(CharBuffer input) throws ParseError {
-    if (null == input) {
-      throw new ParseError("null input string");
-    }
-
-    /*
-      This method implements the following state machine to perform
-      parsing.
-
-      Note that there are no restrictions on whether particular characters
-      (e.g., field-sep, record-sep, etc) are distinct or the same. The
-      state transitions are processed in the order seen in this comment.
-
-      Starting state is FIELD_START
-        encloser -> ENCLOSED_FIELD
-        escape char -> UNENCLOSED_ESCAPE
-        field delim -> FIELD_START (for a new field)
-        record delim -> stops processing
-        all other letters get added to current field, -> UNENCLOSED FIELD
-
-      ENCLOSED_FIELD state:
-        escape char goes to ENCLOSED_ESCAPE
-        encloser goes to ENCLOSED_EXPECT_DELIMITER
-        field sep or record sep gets added to the current string
-        normal letters get added to the current string
-
-      ENCLOSED_ESCAPE state:
-        any character seen here is added literally, back to ENCLOSED_FIELD
-
-      ENCLOSED_EXPECT_DELIMITER state:
-        field sep goes to FIELD_START
-        record sep halts processing.
-        all other characters are errors.
-
-      UNENCLOSED_FIELD state:
-        ESCAPE char goes to UNENCLOSED_ESCAPE
-        FIELD_SEP char goes to FIELD_START
-        RECORD_SEP char halts processing
-        normal chars or the enclosing char get added to the current string
-
-      UNENCLOSED_ESCAPE:
-        add charater literal to current string, return to UNENCLOSED_FIELD
-    */
-
-    char curChar = '\000';
-    ParseState state = ParseState.FIELD_START;
-    int len = input.length();
-    StringBuilder sb = null;
-
-    outputs.clear();
-
-    for (int pos = 0; pos < len; pos++) {
-      curChar = input.get();
-      switch (state) {
-      case FIELD_START:
-        // ready to start processing a new field.
-        if (null != sb) {
-          // We finished processing a previous field. Add to the list.
-          outputs.add(sb.toString());
-        }
-
-        sb = new StringBuilder();
-        if (this.enclosingChar == curChar) {
-          // got an opening encloser.
-          state = ParseState.ENCLOSED_FIELD;
-        } else if (this.escapeChar == curChar) {
-          state = ParseState.UNENCLOSED_ESCAPE;
-        } else if (this.fieldDelim == curChar) {
-          // we have a zero-length field. This is a no-op.
-        } else if (this.recordDelim == curChar) {
-          // we have a zero-length field, that ends processing.
-          pos = len;
-        } else {
-          // current char is part of the field.
-          state = ParseState.UNENCLOSED_FIELD;
-          sb.append(curChar);
-
-          if (this.enclosingRequired) {
-            throw new ParseError("Opening field-encloser expected at position " + pos);
-          }
-        }
-
-        break;
-
-      case ENCLOSED_FIELD:
-        if (this.escapeChar == curChar) {
-          // the next character is escaped. Treat it literally.
-          state = ParseState.ENCLOSED_ESCAPE;
-        } else if (this.enclosingChar == curChar) {
-          // we're at the end of the enclosing field. Expect an EOF or EOR char.
-          state = ParseState.ENCLOSED_EXPECT_DELIMITER;
-        } else {
-          // this is a regular char, or an EOF / EOR inside an encloser. Add to
-          // the current field string, and remain in this state.
-          sb.append(curChar);
-        }
-
-        break;
-
-      case UNENCLOSED_FIELD:
-        if (this.escapeChar == curChar) {
-          // the next character is escaped. Treat it literally.
-          state = ParseState.UNENCLOSED_ESCAPE;
-        } else if (this.fieldDelim == curChar) {
-          // we're at the end of this field; may be the start of another one.
-          state = ParseState.FIELD_START;
-        } else if (this.recordDelim == curChar) {
-          pos = len; // terminate processing immediately.
-        } else {
-          // this is a regular char. Add to the current field string,
-          // and remain in this state.
-          sb.append(curChar);
-        }
-
-        break;
-        
-      case ENCLOSED_ESCAPE:
-        // Treat this character literally, whatever it is, and return to enclosed
-        // field processing.
-        sb.append(curChar);
-        state = ParseState.ENCLOSED_FIELD;
-        break;
-
-      case ENCLOSED_EXPECT_DELIMITER:
-        // We were in an enclosed field, but got the final encloser. Now we expect
-        // either an end-of-field or an end-of-record.
-        if (this.fieldDelim == curChar) {
-          // end of one field is the beginning of the next.
-          state = ParseState.FIELD_START;
-        } else if (this.recordDelim == curChar) {
-          // stop processing.
-          pos = len;
-        } else {
-          // Don't know what to do with this character.
-          throw new ParseError("Expected delimiter at position " + pos);
-        }
-
-        break;
-
-      case UNENCLOSED_ESCAPE:
-        // Treat this character literally, whatever it is, and return to non-enclosed
-        // field processing.
-        sb.append(curChar);
-        state = ParseState.UNENCLOSED_FIELD;
-        break;
-      }
-    }
-
-    if (state == ParseState.FIELD_START && curChar == this.fieldDelim) {
-      // we hit an EOF/EOR as the last legal character and we need to mark
-      // that string as recorded. This if block is outside the for-loop since
-      // we don't have a physical 'epsilon' token in our string.
-      if (null != sb) {
-        outputs.add(sb.toString());
-        sb = new StringBuilder();
-      }
-    }
-
-    if (null != sb) {
-      // There was a field that terminated by running out of chars or an EOR
-      // character. Add to the list.
-      outputs.add(sb.toString());
-    }
-
-    return outputs;
-  }
-
-
-  public boolean isEnclosingRequired() { 
-    return enclosingRequired;
-  }
-
-  @Override
-  public String toString() {
-    return "RecordParser[" + fieldDelim + ',' + recordDelim + ',' + enclosingChar + ','
-        + escapeChar + ',' + enclosingRequired + "]";
-  }
-
-  @Override
-  public int hashCode() {
-    return this.toString().hashCode();
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/SqoopRecord.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/SqoopRecord.java
deleted file mode 100644
index 2b51083..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/SqoopRecord.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.lib.db.DBWritable;
-
-import java.nio.ByteBuffer;
-import java.nio.CharBuffer;
-
-/**
- * Interface implemented by the classes generated by sqoop's orm.ClassWriter.
- */
-public interface SqoopRecord extends DBWritable, Writable {
-  public void parse(CharSequence s) throws RecordParser.ParseError;
-  public void parse(Text s) throws RecordParser.ParseError;
-  public void parse(byte [] s) throws RecordParser.ParseError;
-  public void parse(char [] s) throws RecordParser.ParseError;
-  public void parse(ByteBuffer s) throws RecordParser.ParseError;
-  public void parse(CharBuffer s) throws RecordParser.ParseError;
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ConnManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ConnManager.java
deleted file mode 100644
index 493bb81..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ConnManager.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-
-import org.apache.hadoop.sqoop.util.ExportException;
-import org.apache.hadoop.sqoop.util.ImportException;
-
-/**
- * Abstract interface that manages connections to a database.
- * The implementations of this class drive the actual discussion with
- * the database about table formats, etc.
- */
-public abstract class ConnManager {
-
-  /**
-   * Return a list of all databases on a server
-   */
-  public abstract String [] listDatabases();
-
-  /**
-   * Return a list of all tables in a database
-   */
-  public abstract String [] listTables();
-
-  /**
-   * Return a list of column names in a table in the order returned by the db.
-   */
-  public abstract String [] getColumnNames(String tableName);
-
-  /**
-   * Return the name of the primary key for a table, or null if there is none.
-   */
-  public abstract String getPrimaryKey(String tableName);
-
-  /**
-   * Return java type for SQL type
-   * @param sqlType     sql type
-   * @return            java type
-   */
-  public abstract String toJavaType(int sqlType);
-
-    /**
-     * Return hive type for SQL type
-     * @param sqlType   sql type
-     * @return          hive type
-     */
-  public abstract String toHiveType(int sqlType);
-
-  /**
-   * Return an unordered mapping from colname to sqltype for
-   * all columns in a table.
-   *
-   * The Integer type id is a constant from java.sql.Types
-   */
-  public abstract Map<String, Integer> getColumnTypes(String tableName);
-
-  /**
-   * Execute a SQL statement to read the named set of columns from a table.
-   * If columns is null, all columns from the table are read. This is a direct
-   * (non-parallelized) read of the table back to the current client.
-   * The client is responsible for calling ResultSet.close() when done with the
-   * returned ResultSet object.
-   */
-  public abstract ResultSet readTable(String tableName, String [] columns) throws SQLException;
-
-  /**
-   * @return the actual database connection
-   */
-  public abstract Connection getConnection() throws SQLException;
-
-  /**
-   * @return a string identifying the driver class to load for this JDBC connection type.
-   */
-  public abstract String getDriverClass();
-
-  /**
-   * Execute a SQL statement 's' and print its results to stdout
-   */
-  public abstract void execAndPrint(String s);
-
-  /**
-   * Perform an import of a table from the database into HDFS
-   */
-  public abstract void importTable(ImportJobContext context)
-      throws IOException, ImportException;
-
-  /**
-   * When using a column name in a generated SQL query, how (if at all)
-   * should we escape that column name? e.g., a column named "table"
-   * may need to be quoted with backtiks: "`table`".
-   *
-   * @param colName the column name as provided by the user, etc.
-   * @return how the column name should be rendered in the sql text.
-   */
-  public String escapeColName(String colName) {
-    return colName;
-  }
-
-  /**
-   * When using a table name in a generated SQL query, how (if at all)
-   * should we escape that column name? e.g., a table named "table"
-   * may need to be quoted with backtiks: "`table`".
-   *
-   * @param tableName the table name as provided by the user, etc.
-   * @return how the table name should be rendered in the sql text.
-   */
-  public String escapeTableName(String tableName) {
-    return tableName;
-  }
-
-  /**
-   * Perform any shutdown operations on the connection.
-   */
-  public abstract void close() throws SQLException;
-
-  /**
-   * Export data stored in HDFS into a table in a database
-   */
-  public void exportTable(ExportJobContext context)
-      throws IOException, ExportException {
-    throw new ExportException("This database does not support exports");
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DefaultManagerFactory.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DefaultManagerFactory.java
deleted file mode 100644
index e6a7a2e..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DefaultManagerFactory.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Contains instantiation code for all ConnManager implementations
- * shipped and enabled by default in Sqoop.
- */
-public final class DefaultManagerFactory extends ManagerFactory {
-
-  public static final Log LOG = LogFactory.getLog(DefaultManagerFactory.class.getName());
-
-  public ConnManager accept(SqoopOptions options) {
-    String manualDriver = options.getDriverClassName();
-    if (manualDriver != null) {
-      // User has manually specified JDBC implementation with --driver.
-      // Just use GenericJdbcManager.
-      return new GenericJdbcManager(manualDriver, options);
-    }
-
-    String connectStr = options.getConnectString();
-
-    // java.net.URL follows RFC-2396 literally, which does not allow a ':'
-    // character in the scheme component (section 3.1). JDBC connect strings,
-    // however, commonly have a multi-scheme addressing system. e.g.,
-    // jdbc:mysql://...; so we cannot parse the scheme component via URL
-    // objects. Instead, attempt to pull out the scheme as best as we can.
-
-    // First, see if this is of the form [scheme://hostname-and-etc..]
-    int schemeStopIdx = connectStr.indexOf("//");
-    if (-1 == schemeStopIdx) {
-      // If no hostname start marker ("//"), then look for the right-most ':'
-      // character.
-      schemeStopIdx = connectStr.lastIndexOf(':');
-      if (-1 == schemeStopIdx) {
-        // Warn that this is nonstandard. But we should be as permissive
-        // as possible here and let the ConnectionManagers themselves throw
-        // out the connect string if it doesn't make sense to them.
-        LOG.warn("Could not determine scheme component of connect string");
-
-        // Use the whole string.
-        schemeStopIdx = connectStr.length();
-      }
-    }
-
-    String scheme = connectStr.substring(0, schemeStopIdx);
-
-    if (null == scheme) {
-      // We don't know if this is a mysql://, hsql://, etc.
-      // Can't do anything with this.
-      LOG.warn("Null scheme associated with connect string.");
-      return null;
-    }
-
-    LOG.debug("Trying with scheme: " + scheme);
-
-    if (scheme.equals("jdbc:mysql:")) {
-      if (options.isDirect()) {
-        return new LocalMySQLManager(options);
-      } else {
-        return new MySQLManager(options);
-      }
-    } else if (scheme.equals("jdbc:postgresql:")) {
-      if (options.isDirect()) {
-        return new DirectPostgresqlManager(options);
-      } else {
-        return new PostgresqlManager(options);
-      }
-    } else if (scheme.startsWith("jdbc:hsqldb:")) {
-      return new HsqldbManager(options);
-    } else if (scheme.startsWith("jdbc:oracle:")) {
-      return new OracleManager(options);
-    } else {
-      return null;
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DirectPostgresqlManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DirectPostgresqlManager.java
deleted file mode 100644
index 7c80f70..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/DirectPostgresqlManager.java
+++ /dev/null
@@ -1,459 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.io.SplittableBufferedWriter;
-import org.apache.hadoop.sqoop.util.AsyncSink;
-import org.apache.hadoop.sqoop.util.DirectImportUtils;
-import org.apache.hadoop.sqoop.util.ErrorableAsyncSink;
-import org.apache.hadoop.sqoop.util.ErrorableThread;
-import org.apache.hadoop.sqoop.util.Executor;
-import org.apache.hadoop.sqoop.util.ImportException;
-import org.apache.hadoop.sqoop.util.JdbcUrl;
-import org.apache.hadoop.sqoop.util.LoggingAsyncSink;
-import org.apache.hadoop.sqoop.util.PerfCounters;
-
-/**
- * Manages direct dumps from Postgresql databases via psql COPY TO STDOUT
- * commands.
- */
-public class DirectPostgresqlManager extends PostgresqlManager {
-  public static final Log LOG = LogFactory.getLog(DirectPostgresqlManager.class.getName());
-
-  public DirectPostgresqlManager(final SqoopOptions opts) {
-    // Inform superclass that we're overriding import method via alt. constructor.
-    super(opts, true);
-  }
-
-  private static final String PSQL_CMD = "psql";
-
-  /** Copies data directly into HDFS, adding the user's chosen line terminator
-      char to each record.
-    */
-  static class PostgresqlAsyncSink extends ErrorableAsyncSink {
-    private final SplittableBufferedWriter writer;
-    private final PerfCounters counters;
-    private final SqoopOptions options;
-
-    PostgresqlAsyncSink(final SplittableBufferedWriter w, final SqoopOptions opts,
-        final PerfCounters ctrs) {
-      this.writer = w;
-      this.options = opts;
-      this.counters = ctrs;
-    }
-
-    public void processStream(InputStream is) {
-      child = new PostgresqlStreamThread(is, writer, options, counters);
-      child.start();
-    }
-
-    private static class PostgresqlStreamThread extends ErrorableThread {
-      public static final Log LOG = LogFactory.getLog(PostgresqlStreamThread.class.getName());
-
-      private final SplittableBufferedWriter writer;
-      private final InputStream stream;
-      private final SqoopOptions options;
-      private final PerfCounters counters;
-
-      PostgresqlStreamThread(final InputStream is, final SplittableBufferedWriter w,
-          final SqoopOptions opts, final PerfCounters ctrs) {
-        this.stream = is;
-        this.writer = w;
-        this.options = opts;
-        this.counters = ctrs;
-      }
-
-      public void run() {
-        BufferedReader r = null;
-        SplittableBufferedWriter w = this.writer;
-
-        char recordDelim = this.options.getOutputRecordDelim();
-
-        try {
-          r = new BufferedReader(new InputStreamReader(this.stream));
-
-          // read/write transfer loop here.
-          while (true) {
-            String inLine = r.readLine();
-            if (null == inLine) {
-              break; // EOF
-            }
-
-            w.write(inLine);
-            w.write(recordDelim);
-            w.allowSplit();
-            counters.addBytes(1 + inLine.length());
-          }
-        } catch (IOException ioe) {
-          LOG.error("IOException reading from psql: " + ioe.toString());
-          // set the error bit so our caller can see that something went wrong.
-          setError();
-        } finally {
-          if (null != r) {
-            try {
-              r.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing FIFO stream: " + ioe.toString());
-            }
-          }
-
-          if (null != w) {
-            try {
-              w.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing HDFS stream: " + ioe.toString());
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-    Takes a list of columns and turns them into a string like "col1, col2, col3..."
-   */
-  private String getColumnListStr(String [] cols) {
-    if (null == cols) {
-      return null;
-    }
-
-    StringBuilder sb = new StringBuilder();
-    boolean first = true;
-    for (String col : cols) {
-      if (!first) {
-        sb.append(", ");
-      }
-      sb.append(col);
-      first = false;
-    }
-
-    return sb.toString();
-  }
-
-  /**
-   * @return the Postgresql-specific SQL command to copy the table ("COPY .... TO STDOUT")
-   */
-  private String getCopyCommand(String tableName) {
-    /*
-       Format of this command is:
-
-          COPY table(col, col....) TO STDOUT 
-      or  COPY ( query ) TO STDOUT
-        WITH DELIMITER 'fieldsep'
-        CSV
-        QUOTE 'quotechar'
-        ESCAPE 'escapechar'
-        FORCE QUOTE col, col, col....
-    */
-
-    StringBuilder sb = new StringBuilder();
-    String [] cols = getColumnNames(tableName);
-
-    sb.append("COPY ");
-    String whereClause = this.options.getWhereClause();
-    if (whereClause != null && whereClause.length() > 0) {
-      // Import from a SELECT QUERY
-      sb.append("(");
-      sb.append("SELECT ");
-      if (null != cols) {
-        sb.append(getColumnListStr(cols));
-      } else {
-        sb.append("*");
-      }
-
-      sb.append(" FROM ");
-      sb.append(tableName);
-      sb.append(" WHERE ");
-      sb.append(whereClause);
-      sb.append(")");
-    } else {
-      // Import just the table.
-      sb.append(tableName);
-      if (null != cols) {
-        // specify columns.
-        sb.append("(");
-        sb.append(getColumnListStr(cols));
-        sb.append(")");
-      }
-    }
-
-    // Translate delimiter characters to '\ooo' octal representation.
-    sb.append(" TO STDOUT WITH DELIMITER E'\\");
-    sb.append(Integer.toString((int) this.options.getOutputFieldDelim(), 8));
-    sb.append("' CSV ");
-    if (this.options.getOutputEnclosedBy() != '\0') {
-      sb.append("QUOTE E'\\");
-      sb.append(Integer.toString((int) this.options.getOutputEnclosedBy(), 8));
-      sb.append("' ");
-    }
-    if (this.options.getOutputEscapedBy() != '\0') {
-      sb.append("ESCAPE E'\\");
-      sb.append(Integer.toString((int) this.options.getOutputEscapedBy(), 8));
-      sb.append("' ");
-    }
-
-    // add the "FORCE QUOTE col, col, col..." clause if quotes are required.
-    if (null != cols && this.options.isOutputEncloseRequired()) {
-      sb.append("FORCE QUOTE ");
-      sb.append(getColumnListStr(cols));
-    }
-
-    sb.append(";");
-
-    String copyCmd = sb.toString();
-    LOG.debug("Copy command is " + copyCmd);
-    return copyCmd;
-  }
-
-  /** Write the COPY command to a temp file
-    * @return the filename we wrote to.
-    */
-  private String writeCopyCommand(String command) throws IOException {
-    String tmpDir = options.getTempDir();
-    File tempFile = File.createTempFile("tmp-",".sql", new File(tmpDir));
-    BufferedWriter w = new BufferedWriter(
-        new OutputStreamWriter(new FileOutputStream(tempFile)));
-    w.write(command);
-    w.newLine();
-    w.close();
-    return tempFile.toString();
-  }
-
-  /** Write the user's password to a file that is chmod 0600.
-      @return the filename.
-    */
-  private String writePasswordFile(String password) throws IOException {
-
-    String tmpDir = options.getTempDir();
-    File tempFile = File.createTempFile("pgpass",".pgpass", new File(tmpDir));
-    LOG.debug("Writing password to tempfile: " + tempFile);
-
-    // Make sure it's only readable by the current user.
-    DirectImportUtils.setFilePermissions(tempFile, "0600");
-
-    // Actually write the password data into the file.
-    BufferedWriter w = new BufferedWriter(
-        new OutputStreamWriter(new FileOutputStream(tempFile)));
-    w.write("*:*:*:*:" + password);
-    w.close();
-    return tempFile.toString();
-  }
-
-  @Override
-  /**
-   * Import the table into HDFS by using psql to pull the data out of the db
-   * via COPY FILE TO STDOUT.
-   */
-  public void importTable(ImportJobContext context)
-    throws IOException, ImportException {
-
-    String tableName = context.getTableName();
-    String jarFile = context.getJarFile();
-    SqoopOptions options = context.getOptions();
-
-    LOG.info("Beginning psql fast path import");
-
-    if (options.getFileLayout() != SqoopOptions.FileLayout.TextFile) {
-      // TODO(aaron): Support SequenceFile-based load-in
-      LOG.warn("File import layout" + options.getFileLayout()
-          + " is not supported by");
-      LOG.warn("Postgresql direct import; import will proceed as text files.");
-    }
-
-    String commandFilename = null;
-    String passwordFilename = null;
-    Process p = null;
-    AsyncSink sink = null;
-    AsyncSink errSink = null;
-    PerfCounters counters = new PerfCounters();
-
-    try {
-      // Get the COPY TABLE command to issue, write this to a file, and pass it
-      // in to psql with -f filename.
-      // Then make sure we delete this file in our finally block.
-      String copyCmd = getCopyCommand(tableName);
-      commandFilename = writeCopyCommand(copyCmd);
-
-      // Arguments to pass to psql on the command line.
-      ArrayList<String> args = new ArrayList<String>();
-
-      // Environment to pass to psql.
-      List<String> envp = Executor.getCurEnvpStrings();
-
-      // We need to parse the connect string URI to determine the database name
-      // and the host and port. If the host is localhost and the port is not specified,
-      // we don't want to pass this to psql, because we want to force the use of a
-      // UNIX domain socket, not a TCP/IP socket.
-      String connectString = options.getConnectString();
-      String databaseName = JdbcUrl.getDatabaseName(connectString);
-      String hostname = JdbcUrl.getHostName(connectString);
-      int port = JdbcUrl.getPort(connectString);
-
-      if (null == databaseName) {
-        throw new ImportException("Could not determine database name");
-      }
-
-      LOG.info("Performing import of table " + tableName + " from database " + databaseName);
-      args.add(PSQL_CMD); // requires that this is on the path.
-      args.add("--tuples-only");
-      args.add("--quiet");
-
-      String username = options.getUsername();
-      if (username != null) {
-        args.add("--username");
-        args.add(username);
-        String password = options.getPassword();
-        if (null != password) {
-          passwordFilename = writePasswordFile(password);
-          // Need to send PGPASSFILE environment variable specifying
-          // location of our postgres file.
-          envp.add("PGPASSFILE=" + passwordFilename);
-        }
-      }
-
-      if (!DirectImportUtils.isLocalhost(hostname) || port != -1) {
-        args.add("--host");
-        args.add(hostname);
-        args.add("--port");
-        args.add(Integer.toString(port));
-      }
-
-      if (null != databaseName && databaseName.length() > 0) {
-        args.add(databaseName);
-      }
-
-      // The COPY command is in a script file.
-      args.add("-f");
-      args.add(commandFilename);
-
-      // begin the import in an external process.
-      LOG.debug("Starting psql with arguments:");
-      for (String arg : args) {
-        LOG.debug("  " + arg);
-      }
-
-      // This writer will be closed by AsyncSink.
-      SplittableBufferedWriter w = DirectImportUtils.createHdfsSink(
-          options.getConf(), options, tableName);
-
-      // Actually start the psql dump.
-      p = Runtime.getRuntime().exec(args.toArray(new String[0]),
-          envp.toArray(new String[0]));
-
-      // read from the stdout pipe into the HDFS writer.
-      InputStream is = p.getInputStream();
-      sink = new PostgresqlAsyncSink(w, options, counters);
-
-      LOG.debug("Starting stream sink");
-      counters.startClock();
-      sink.processStream(is);
-      errSink = new LoggingAsyncSink(LOG);
-      errSink.processStream(p.getErrorStream());
-    } finally {
-      // block until the process is done.
-      LOG.debug("Waiting for process completion");
-      int result = 0;
-      if (null != p) {
-        while (true) {
-          try {
-            result = p.waitFor();
-          } catch (InterruptedException ie) {
-            // interrupted; loop around.
-            continue;
-          }
-
-          break;
-        }
-      }
-
-      // Remove any password file we wrote
-      if (null != passwordFilename) {
-        if (!new File(passwordFilename).delete()) {
-          LOG.error("Could not remove postgresql password file " + passwordFilename);
-          LOG.error("You should remove this file to protect your credentials.");
-        }
-      }
-
-      if (null != commandFilename) {
-        // We wrote the COPY comand to a tmpfile. Remove it.
-        if (!new File(commandFilename).delete()) {
-          LOG.info("Could not remove temp file: " + commandFilename);
-        }
-      }
-
-      // block until the stream sink is done too.
-      int streamResult = 0;
-      if (null != sink) {
-        while (true) {
-          try {
-            streamResult = sink.join();
-          } catch (InterruptedException ie) {
-            // interrupted; loop around.
-            continue;
-          }
-
-          break;
-        }
-      }
-
-      // Attempt to block for stderr stream sink; errors are advisory.
-      if (null != errSink) {
-        try {
-          if (0 != errSink.join()) {
-            LOG.info("Encountered exception reading stderr stream");
-          }
-        } catch (InterruptedException ie) {
-          LOG.info("Thread interrupted waiting for stderr to complete: "
-              + ie.toString());
-        }
-      }
-
-      LOG.info("Transfer loop complete.");
-
-      if (0 != result) {
-        throw new IOException("psql terminated with status "
-            + Integer.toString(result));
-      }
-
-      if (0 != streamResult) {
-        throw new IOException("Encountered exception in stream sink");
-      }
-
-      counters.stopClock();
-      LOG.info("Transferred " + counters.toString());
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ExportJobContext.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ExportJobContext.java
deleted file mode 100644
index cb32a51..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ExportJobContext.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-/**
- * A set of parameters describing an export operation; this is passed to
- * ConnManager.exportTable() as its argument.
- */
-public class ExportJobContext {
-
-  private String tableName;
-  private String jarFile;
-  private SqoopOptions options;
-
-  public ExportJobContext(final String table, final String jar, final SqoopOptions opts) {
-    this.tableName = table;
-    this.jarFile = jar;
-    this.options = opts;
-  }
-
-  /** @return the name of the table to export. */
-  public String getTableName() {
-    return tableName;
-  }
-
-  /** @return the name of the jar file containing the user's compiled
-   * ORM classes to use during the export.
-   */
-  public String getJarFile() {
-    return jarFile;
-  }
-
-  /** @return the SqoopOptions configured by the user */
-  public SqoopOptions getOptions() {
-    return options;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/GenericJdbcManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/GenericJdbcManager.java
deleted file mode 100644
index 104ce67..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/GenericJdbcManager.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.sql.Connection;
-import java.sql.SQLException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-/**
- * Database manager that is connects to a generic JDBC-compliant
- * database; its constructor is parameterized on the JDBC Driver
- * class to load.
- */
-public class GenericJdbcManager extends SqlManager {
-
-  public static final Log LOG = LogFactory.getLog(GenericJdbcManager.class.getName());
-
-  private String jdbcDriverClass;
-  private Connection connection;
-
-  public GenericJdbcManager(final String driverClass, final SqoopOptions opts) {
-    super(opts);
-
-    this.jdbcDriverClass = driverClass;
-  }
-
-  @Override
-  public Connection getConnection() throws SQLException {
-    if (null == this.connection) {
-      this.connection = makeConnection();
-    }
-
-    return this.connection;
-  }
-
-  protected boolean hasOpenConnection() {
-    return this.connection != null;
-  }
-
-  public void close() throws SQLException {
-    super.close();
-    if (null != this.connection) {
-      this.connection.close();
-      this.connection = null;
-    }
-  }
-
-  public String getDriverClass() {
-    return jdbcDriverClass;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/HsqldbManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/HsqldbManager.java
deleted file mode 100644
index 210dfb0..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/HsqldbManager.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-/**
- * Manages connections to hsqldb databases.
- * Extends generic SQL manager.
- */
-public class HsqldbManager extends GenericJdbcManager {
-
-  public static final Log LOG = LogFactory.getLog(HsqldbManager.class.getName());
-
-  // driver class to ensure is loaded when making db connection.
-  private static final String DRIVER_CLASS = "org.hsqldb.jdbcDriver";
-
-  // HsqlDb doesn't have a notion of multiple "databases"; the user's database is always called
-  // "PUBLIC";
-  private static final String HSQL_SCHEMA_NAME = "PUBLIC";
-
-  public HsqldbManager(final SqoopOptions opts) {
-    super(DRIVER_CLASS, opts);
-  }
-
-  /**
-   * Note: HSqldb only supports a single schema named "PUBLIC"
-   */
-  @Override
-  public String[] listDatabases() {
-    String [] databases = {HSQL_SCHEMA_NAME};
-    return databases;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ImportJobContext.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ImportJobContext.java
deleted file mode 100644
index 06c2022..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ImportJobContext.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-/**
- * A set of parameters describing an import operation; this is passed to
- * ConnManager.importTable() as its argument.
- */
-public class ImportJobContext {
-
-  private String tableName;
-  private String jarFile;
-  private SqoopOptions options;
-
-  public ImportJobContext(final String table, final String jar, final SqoopOptions opts) {
-    this.tableName = table;
-    this.jarFile = jar;
-    this.options = opts;
-  }
-
-  /** @return the name of the table to import. */
-  public String getTableName() {
-    return tableName;
-  }
-
-  /** @return the name of the jar file containing the user's compiled
-   * ORM classes to use during the import.
-   */
-  public String getJarFile() {
-    return jarFile;
-  }
-
-  /** @return the SqoopOptions configured by the user */
-  public SqoopOptions getOptions() {
-    return options;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/LocalMySQLManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/LocalMySQLManager.java
deleted file mode 100644
index 0180c06..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/LocalMySQLManager.java
+++ /dev/null
@@ -1,532 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.nio.CharBuffer;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.io.SplittableBufferedWriter;
-import org.apache.hadoop.sqoop.lib.FieldFormatter;
-import org.apache.hadoop.sqoop.lib.RecordParser;
-import org.apache.hadoop.sqoop.util.AsyncSink;
-import org.apache.hadoop.sqoop.util.DirectImportUtils;
-import org.apache.hadoop.sqoop.util.ErrorableAsyncSink;
-import org.apache.hadoop.sqoop.util.ErrorableThread;
-import org.apache.hadoop.sqoop.util.ImportException;
-import org.apache.hadoop.sqoop.util.JdbcUrl;
-import org.apache.hadoop.sqoop.util.LoggingAsyncSink;
-import org.apache.hadoop.sqoop.util.PerfCounters;
-
-/**
- * Manages direct connections to MySQL databases
- * so we can use mysqldump to get really fast dumps.
- */
-public class LocalMySQLManager extends MySQLManager {
-
-  public static final Log LOG = LogFactory.getLog(LocalMySQLManager.class.getName());
-
-  // AsyncSinks used to import data from mysqldump directly into HDFS.
-
-  /**
-   * Copies data directly from mysqldump into HDFS, after stripping some
-   * header and footer characters that are attached to each line in mysqldump.
-   */
-  static class CopyingAsyncSink extends ErrorableAsyncSink {
-    private final SplittableBufferedWriter writer;
-    private final PerfCounters counters;
-
-    CopyingAsyncSink(final SplittableBufferedWriter w,
-        final PerfCounters ctrs) {
-      this.writer = w;
-      this.counters = ctrs;
-    }
-
-    public void processStream(InputStream is) {
-      child = new CopyingStreamThread(is, writer, counters);
-      child.start();
-    }
-
-    private static class CopyingStreamThread extends ErrorableThread {
-      public static final Log LOG = LogFactory.getLog(
-          CopyingStreamThread.class.getName());
-
-      private final SplittableBufferedWriter writer;
-      private final InputStream stream;
-      private final PerfCounters counters;
-
-      CopyingStreamThread(final InputStream is,
-          final SplittableBufferedWriter w, final PerfCounters ctrs) {
-        this.writer = w;
-        this.stream = is;
-        this.counters = ctrs;
-      }
-
-      public void run() {
-        BufferedReader r = null;
-        SplittableBufferedWriter w = this.writer;
-
-        try {
-          r = new BufferedReader(new InputStreamReader(this.stream));
-
-          // Actually do the read/write transfer loop here.
-          int preambleLen = -1; // set to this for "undefined"
-          while (true) {
-            String inLine = r.readLine();
-            if (null == inLine) {
-              break; // EOF.
-            }
-
-            // this line is of the form "INSERT .. VALUES ( actual value text );"
-            // strip the leading preamble up to the '(' and the trailing ');'.
-            if (preambleLen == -1) {
-              // we haven't determined how long the preamble is. It's constant
-              // across all lines, so just figure this out once.
-              String recordStartMark = "VALUES (";
-              preambleLen = inLine.indexOf(recordStartMark) + recordStartMark.length();
-            }
-
-            // chop off the leading and trailing text as we write the
-            // output to HDFS.
-            int len = inLine.length() - 2 - preambleLen;
-            w.write(inLine, preambleLen, len);
-            w.newLine();
-            counters.addBytes(1 + len);
-          }
-        } catch (IOException ioe) {
-          LOG.error("IOException reading from mysqldump: " + ioe.toString());
-          // flag this error so we get an error status back in the caller.
-          setError();
-        } finally {
-          if (null != r) {
-            try {
-              r.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing FIFO stream: " + ioe.toString());
-            }
-          }
-
-          if (null != w) {
-            try {
-              w.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing HDFS stream: " + ioe.toString());
-            }
-          }
-        }
-      }
-    }
-  }
-
-
-  /**
-   * The ReparsingAsyncSink will instantiate a RecordParser to read mysqldump's
-   * output, and re-emit the text in the user's specified output format.
-   */
-  static class ReparsingAsyncSink extends ErrorableAsyncSink {
-    private final SplittableBufferedWriter writer;
-    private final SqoopOptions options;
-    private final PerfCounters counters;
-
-    ReparsingAsyncSink(final SplittableBufferedWriter w,
-        final SqoopOptions opts, final PerfCounters ctrs) {
-      this.writer = w;
-      this.options = opts;
-      this.counters = ctrs;
-    }
-
-    public void processStream(InputStream is) {
-      child = new ReparsingStreamThread(is, writer, options, counters);
-      child.start();
-    }
-
-    private static class ReparsingStreamThread extends ErrorableThread {
-      public static final Log LOG = LogFactory.getLog(
-          ReparsingStreamThread.class.getName());
-
-      private final SplittableBufferedWriter writer;
-      private final SqoopOptions options;
-      private final InputStream stream;
-      private final PerfCounters counters;
-
-      ReparsingStreamThread(final InputStream is,
-          final SplittableBufferedWriter w, final SqoopOptions opts,
-          final PerfCounters ctrs) {
-        this.writer = w;
-        this.options = opts;
-        this.stream = is;
-        this.counters = ctrs;
-      }
-
-      private static final char MYSQL_FIELD_DELIM = ',';
-      private static final char MYSQL_RECORD_DELIM = '\n';
-      private static final char MYSQL_ENCLOSE_CHAR = '\'';
-      private static final char MYSQL_ESCAPE_CHAR = '\\';
-      private static final boolean MYSQL_ENCLOSE_REQUIRED = false;
-
-      private static final RecordParser MYSQLDUMP_PARSER;
-
-      static {
-        // build a record parser for mysqldump's format
-        MYSQLDUMP_PARSER = new RecordParser(MYSQL_FIELD_DELIM,
-            MYSQL_RECORD_DELIM, MYSQL_ENCLOSE_CHAR, MYSQL_ESCAPE_CHAR,
-            MYSQL_ENCLOSE_REQUIRED);
-      }
-
-      public void run() {
-        BufferedReader r = null;
-        SplittableBufferedWriter w = this.writer;
-
-        try {
-          r = new BufferedReader(new InputStreamReader(this.stream));
-
-          char outputFieldDelim = options.getOutputFieldDelim();
-          char outputRecordDelim = options.getOutputRecordDelim();
-          String outputEnclose = "" + options.getOutputEnclosedBy();
-          String outputEscape = "" + options.getOutputEscapedBy();
-          boolean outputEncloseRequired = options.isOutputEncloseRequired(); 
-          char [] encloseFor = { outputFieldDelim, outputRecordDelim };
-
-          // Actually do the read/write transfer loop here.
-          int preambleLen = -1; // set to this for "undefined"
-          while (true) {
-            String inLine = r.readLine();
-            if (null == inLine) {
-              break; // EOF.
-            }
-
-            // this line is of the form "INSERT .. VALUES ( actual value text );"
-            // strip the leading preamble up to the '(' and the trailing ');'.
-            if (preambleLen == -1) {
-              // we haven't determined how long the preamble is. It's constant
-              // across all lines, so just figure this out once.
-              String recordStartMark = "VALUES (";
-              preambleLen = inLine.indexOf(recordStartMark) + recordStartMark.length();
-            }
-
-            // Wrap the input string in a char buffer that ignores the leading and trailing
-            // text.
-            CharBuffer charbuf = CharBuffer.wrap(inLine, preambleLen, inLine.length() - 2);
-
-            // Pass this along to the parser
-            List<String> fields = null;
-            try {
-              fields = MYSQLDUMP_PARSER.parseRecord(charbuf);
-            } catch (RecordParser.ParseError pe) {
-              LOG.warn("ParseError reading from mysqldump: " + pe.toString() + "; record skipped");
-            }
-
-            // For all of the output fields, emit them using the delimiters the user chooses.
-            boolean first = true;
-            int recordLen = 1; // for the delimiter.
-            for (String field : fields) {
-              if (!first) {
-                w.write(outputFieldDelim);
-              } else {
-                first = false;
-              }
-
-              String fieldStr = FieldFormatter.escapeAndEnclose(field, outputEscape, outputEnclose,
-                  encloseFor, outputEncloseRequired);
-              w.write(fieldStr);
-              recordLen += fieldStr.length();
-            }
-
-            w.write(outputRecordDelim);
-            w.allowSplit();
-            counters.addBytes(recordLen);
-          }
-        } catch (IOException ioe) {
-          LOG.error("IOException reading from mysqldump: " + ioe.toString());
-          // flag this error so the parent can handle it appropriately.
-          setError();
-        } finally {
-          if (null != r) {
-            try {
-              r.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing FIFO stream: " + ioe.toString());
-            }
-          }
-
-          if (null != w) {
-            try {
-              w.close();
-            } catch (IOException ioe) {
-              LOG.info("Error closing HDFS stream: " + ioe.toString());
-            }
-          }
-        }
-      }
-    }
-  }
-
-
-  public LocalMySQLManager(final SqoopOptions options) {
-    super(options, false);
-  }
-
-  private static final String MYSQL_DUMP_CMD = "mysqldump";
-
-  /**
-   * @return true if the user's output delimiters match those used by mysqldump.
-   * fields: ,
-   * lines: \n
-   * optional-enclose: \'
-   * escape: \\
-   */
-  private boolean outputDelimsAreMySQL() {
-    return options.getOutputFieldDelim() == ','
-        && options.getOutputRecordDelim() == '\n'
-        && options.getOutputEnclosedBy() == '\''
-        && options.getOutputEscapedBy() == '\\'
-        && !options.isOutputEncloseRequired(); // encloser is optional
-  }
-  
-  /**
-   * Writes the user's password to a tmp file with 0600 permissions.
-   * @return the filename used.
-   */
-  private String writePasswordFile() throws IOException {
-    // Create the temp file to hold the user's password.
-    String tmpDir = options.getTempDir();
-    File tempFile = File.createTempFile("mysql-cnf",".cnf", new File(tmpDir));
-
-    // Make the password file only private readable.
-    DirectImportUtils.setFilePermissions(tempFile, "0600");
-
-    // If we're here, the password file is believed to be ours alone.
-    // The inability to set chmod 0600 inside Java is troublesome. We have to trust
-    // that the external 'chmod' program in the path does the right thing, and returns
-    // the correct exit status. But given our inability to re-read the permissions
-    // associated with a file, we'll have to make do with this.
-    String password = options.getPassword();
-    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(tempFile)));
-    w.write("[client]\n");
-    w.write("password=" + password + "\n");
-    w.close();
-
-    return tempFile.toString();
-  }
-
-  /**
-   * Import the table into HDFS by using mysqldump to pull out the data from
-   * the database and upload the files directly to HDFS.
-   */
-  public void importTable(ImportJobContext context)
-      throws IOException, ImportException {
-
-    String tableName = context.getTableName();
-    String jarFile = context.getJarFile();
-    SqoopOptions options = context.getOptions();
-
-    LOG.info("Beginning mysqldump fast path import");
-
-    if (options.getFileLayout() != SqoopOptions.FileLayout.TextFile) {
-      // TODO(aaron): Support SequenceFile-based load-in
-      LOG.warn("File import layout " + options.getFileLayout()
-          + " is not supported by");
-      LOG.warn("MySQL direct import; import will proceed as text files.");
-    }
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    // We need to parse the connect string URI to determine the database
-    // name. Using java.net.URL directly on the connect string will fail because
-    // Java doesn't respect arbitrary JDBC-based schemes. So we chop off the scheme
-    // (everything before '://') and replace it with 'http', which we know will work.
-    String connectString = options.getConnectString();
-    String databaseName = JdbcUrl.getDatabaseName(connectString);
-    String hostname = JdbcUrl.getHostName(connectString);
-    int port = JdbcUrl.getPort(connectString);
-
-    if (null == databaseName) {
-      throw new ImportException("Could not determine database name");
-    }
-
-    LOG.info("Performing import of table " + tableName + " from database " + databaseName);
-    args.add(MYSQL_DUMP_CMD); // requires that this is on the path.
-
-    String password = options.getPassword();
-    String passwordFile = null;
-
-    Process p = null;
-    AsyncSink sink = null;
-    AsyncSink errSink = null;
-    PerfCounters counters = new PerfCounters();
-    try {
-      // --defaults-file must be the first argument.
-      if (null != password && password.length() > 0) {
-        passwordFile = writePasswordFile();
-        args.add("--defaults-file=" + passwordFile);
-      }
-
-      String whereClause = options.getWhereClause();
-      if (null != whereClause) {
-        // Don't use the --where="<whereClause>" version because spaces in it can confuse
-        // Java, and adding in surrounding quotes confuses Java as well.
-        args.add("-w");
-        args.add(whereClause);
-      }
-
-      if (!DirectImportUtils.isLocalhost(hostname) || port != -1) {
-        args.add("--host=" + hostname);
-        args.add("--port=" + Integer.toString(port));
-      }
-
-      args.add("--skip-opt");
-      args.add("--compact");
-      args.add("--no-create-db");
-      args.add("--no-create-info");
-      args.add("--quick"); // no buffering
-      args.add("--single-transaction"); 
-
-      String username = options.getUsername();
-      if (null != username) {
-        args.add("--user=" + username);
-      }
-
-      // If the user supplied extra args, add them here.
-      String [] extra = options.getExtraArgs();
-      if (null != extra) {
-        for (String arg : extra) {
-          args.add(arg);
-        }
-      }
-
-      args.add(databaseName);
-      args.add(tableName);
-
-      // begin the import in an external process.
-      LOG.debug("Starting mysqldump with arguments:");
-      for (String arg : args) {
-        LOG.debug("  " + arg);
-      }
-
-      // This writer will be closed by AsyncSink.
-      SplittableBufferedWriter w = DirectImportUtils.createHdfsSink(
-          options.getConf(), options, tableName);
-
-      // Actually start the mysqldump.
-      p = Runtime.getRuntime().exec(args.toArray(new String[0]));
-
-      // read from the stdout pipe into the HDFS writer.
-      InputStream is = p.getInputStream();
-
-      if (outputDelimsAreMySQL()) {
-        LOG.debug("Output delimiters conform to mysqldump; using straight copy"); 
-        sink = new CopyingAsyncSink(w, counters);
-      } else {
-        LOG.debug("User-specified delimiters; using reparsing import");
-        LOG.info("Converting data to use specified delimiters.");
-        LOG.info("(For the fastest possible import, use");
-        LOG.info("--mysql-delimiters to specify the same field");
-        LOG.info("delimiters as are used by mysqldump.)");
-        sink = new ReparsingAsyncSink(w, options, counters);
-      }
-
-      // Start an async thread to read and upload the whole stream.
-      counters.startClock();
-      sink.processStream(is);
-
-      // Start an async thread to send stderr to log4j.
-      errSink = new LoggingAsyncSink(LOG);
-      errSink.processStream(p.getErrorStream());
-    } finally {
-
-      // block until the process is done.
-      int result = 0;
-      if (null != p) {
-        while (true) {
-          try {
-            result = p.waitFor();
-          } catch (InterruptedException ie) {
-            // interrupted; loop around.
-            continue;
-          }
-
-          break;
-        }
-      }
-
-      // Remove the password file.
-      if (null != passwordFile) {
-        if (!new File(passwordFile).delete()) {
-          LOG.error("Could not remove mysql password file " + passwordFile);
-          LOG.error("You should remove this file to protect your credentials.");
-        }
-      }
-
-      // block until the stream sink is done too.
-      int streamResult = 0;
-      if (null != sink) {
-        while (true) {
-          try {
-            streamResult = sink.join();
-          } catch (InterruptedException ie) {
-            // interrupted; loop around.
-            continue;
-          }
-
-          break;
-        }
-      }
-
-      // Try to wait for stderr to finish, but regard any errors as advisory.
-      if (null != errSink) {
-        try {
-          if (0 != errSink.join()) {
-            LOG.info("Encountered exception reading stderr stream");
-          }
-        } catch (InterruptedException ie) {
-          LOG.info("Thread interrupted waiting for stderr to complete: "
-              + ie.toString());
-        }
-      }
-
-      LOG.info("Transfer loop complete.");
-
-      if (0 != result) {
-        throw new IOException("mysqldump terminated with status "
-            + Integer.toString(result));
-      }
-
-      if (0 != streamResult) {
-        throw new IOException("Encountered exception in stream sink");
-      }
-
-      counters.stopClock();
-      LOG.info("Transferred " + counters.toString());
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ManagerFactory.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ManagerFactory.java
deleted file mode 100644
index 4bcd756..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/ManagerFactory.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-/**
- * Interface for factory classes for ConnManager implementations.
- * ManagerFactories are instantiated by o.a.h.s.ConnFactory and
- * stored in an ordered list. The ConnFactory.getManager() implementation
- * calls the accept() method of each ManagerFactory, in order until
- * one such call returns a non-null ConnManager instance.
- */
-public abstract class ManagerFactory {
-  public abstract ConnManager accept(SqoopOptions options);
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/MySQLManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/MySQLManager.java
deleted file mode 100644
index c9290be..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/MySQLManager.java
+++ /dev/null
@@ -1,229 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.util.ImportException;
-
-/**
- * Manages connections to MySQL databases
- */
-public class MySQLManager extends GenericJdbcManager {
-
-  public static final Log LOG = LogFactory.getLog(MySQLManager.class.getName());
-
-  // driver class to ensure is loaded when making db connection.
-  private static final String DRIVER_CLASS = "com.mysql.jdbc.Driver";
-
-  // set to true after we warn the user that we can use direct fastpath.
-  private static boolean warningPrinted = false;
-
-  public MySQLManager(final SqoopOptions opts) {
-    super(DRIVER_CLASS, opts);
-  }
-
-  protected MySQLManager(final SqoopOptions opts, boolean ignored) {
-    // constructor used by subclasses to avoid the --direct warning.
-    super(DRIVER_CLASS, opts);
-  }
-
-  @Override
-  protected String getColNamesQuery(String tableName) {
-    // Use mysql-specific hints and LIMIT to return fast
-    return "SELECT t.* FROM " + escapeTableName(tableName) + " AS t LIMIT 1";
-  }
-
-  @Override
-  public String[] listDatabases() {
-    // TODO(aaron): Add an automated unit test for this.
-
-    ResultSet results;
-    try {
-      results = execute("SHOW DATABASES");
-    } catch (SQLException sqlE) {
-      LOG.error("Error executing statement: " + sqlE.toString());
-      return null;
-    }
-
-    try {
-      ArrayList<String> databases = new ArrayList<String>();
-      while (results.next()) {
-        String dbName = results.getString(1);
-        databases.add(dbName);
-      }
-
-      return databases.toArray(new String[0]);
-    } catch (SQLException sqlException) {
-      LOG.error("Error reading from database: " + sqlException.toString());
-      return null;
-    } finally {
-      try {
-        results.close();
-      } catch (SQLException sqlE) {
-        LOG.warn("Exception closing ResultSet: " + sqlE.toString());
-      }
-    }
-  }
-
-  @Override
-  public void importTable(ImportJobContext context)
-        throws IOException, ImportException {
-
-    // Check that we're not doing a MapReduce from localhost. If we are, point
-    // out that we could use mysqldump.
-    if (!MySQLManager.warningPrinted) {
-      String connectString = context.getOptions().getConnectString();
-
-      if (null != connectString && connectString.indexOf("//localhost") != -1) {
-        // if we're not doing a remote connection, they should have a LocalMySQLManager.
-        // TODO(aaron): After LocalMySQLManager supports --host/--port, this should
-        // always be issued.
-        LOG.warn("It looks like you are importing from mysql on");
-        LOG.warn("localhost. This transfer can be faster! Use the");
-        LOG.warn("--direct option to exercise a MySQL-specific fast");
-        LOG.warn("path.");
-
-        MySQLManager.warningPrinted = true; // don't display this twice.
-      }
-    }
-
-    checkDateTimeBehavior(context);
-
-    // Then run the normal importTable() method.
-    super.importTable(context);
-  }
-
-  /**
-   * MySQL allows TIMESTAMP fields to have the value '0000-00-00 00:00:00',
-   * which causes errors in import. If the user has not set the
-   * zeroDateTimeBehavior property already, we set it for them to coerce
-   * the type to null.
-   */
-  private void checkDateTimeBehavior(ImportJobContext context) {
-    final String zeroBehaviorStr = "zeroDateTimeBehavior";
-    final String convertToNull = "=convertToNull";
-
-    String connectStr = context.getOptions().getConnectString();
-    if (connectStr.indexOf("jdbc:") != 0) {
-      // This connect string doesn't have the prefix we expect.
-      // We can't parse the rest of it here.
-      return;
-    }
-
-    // This starts with 'jdbc:mysql://' ... let's remove the 'jdbc:'
-    // prefix so that java.net.URI can parse the rest of the line.
-    String uriComponent = connectStr.substring(5);
-    try {
-      URI uri = new URI(uriComponent);
-      String query = uri.getQuery(); // get the part after a '?'
-
-      // If they haven't set the zeroBehavior option, set it to
-      // squash-null for them.
-      if (null == query) {
-        connectStr = connectStr + "?" + zeroBehaviorStr + convertToNull;
-        LOG.info("Setting zero DATETIME behavior to convertToNull (mysql)");
-      } else if (query.length() == 0) {
-        connectStr = connectStr + zeroBehaviorStr + convertToNull;
-        LOG.info("Setting zero DATETIME behavior to convertToNull (mysql)");
-      } else if (query.indexOf(zeroBehaviorStr) == -1) {
-        if (!connectStr.endsWith("&")) {
-          connectStr = connectStr + "&";
-        }
-        connectStr = connectStr + zeroBehaviorStr + convertToNull;
-        LOG.info("Setting zero DATETIME behavior to convertToNull (mysql)");
-      }
-
-      LOG.debug("Rewriting connect string to " + connectStr);
-      context.getOptions().setConnectString(connectStr);
-    } catch (URISyntaxException use) {
-      // Just ignore this. If we can't parse the URI, don't attempt
-      // to add any extra flags to it.
-      LOG.debug("mysql: Couldn't parse connect str in checkDateTimeBehavior: "
-          + use);
-    }
-  }
-
-  /**
-   * Executes an arbitrary SQL statement. Sets mysql-specific parameter
-   * to ensure the entire table is not buffered in RAM before reading
-   * any rows. A consequence of this is that every ResultSet returned
-   * by this method *MUST* be close()'d, or read to exhaustion before
-   * another query can be executed from this ConnManager instance.
-   *
-   * @param stmt The SQL statement to execute
-   * @return A ResultSet encapsulating the results or null on error
-   */
-  protected ResultSet execute(String stmt, Object... args) throws SQLException {
-    PreparedStatement statement = null;
-    statement = this.getConnection().prepareStatement(stmt,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    statement.setFetchSize(Integer.MIN_VALUE); // MySQL: read row-at-a-time.
-    if (null != args) {
-      for (int i = 0; i < args.length; i++) {
-        statement.setObject(i + 1, args[i]);
-      }
-    }
-
-    LOG.info("Executing SQL statement: " + stmt);
-    return statement.executeQuery();
-  }
-
-  /**
-   * When using a column name in a generated SQL query, how (if at all)
-   * should we escape that column name? e.g., a column named "table"
-   * may need to be quoted with backtiks: "`table`".
-   *
-   * @param colName the column name as provided by the user, etc.
-   * @return how the column name should be rendered in the sql text.
-   */
-  public String escapeColName(String colName) {
-    if (null == colName) {
-      return null;
-    }
-    return "`" + colName + "`";
-  }
-
-  /**
-   * When using a table name in a generated SQL query, how (if at all)
-   * should we escape that column name? e.g., a table named "table"
-   * may need to be quoted with backtiks: "`table`".
-   *
-   * @param tableName the table name as provided by the user, etc.
-   * @return how the table name should be rendered in the sql text.
-   */
-  public String escapeTableName(String tableName) {
-    if (null == tableName) {
-      return null;
-    }
-    return "`" + tableName + "`";
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/OracleManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/OracleManager.java
deleted file mode 100644
index e6db8e5..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/OracleManager.java
+++ /dev/null
@@ -1,272 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.DriverManager;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.sql.Types;
-import java.util.ArrayList;
-import java.util.TimeZone;
-import java.lang.reflect.Method;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.mapred.ImportJob;
-import org.apache.hadoop.sqoop.util.ImportException;
-
-/**
- * Manages connections to Oracle databases.
- * Requires MAPREDUCE-716 and the Oracle JDBC driver.
- */
-public class OracleManager extends GenericJdbcManager {
-
-  public static final Log LOG = LogFactory.getLog(OracleManager.class.getName());
-
-  // driver class to ensure is loaded when making db connection.
-  private static final String DRIVER_CLASS = "oracle.jdbc.OracleDriver";
-
-  public OracleManager(final SqoopOptions opts) {
-    super(DRIVER_CLASS, opts);
-  }
-
-  protected String getColNamesQuery(String tableName) {
-    // SqlManager uses "tableName AS t" which doesn't work in Oracle.
-    return "SELECT t.* FROM " + escapeTableName(tableName) + " t";
-  }
-
-  /**
-   * Create a connection to the database; usually used only from within
-   * getConnection(), which enforces a singleton guarantee around the
-   * Connection object.
-   *
-   * Oracle-specific driver uses READ_COMMITTED which is the weakest
-   * semantics Oracle supports.
-   */
-  protected Connection makeConnection() throws SQLException {
-
-    Connection connection;
-    String driverClass = getDriverClass();
-
-    try {
-      Class.forName(driverClass);
-    } catch (ClassNotFoundException cnfe) {
-      throw new RuntimeException("Could not load db driver class: " + driverClass);
-    }
-
-    String username = options.getUsername();
-    String password = options.getPassword();
-    if (null == username) {
-      connection = DriverManager.getConnection(options.getConnectString());
-    } else {
-      connection = DriverManager.getConnection(options.getConnectString(), username, password);
-    }
-
-    // We only use this for metadata queries. Loosest semantics are okay.
-    connection.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED);
-
-    // Setting session time zone
-    setSessionTimeZone(connection);
-
-    return connection;
-  }
-
-  /**
-   * Set session time zone
-   * @param conn      Connection object
-   * @throws          SQLException instance
-   */
-  private void setSessionTimeZone(Connection conn) throws SQLException {
-    // need to use reflection to call the method setSessionTimeZone on the OracleConnection class
-    // because oracle specific java libraries are not accessible in this context
-    Method method;
-    try {
-      method = conn.getClass().getMethod(
-              "setSessionTimeZone", new Class [] {String.class});
-    } catch (Exception ex) {
-      LOG.error("Could not find method setSessionTimeZone in " + conn.getClass().getName(), ex);
-      // rethrow SQLException
-      throw new SQLException(ex);
-    }
-
-    // Need to set the time zone in order for Java
-    // to correctly access the column "TIMESTAMP WITH LOCAL TIME ZONE"
-    String clientTimeZone = TimeZone.getDefault().getID();
-    try {
-      method.setAccessible(true);
-      method.invoke(conn, clientTimeZone);
-      LOG.info("Time zone has been set");
-    } catch (Exception ex) {
-      LOG.warn("Time zone " + clientTimeZone +
-               " could not be set on oracle database.");
-      LOG.info("Setting default time zone: UTC");
-      try {
-        method.invoke(conn, "UTC");
-      } catch (Exception ex2) {
-        LOG.error("Could not set time zone for oracle connection", ex2);
-        // rethrow SQLException
-        throw new SQLException(ex);
-      }
-    }
-  }
-
-  /**
-   * This importTable() implementation continues to use the older DBInputFormat
-   * because DataDrivenDBInputFormat does not currently work with Oracle.
-   */
-  public void importTable(ImportJobContext context)
-      throws IOException, ImportException {
-
-    String tableName = context.getTableName();
-    String jarFile = context.getJarFile();
-    SqoopOptions options = context.getOptions();
-    ImportJob importer = new ImportJob(options);
-    String splitCol = options.getSplitByCol();
-    if (null == splitCol) {
-      // If the user didn't specify a splitting column, try to infer one.
-      splitCol = getPrimaryKey(tableName);
-    }
-
-    if (null == splitCol) {
-      // Can't infer a primary key.
-      throw new ImportException("No primary key could be found for table " + tableName
-          + ". Please specify one with --split-by.");
-    }
-
-    importer.runImport(tableName, jarFile, splitCol, options.getConf());
-  }
-
-  /**
-   * Resolve a database-specific type to the Java type that should contain it.
-   * @param sqlType
-   * @return the name of a Java type to hold the sql datatype, or null if none.
-   */
-  public String toJavaType(int sqlType) {
-    String defaultJavaType = super.toJavaType(sqlType);
-    return (defaultJavaType == null) ? dbToJavaType(sqlType) : defaultJavaType;
-  }
-
-  /**
-   * Attempt to map sql type to java type
-   * @param sqlType     sql type
-   * @return            java type
-   */
-  private String dbToJavaType(int sqlType) {
-    // load class oracle.jdbc.OracleTypes
-    // need to use reflection because oracle specific libraries
-    // are not accessible in this context
-    Class typeClass = getTypeClass("oracle.jdbc.OracleTypes");
-
-    // check if it is TIMESTAMPTZ
-    int dbType = getDatabaseType(typeClass, "TIMESTAMPTZ");
-    if (sqlType == dbType) {
-      return "java.sql.Timestamp";
-    }
-
-    // check if it is TIMESTAMPLTZ
-    dbType = getDatabaseType(typeClass, "TIMESTAMPLTZ");
-    if (sqlType == dbType) {
-      return "java.sql.Timestamp";
-    }
-
-    // return null if no java type was found for sqlType
-    return null;
-  }
-    
-  /**
-   * Attempt to map sql type to hive type
-   * @param sqlType     sql data type
-   * @return            hive data type
-   */
-  public String toHiveType(int sqlType) {
-    String defaultHiveType = super.toHiveType(sqlType);
-    return (defaultHiveType == null) ? dbToHiveType(sqlType) : defaultHiveType;
-  }
-
-  /**
-   * Resolve a database-specific type to Hive type
-   * @param sqlType     sql type
-   * @return            hive type
-   */
-  private String dbToHiveType(int sqlType) {
-    // load class oracle.jdbc.OracleTypes
-    // need to use reflection because oracle specific libraries
-    // are not accessible in this context
-    Class typeClass = getTypeClass("oracle.jdbc.OracleTypes");
-
-    // check if it is TIMESTAMPTZ
-    int dbType = getDatabaseType(typeClass, "TIMESTAMPTZ");
-    if (sqlType == dbType) {
-      return "STRING";
-    }
-
-    // check if it is TIMESTAMPLTZ
-    dbType = getDatabaseType(typeClass, "TIMESTAMPLTZ");
-    if (sqlType == dbType) {
-      return "STRING";
-    }
-
-    // return null if no hive type was found for sqlType
-    return null;
-  }
-
-  /**
-   * Get database type
-   * @param clazz         oracle class representing sql types
-   * @param fieldName     field name
-   * @return              value of database type constant
-   */
-  private int getDatabaseType(Class clazz, String fieldName) {
-    // need to use reflection to extract constant values
-    // because the database specific java libraries are not accessible in this context
-    int value = -1;
-    try {
-      java.lang.reflect.Field field = clazz.getDeclaredField(fieldName);
-      value = field.getInt(null);
-    } catch (NoSuchFieldException ex) {
-      LOG.error("Could not retrieve value for field " + fieldName, ex);
-    } catch (IllegalAccessException ex) {
-      LOG.error("Could not retrieve value for field " + fieldName, ex);
-    }
-    return value;
-  }
-
-  /**
-   * Load class by name
-   * @param className     class name
-   * @return              class instance
-   */
-  private Class getTypeClass(String className) {
-    // need to use reflection to load class
-    // because the database specific java libraries are not accessible in this context
-    Class typeClass = null;
-    try {
-      typeClass = Class.forName(className);
-    } catch (ClassNotFoundException ex) {
-      LOG.error("Could not load class " + className, ex);
-    }
-    return typeClass;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/PostgresqlManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/PostgresqlManager.java
deleted file mode 100644
index 283d741..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/PostgresqlManager.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.IOException;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.util.ImportException;
-
-/**
- * Manages connections to Postgresql databases
- */
-public class PostgresqlManager extends GenericJdbcManager {
-
-  public static final Log LOG = LogFactory.getLog(PostgresqlManager.class.getName());
-
-  // driver class to ensure is loaded when making db connection.
-  private static final String DRIVER_CLASS = "org.postgresql.Driver";
-
-  private static final int POSTGRESQL_FETCH_SIZE = 50; // Fetch 50 rows at a time.
-
-  // set to true after we warn the user that we can use direct fastpath.
-  private static boolean warningPrinted = false;
-
-  public PostgresqlManager(final SqoopOptions opts) {
-    super(DRIVER_CLASS, opts);
-  }
-
-  protected PostgresqlManager(final SqoopOptions opts, boolean ignored) {
-    // constructor used by subclasses to avoid the --direct warning.
-    super(DRIVER_CLASS, opts);
-  }
-
-  @Override
-  public void close() throws SQLException {
-    if (this.hasOpenConnection()) {
-      this.getConnection().commit(); // Commit any changes made thus far.
-    }
-
-    super.close();
-  }
-
-  @Override
-  protected String getColNamesQuery(String tableName) {
-    // Use LIMIT to return fast
-    return "SELECT t.* FROM " + escapeTableName(tableName) + " AS t LIMIT 1";
-  }
-
-  @Override
-  public void importTable(ImportJobContext context)
-        throws IOException, ImportException {
-
-    // The user probably should have requested --direct to invoke pg_dump.
-    // Display a warning informing them of this fact.
-    if (!PostgresqlManager.warningPrinted) {
-      String connectString = context.getOptions().getConnectString();
-
-      LOG.warn("It looks like you are importing from postgresql.");
-      LOG.warn("This transfer can be faster! Use the --direct");
-      LOG.warn("option to exercise a postgresql-specific fast path.");
-
-      PostgresqlManager.warningPrinted = true; // don't display this twice.
-    }
-
-    // Then run the normal importTable() method.
-    super.importTable(context);
-  }
-
-  @Override
-  public String getPrimaryKey(String tableName) {
-    // Postgresql stores table names using lower-case internally; need
-    // to always convert to lowercase before querying the metadata dictionary.
-    return super.getPrimaryKey(tableName.toLowerCase());
-  }
-
-  /**
-   * Executes an arbitrary SQL statement. Sets the cursor fetch size
-   * to ensure the entire table is not buffered in RAM before reading
-   * any rows. A consequence of this is that every ResultSet returned
-   * by this method *MUST* be close()'d, or read to exhaustion before
-   * another query can be executed from this ConnManager instance.
-   *
-   * @param stmt The SQL statement to execute
-   * @return A ResultSet encapsulating the results or null on error
-   */
-  protected ResultSet execute(String stmt, Object... args) throws SQLException {
-    PreparedStatement statement = null;
-    statement = this.getConnection().prepareStatement(stmt,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    statement.setFetchSize(POSTGRESQL_FETCH_SIZE);
-    if (null != args) {
-      for (int i = 0; i < args.length; i++) {
-        statement.setObject(i + 1, args[i]);
-      }
-    }
-
-    LOG.info("Executing SQL statement: " + stmt);
-    return statement.executeQuery();
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/SqlManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/SqlManager.java
deleted file mode 100644
index 42846d9..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/manager/SqlManager.java
+++ /dev/null
@@ -1,449 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.hive.HiveTypes;
-import org.apache.hadoop.sqoop.mapreduce.DataDrivenImportJob;
-import org.apache.hadoop.sqoop.mapreduce.ExportJob;
-import org.apache.hadoop.sqoop.util.ExportException;
-import org.apache.hadoop.sqoop.util.ImportException;
-import org.apache.hadoop.sqoop.util.ResultSetPrinter;
-
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.DatabaseMetaData;
-import java.sql.DriverManager;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
-import java.sql.SQLException;
-import java.sql.Types;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * ConnManager implementation for generic SQL-compliant database.
- * This is an abstract class; it requires a database-specific
- * ConnManager implementation to actually create the connection.
- */
-public abstract class SqlManager extends ConnManager {
-
-  public static final Log LOG = LogFactory.getLog(SqlManager.class.getName());
-
-  protected SqoopOptions options;
-
-  /**
-   * Constructs the SqlManager
-   * @param opts
-   * @param specificMgr
-   */
-  public SqlManager(final SqoopOptions opts) {
-    this.options = opts;
-  }
-
-  /**
-   * @return the SQL query to use in getColumnNames() in case this logic must
-   * be tuned per-database, but the main extraction loop is still inheritable.
-   */
-  protected String getColNamesQuery(String tableName) {
-    // adding where clause to prevent loading a big table
-    return "SELECT t.* FROM " + escapeTableName(tableName) + " AS t WHERE 1=0";
-  }
-
-  @Override
-  public String[] getColumnNames(String tableName) {
-    String stmt = getColNamesQuery(tableName);
-
-    ResultSet results;
-    try {
-      results = execute(stmt);
-    } catch (SQLException sqlE) {
-      LOG.error("Error executing statement: " + sqlE.toString());
-      return null;
-    }
-
-    try {
-      int cols = results.getMetaData().getColumnCount();
-      ArrayList<String> columns = new ArrayList<String>();
-      ResultSetMetaData metadata = results.getMetaData();
-      for (int i = 1; i < cols + 1; i++) {
-        String colName = metadata.getColumnName(i);
-        if (colName == null || colName.equals("")) {
-          colName = metadata.getColumnLabel(i);
-        }
-        columns.add(colName);
-      }
-      return columns.toArray(new String[0]);
-    } catch (SQLException sqlException) {
-      LOG.error("Error reading from database: " + sqlException.toString());
-      return null;
-    } finally {
-      try {
-        results.close();
-        getConnection().commit();
-      } catch (SQLException sqlE) {
-        LOG.warn("SQLException closing ResultSet: " + sqlE.toString());
-      }
-    }
-  }
-
-  /**
-   * @return the SQL query to use in getColumnTypes() in case this logic must
-   * be tuned per-database, but the main extraction loop is still inheritable.
-   */
-  protected String getColTypesQuery(String tableName) {
-    return getColNamesQuery(tableName);
-  }
-  
-  @Override
-  public Map<String, Integer> getColumnTypes(String tableName) {
-    String stmt = getColTypesQuery(tableName);
-
-    ResultSet results;
-    try {
-      results = execute(stmt);
-    } catch (SQLException sqlE) {
-      LOG.error("Error executing statement: " + sqlE.toString());
-      return null;
-    }
-
-    try {
-      Map<String, Integer> colTypes = new HashMap<String, Integer>();
-
-      int cols = results.getMetaData().getColumnCount();
-      ResultSetMetaData metadata = results.getMetaData();
-      for (int i = 1; i < cols + 1; i++) {
-        int typeId = metadata.getColumnType(i);
-        String colName = metadata.getColumnName(i);
-        if (colName == null || colName.equals("")) {
-          colName = metadata.getColumnLabel(i);
-        }
-
-        colTypes.put(colName, Integer.valueOf(typeId));
-      }
-
-      return colTypes;
-    } catch (SQLException sqlException) {
-      LOG.error("Error reading from database: " + sqlException.toString());
-      return null;
-    } finally {
-      try {
-        results.close();
-        getConnection().commit();
-      } catch (SQLException sqlE) { 
-        LOG.warn("SQLException closing ResultSet: " + sqlE.toString());
-      }
-    }
-  }
-
-  @Override
-  public ResultSet readTable(String tableName, String[] columns) throws SQLException {
-    if (columns == null) {
-      columns = getColumnNames(tableName);
-    }
-
-    StringBuilder sb = new StringBuilder();
-    sb.append("SELECT ");
-    boolean first = true;
-    for (String col : columns) {
-      if (!first) {
-        sb.append(", ");
-      }
-      sb.append(escapeColName(col));
-      first = false;
-    }
-    sb.append(" FROM ");
-    sb.append(escapeTableName(tableName));
-    sb.append(" AS ");   // needed for hsqldb; doesn't hurt anyone else.
-    sb.append(escapeTableName(tableName));
-
-    return execute(sb.toString());
-  }
-
-  @Override
-  public String[] listDatabases() {
-    // TODO(aaron): Implement this!
-    LOG.error("Generic SqlManager.listDatabases() not implemented.");
-    return null;
-  }
-
-  @Override
-  public String[] listTables() {
-    ResultSet results = null;
-    String [] tableTypes = {"TABLE"};
-    try {
-      try {
-        DatabaseMetaData metaData = this.getConnection().getMetaData();
-        results = metaData.getTables(null, null, null, tableTypes);
-      } catch (SQLException sqlException) {
-        LOG.error("Error reading database metadata: " + sqlException.toString());
-        return null;
-      }
-
-      if (null == results) {
-        return null;
-      }
-
-      try {
-        ArrayList<String> tables = new ArrayList<String>();
-        while (results.next()) {
-          String tableName = results.getString("TABLE_NAME");
-          tables.add(tableName);
-        }
-
-        return tables.toArray(new String[0]);
-      } catch (SQLException sqlException) {
-        LOG.error("Error reading from database: " + sqlException.toString());
-        return null;
-      }
-    } finally {
-      if (null != results) {
-        try {
-          results.close();
-          getConnection().commit();
-        } catch (SQLException sqlE) {
-          LOG.warn("Exception closing ResultSet: " + sqlE.toString());
-        }
-      }
-    }
-  }
-
-  @Override
-  public String getPrimaryKey(String tableName) {
-    try {
-      DatabaseMetaData metaData = this.getConnection().getMetaData();
-      ResultSet results = metaData.getPrimaryKeys(null, null, tableName);
-      if (null == results) {
-        return null;
-      }
-      
-      try {
-        if (results.next()) {
-          return results.getString("COLUMN_NAME");
-        } else {
-          return null;
-        }
-      } finally {
-        results.close();
-        getConnection().commit();
-      }
-    } catch (SQLException sqlException) {
-      LOG.error("Error reading primary key metadata: " + sqlException.toString());
-      return null;
-    }
-  }
-
-  /**
-   * Retrieve the actual connection from the outer ConnManager
-   */
-  public abstract Connection getConnection() throws SQLException;
-
-  /**
-   * Default implementation of importTable() is to launch a MapReduce job
-   * via DataDrivenImportJob to read the table with DataDrivenDBInputFormat.
-   */
-  public void importTable(ImportJobContext context)
-      throws IOException, ImportException {
-    String tableName = context.getTableName();
-    String jarFile = context.getJarFile();
-    SqoopOptions options = context.getOptions();
-    DataDrivenImportJob importer = new DataDrivenImportJob(options);
-    String splitCol = options.getSplitByCol();
-    if (null == splitCol) {
-      // If the user didn't specify a splitting column, try to infer one.
-      splitCol = getPrimaryKey(tableName);
-    }
-
-    if (null == splitCol) {
-      // Can't infer a primary key.
-      throw new ImportException("No primary key could be found for table " + tableName
-          + ". Please specify one with --split-by.");
-    }
-
-    importer.runImport(tableName, jarFile, splitCol, options.getConf());
-  }
-
-  /**
-   * executes an arbitrary SQL statement
-   * @param stmt The SQL statement to execute
-   * @return A ResultSet encapsulating the results or null on error
-   */
-  protected ResultSet execute(String stmt, Object... args) throws SQLException {
-    PreparedStatement statement = null;
-    statement = this.getConnection().prepareStatement(stmt,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    if (null != args) {
-      for (int i = 0; i < args.length; i++) {
-        statement.setObject(i + 1, args[i]);
-      }
-    }
-
-    LOG.info("Executing SQL statement: " + stmt);
-    return statement.executeQuery();
-  }
-
-  /**
-   * Resolve a database-specific type to the Java type that should contain it.
-   * @param sqlType
-   * @return the name of a Java type to hold the sql datatype, or null if none.
-   */
-  public String toJavaType(int sqlType) {
-    // mappings from http://java.sun.com/j2se/1.3/docs/guide/jdbc/getstart/mapping.html
-    if (sqlType == Types.INTEGER) {
-      return "Integer";
-    } else if (sqlType == Types.VARCHAR) {
-      return "String";
-    } else if (sqlType == Types.CHAR) {
-      return "String";
-    } else if (sqlType == Types.LONGVARCHAR) {
-      return "String";
-    } else if (sqlType == Types.NUMERIC) {
-      return "java.math.BigDecimal";
-    } else if (sqlType == Types.DECIMAL) {
-      return "java.math.BigDecimal";
-    } else if (sqlType == Types.BIT) {
-      return "Boolean";
-    } else if (sqlType == Types.BOOLEAN) {
-      return "Boolean";
-    } else if (sqlType == Types.TINYINT) {
-      return "Integer";
-    } else if (sqlType == Types.SMALLINT) {
-      return "Integer";
-    } else if (sqlType == Types.BIGINT) {
-      return "Long";
-    } else if (sqlType == Types.REAL) {
-      return "Float";
-    } else if (sqlType == Types.FLOAT) {
-      return "Double";
-    } else if (sqlType == Types.DOUBLE) {
-      return "Double";
-    } else if (sqlType == Types.DATE) {
-      return "java.sql.Date";
-    } else if (sqlType == Types.TIME) {
-      return "java.sql.Time";
-    } else if (sqlType == Types.TIMESTAMP) {
-      return "java.sql.Timestamp";
-    } else {
-      // TODO(aaron): Support BINARY, VARBINARY, LONGVARBINARY, DISTINCT, CLOB, BLOB, ARRAY,
-      // STRUCT, REF, JAVA_OBJECT.
-      // return database specific java data type
-      return null;
-    }
-  }
-
-  /**
-   * Resolve a database-specific type to Hive data type
-   * @param sqlType     sql type
-   * @return            hive type
-   */
-  public String toHiveType(int sqlType) {
-    return HiveTypes.toHiveType(sqlType);
-  }
-
-  public void close() throws SQLException {
-  }
-
-  /**
-   * Poor man's SQL query interface; used for debugging.
-   * @param s
-   */
-  public void execAndPrint(String s) {
-    System.out.println("Executing statement: " + s);
-    ResultSet results;
-    try {
-      results = execute(s);
-    } catch (SQLException sqlE) {
-      LOG.error("Error executing statement: " + sqlE.toString());
-      return;
-    }
-
-    try {
-      try {
-        int cols = results.getMetaData().getColumnCount();
-        System.out.println("Got " + cols + " columns back");
-        if (cols > 0) {
-          System.out.println("Schema: " + results.getMetaData().getSchemaName(1));
-          System.out.println("Table: " + results.getMetaData().getTableName(1));
-        }
-      } catch (SQLException sqlE) {
-        LOG.error("SQLException reading result metadata: " + sqlE.toString());
-      }
-
-      try {
-        new ResultSetPrinter().printResultSet(System.out, results);
-      } catch (IOException ioe) {
-        LOG.error("IOException writing results to stdout: " + ioe.toString());
-        return;
-      }
-    } finally {
-      try {
-        results.close();
-        getConnection().commit();
-      } catch (SQLException sqlE) {
-        LOG.warn("SQLException closing ResultSet: " + sqlE.toString());
-      }
-    }
-  }
-
-  /**
-   * Create a connection to the database; usually used only from within
-   * getConnection(), which enforces a singleton guarantee around the
-   * Connection object.
-   */
-  protected Connection makeConnection() throws SQLException {
-
-    Connection connection;
-    String driverClass = getDriverClass();
-
-    try {
-      Class.forName(driverClass);
-    } catch (ClassNotFoundException cnfe) {
-      throw new RuntimeException("Could not load db driver class: " + driverClass);
-    }
-
-    String username = options.getUsername();
-    String password = options.getPassword();
-    if (null == username) {
-      connection = DriverManager.getConnection(options.getConnectString());
-    } else {
-      connection = DriverManager.getConnection(options.getConnectString(), username, password);
-    }
-
-    // We only use this for metadata queries. Loosest semantics are okay.
-    connection.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED);
-    connection.setAutoCommit(false);
-
-    return connection;
-  }
-
-  /**
-   * Export data stored in HDFS into a table in a database
-   */
-  public void exportTable(ExportJobContext context)
-      throws IOException, ExportException {
-    ExportJob exportJob = new ExportJob(context);
-    exportJob.runExport();
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/AutoProgressMapRunner.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/AutoProgressMapRunner.java
deleted file mode 100644
index e169170..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/AutoProgressMapRunner.java
+++ /dev/null
@@ -1,200 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.MapRunnable;
-import org.apache.hadoop.mapred.MapRunner;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-
-/**
- * MapRunnable implementation that spawns a background thread
- * to periodically increment the progress of the operation.
- * This is used because queries to the database can be expected
- * to block for more than 10 minutes when performing the initial
- * record input.
- *
- * The background thread can be configured to stop providing
- * progress after a fixed period of time; after this time period,
- * some other means (e.g., emitting records) must be used to sustain
- * Mapper progress.
- */
-public class AutoProgressMapRunner<K1, V1, K2, V2>
-    extends MapRunner<K1, V1, K2, V2> implements MapRunnable<K1, V1, K2, V2> {
-
-  public static final Log LOG = LogFactory.getLog(AutoProgressMapRunner.class.getName());
-
-  /** Total number of millis for which progress will be reported
-      by the auto-progress thread. If this is zero, then the auto-progress
-      thread will never voluntarily exit.
-    */
-  private int maxProgressPeriod;
-
-  /** Number of milliseconds to sleep for between loop iterations. Must be less
-      than report interval.
-    */
-  private int sleepInterval;
-
-  /** Number of milliseconds between calls to Reporter.progress(). Should be a multiple
-      of the sleepInterval.
-    */
-  private int reportInterval;
-
-  public static final String MAX_PROGRESS_PERIOD_KEY = "sqoop.mapred.auto.progress.max";
-  public static final String SLEEP_INTERVAL_KEY = "sqoop.mapred.auto.progress.sleep";
-  public static final String REPORT_INTERVAL_KEY = "sqoop.mapred.auto.progress.report";
-
-  // Sleep for 10 seconds at a time.
-  static final int DEFAULT_SLEEP_INTERVAL = 10000;
-
-  // Report progress every 30 seconds.
-  static final int DEFAULT_REPORT_INTERVAL = 30000;
-
-  // Disable max progress, by default.
-  static final int DEFAULT_MAX_PROGRESS = 0;
-
-  private class ProgressThread extends Thread {
-
-    private boolean keepGoing; // while this is true, thread runs.
-    private Reporter reporter;
-    private long startTimeMillis;
-    private long lastReportMillis;
-
-    public ProgressThread(final Reporter r) {
-      this.reporter = r;
-      this.keepGoing = true;
-    }
-
-    public void signalShutdown() {
-      synchronized(this) {
-        // Synchronize this to ensure a fence before interrupt.
-        this.keepGoing = false;
-      }
-      this.interrupt();
-    }
-
-    public void run() {
-      boolean doKeepGoing = true;
-
-      this.lastReportMillis = System.currentTimeMillis();
-      this.startTimeMillis = this.lastReportMillis;
-
-      final long MAX_PROGRESS = AutoProgressMapRunner.this.maxProgressPeriod;
-      final long REPORT_INTERVAL = AutoProgressMapRunner.this.reportInterval;
-      final long SLEEP_INTERVAL = AutoProgressMapRunner.this.sleepInterval;
-
-      // in a loop:
-      //   * Check that we haven't run for too long (maxProgressPeriod)
-      //   * If it's been a report interval since we last made progress, make more.
-      //   * Sleep for a bit.
-      //   * If the parent thread has signaled for exit, do so.
-      while (doKeepGoing) {
-        long curTimeMillis = System.currentTimeMillis();
-
-        if (MAX_PROGRESS != 0 && curTimeMillis - this.startTimeMillis > MAX_PROGRESS) {
-          synchronized(this) {
-            this.keepGoing = false;
-          }
-          LOG.info("Auto-progress thread exiting after " + MAX_PROGRESS + " ms.");
-          break;
-        }
-
-        if (curTimeMillis - this.lastReportMillis > REPORT_INTERVAL) {
-          // It's been a full report interval -- claim progress.
-          LOG.debug("Auto-progress thread reporting progress");
-          this.reporter.progress();
-          this.lastReportMillis = curTimeMillis;
-        }
-
-        // Unless we got an interrupt while we were working,
-        // sleep a bit before doing more work.
-        if (!this.interrupted()) {
-          try {
-            Thread.sleep(SLEEP_INTERVAL);
-          } catch (InterruptedException ie) {
-            // we were notified on something; not necessarily an error.
-          }
-        }
-
-        synchronized(this) {
-          // Read shared field in a synchronized block.
-          doKeepGoing = this.keepGoing;
-        }
-      }
-
-      LOG.info("Auto-progress thread is finished. keepGoing=" + doKeepGoing);
-    }
-  }
-
-  public void configure(JobConf job) {
-    this.maxProgressPeriod = job.getInt(MAX_PROGRESS_PERIOD_KEY, DEFAULT_MAX_PROGRESS);
-    this.sleepInterval = job.getInt(SLEEP_INTERVAL_KEY, DEFAULT_SLEEP_INTERVAL);
-    this.reportInterval = job.getInt(REPORT_INTERVAL_KEY, DEFAULT_REPORT_INTERVAL);
-
-    if (this.reportInterval < 1) {
-      LOG.warn("Invalid " + REPORT_INTERVAL_KEY + "; setting to " + DEFAULT_REPORT_INTERVAL);
-      this.reportInterval = DEFAULT_REPORT_INTERVAL;
-    }
-
-    if (this.sleepInterval > this.reportInterval || this.sleepInterval < 1) {
-      LOG.warn("Invalid " + SLEEP_INTERVAL_KEY + "; setting to " + DEFAULT_SLEEP_INTERVAL);
-      this.sleepInterval = DEFAULT_SLEEP_INTERVAL;
-    }
-
-    if (this.maxProgressPeriod < 0) {
-      LOG.warn("Invalid " + MAX_PROGRESS_PERIOD_KEY + "; setting to " + DEFAULT_MAX_PROGRESS);
-      this.maxProgressPeriod = DEFAULT_MAX_PROGRESS;
-    }
-
-    super.configure(job);
-  }
-
-  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output, Reporter reporter)
-      throws IOException {
-
-    ProgressThread thread = this.new ProgressThread(reporter);
-
-    try {
-      thread.setDaemon(true);
-      thread.start();
-      // Use default MapRunner to actually drive the mapping.
-      super.run(input, output, reporter);
-    } finally {
-      // Tell the progress thread to exit..
-      LOG.debug("Instructing auto-progress thread to quit.");
-      thread.signalShutdown();
-      try {
-        // And wait for that to happen.
-        LOG.debug("Waiting for progress thread shutdown...");
-        thread.join();
-        LOG.debug("Progress thread shutdown detected.");
-      } catch (InterruptedException ie) {
-        LOG.warn("Interrupted when waiting on auto-progress thread: " + ie.toString());
-      }
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/ImportJob.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/ImportJob.java
deleted file mode 100644
index d889415..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/ImportJob.java
+++ /dev/null
@@ -1,169 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.SequenceFileOutputFormat;
-import org.apache.hadoop.mapred.lib.db.DBConfiguration;
-import org.apache.hadoop.mapred.lib.db.DBInputFormat;
-import org.apache.hadoop.mapred.lib.db.DBWritable;
-
-import org.apache.hadoop.sqoop.ConnFactory;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.orm.TableClassName;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-import org.apache.hadoop.sqoop.util.PerfCounters;
-
-/**
- * Actually runs a jdbc import job using the ORM files generated by the sqoop.orm package.
- */
-public class ImportJob {
-
-  public static final Log LOG = LogFactory.getLog(ImportJob.class.getName());
-
-  private SqoopOptions options;
-
-  public ImportJob(final SqoopOptions opts) {
-    this.options = opts;
-  }
-
-  /**
-   * Run an import job to read a table in to HDFS
-   *
-   * @param tableName  the database table to read
-   * @param ormJarFile the Jar file to insert into the dcache classpath. (may be null)
-   * @param orderByCol the column of the database table to use to order the import
-   * @param conf A fresh Hadoop Configuration to use to build an MR job.
-   */
-  public void runImport(String tableName, String ormJarFile, String orderByCol,
-      Configuration conf) throws IOException {
-
-    LOG.info("Beginning data import of " + tableName);
-
-    String tableClassName = new TableClassName(options).getClassForTable(tableName);
-
-    boolean isLocal = "local".equals(conf.get("mapred.job.tracker"));
-    ClassLoader prevClassLoader = null;
-    if (isLocal) {
-      // If we're using the LocalJobRunner, then instead of using the compiled jar file
-      // as the job source, we're running in the current thread. Push on another classloader
-      // that loads from that jar in addition to everything currently on the classpath.
-      prevClassLoader = ClassLoaderStack.addJarFile(ormJarFile, tableClassName);
-    }
-
-    try {
-      JobConf job = new JobConf(conf);
-      job.setJar(ormJarFile);
-
-      String hdfsWarehouseDir = options.getWarehouseDir();
-      Path outputPath;
-
-      if (null != hdfsWarehouseDir) {
-        Path hdfsWarehousePath = new Path(hdfsWarehouseDir);
-        hdfsWarehousePath.makeQualified(FileSystem.get(job));
-        outputPath = new Path(hdfsWarehousePath, tableName);
-      } else {
-        outputPath = new Path(tableName);
-      }
-
-      if (options.getFileLayout() == SqoopOptions.FileLayout.TextFile) {
-        job.setOutputFormat(RawKeyTextOutputFormat.class);
-        job.setMapperClass(TextImportMapper.class);
-        job.setOutputKeyClass(Text.class);
-        job.setOutputValueClass(NullWritable.class);
-        if (options.shouldUseCompression()) {
-          FileOutputFormat.setCompressOutput(job, true);
-          FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
-        }
-      } else if (options.getFileLayout() == SqoopOptions.FileLayout.SequenceFile) {
-        job.setOutputFormat(SequenceFileOutputFormat.class);
-        if (options.shouldUseCompression()) {
-          SequenceFileOutputFormat.setCompressOutput(job, true);
-          SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
-        }
-        job.set("mapred.output.value.class", tableClassName);
-      } else {
-        LOG.warn("Unknown file layout specified: " + options.getFileLayout() + "; using text.");
-      }
-
-      job.setNumReduceTasks(0);
-      job.setNumMapTasks(1);
-      job.setInputFormat(DBInputFormat.class);
-      job.setMapRunnerClass(AutoProgressMapRunner.class);
-
-      FileOutputFormat.setOutputPath(job, outputPath);
-
-      ConnManager mgr = new ConnFactory(conf).getManager(options);
-      String username = options.getUsername();
-      if (null == username || username.length() == 0) {
-        DBConfiguration.configureDB(job, mgr.getDriverClass(), options.getConnectString());
-      } else {
-        DBConfiguration.configureDB(job, mgr.getDriverClass(), options.getConnectString(),
-            username, options.getPassword());
-      }
-
-      String [] colNames = options.getColumns();
-      if (null == colNames) {
-        colNames = mgr.getColumnNames(tableName);
-      }
-
-      // It's ok if the where clause is null in DBInputFormat.setInput.
-      String whereClause = options.getWhereClause();
-
-      // We can't set the class properly in here, because we may not have the
-      // jar loaded in this JVM. So we start by calling setInput() with DBWritable,
-      // and then overriding the string manually.
-      DBInputFormat.setInput(job, DBWritable.class, tableName, whereClause,
-          orderByCol, colNames);
-      job.set(DBConfiguration.INPUT_CLASS_PROPERTY, tableClassName);
-
-      PerfCounters counters = new PerfCounters();
-      counters.startClock();
-
-      RunningJob runningJob = JobClient.runJob(job);
-
-      counters.stopClock();
-      // TODO(aaron): Is this the correct way to determine how much data got written?
-      counters.addBytes(runningJob.getCounters().getGroup("FileSystemCounters")
-          .getCounterForName("FILE_BYTES_WRITTEN").getCounter());
-      LOG.info("Transferred " + counters.toString());
-    } finally {
-      if (isLocal && null != prevClassLoader) {
-        // unload the special classloader for this jar.
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/RawKeyTextOutputFormat.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/RawKeyTextOutputFormat.java
deleted file mode 100644
index 16c69b8..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/RawKeyTextOutputFormat.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapred.Reporter;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.util.*;
-
-/** An {@link OutputFormat} that writes plain text files.
- * Only writes the key. Does not write any delimiter/newline after the key.
- */
-public class RawKeyTextOutputFormat<K, V> extends FileOutputFormat<K, V> {
-
-  protected static class RawKeyRecordWriter<K, V>
-    implements RecordWriter<K, V> {
-    private static final String utf8 = "UTF-8";
-
-    protected DataOutputStream out;
-
-    public RawKeyRecordWriter(DataOutputStream out) {
-      this.out = out;
-    }
-
-    /**
-     * Write the object to the byte stream, handling Text as a special
-     * case.
-     * @param o the object to print
-     * @throws IOException if the write throws, we pass it on
-     */
-    private void writeObject(Object o) throws IOException {
-      if (o instanceof Text) {
-        Text to = (Text) o;
-        out.write(to.getBytes(), 0, to.getLength());
-      } else {
-        out.write(o.toString().getBytes(utf8));
-      }
-    }
-
-    public synchronized void write(K key, V value)
-      throws IOException {
-
-      writeObject(key);
-    }
-
-    public synchronized void close(Reporter reporter) throws IOException {
-      out.close();
-    }
-  }
-
-  public RecordWriter<K, V> getRecordWriter(FileSystem ignored,
-                                                  JobConf job,
-                                                  String name,
-                                                  Progressable progress)
-    throws IOException {
-    boolean isCompressed = getCompressOutput(job);
-    if (!isCompressed) {
-      Path file = FileOutputFormat.getTaskOutputPath(job, name);
-      FileSystem fs = file.getFileSystem(job);
-      FSDataOutputStream fileOut = fs.create(file, progress);
-      return new RawKeyRecordWriter<K, V>(fileOut);
-    } else {
-      Class<? extends CompressionCodec> codecClass =
-        getOutputCompressorClass(job, GzipCodec.class);
-      // create the named codec
-      CompressionCodec codec = ReflectionUtils.newInstance(codecClass, job);
-      // build the filename including the extension
-      Path file = 
-        FileOutputFormat.getTaskOutputPath(job, 
-                                           name + codec.getDefaultExtension());
-      FileSystem fs = file.getFileSystem(job);
-      FSDataOutputStream fileOut = fs.create(file, progress);
-      return new RawKeyRecordWriter<K, V>(new DataOutputStream
-                                        (codec.createOutputStream(fileOut)));
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/TextImportMapper.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/TextImportMapper.java
deleted file mode 100644
index 654a7d6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapred/TextImportMapper.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.MapReduceBase;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.lib.db.DBWritable;
-
-/**
- * Converts an input record into a string representation and emit it.
- * 
- *
- */
-public class TextImportMapper extends MapReduceBase
-    implements Mapper<LongWritable, DBWritable, Text, NullWritable> {
-
-  private Text outkey;
-
-  public TextImportMapper() {
-    outkey = new Text();
-  }
-
-  public void map(LongWritable key, DBWritable val, OutputCollector<Text, NullWritable> output,
-      Reporter reporter) throws IOException {
-
-    outkey.set(val.toString());
-    output.collect(outkey, NullWritable.get());
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/AutoProgressMapper.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/AutoProgressMapper.java
deleted file mode 100644
index 8a889a8..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/AutoProgressMapper.java
+++ /dev/null
@@ -1,186 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-
-/**
- * Identity mapper that continuously reports progress via a background thread.
- */
-public class AutoProgressMapper<KEYIN, VALIN, KEYOUT, VALOUT>
-    extends Mapper<KEYIN, VALIN, KEYOUT, VALOUT> {
-
-  public static final Log LOG = LogFactory.getLog(AutoProgressMapper.class.getName());
-
-  /** Total number of millis for which progress will be reported
-      by the auto-progress thread. If this is zero, then the auto-progress
-      thread will never voluntarily exit.
-    */
-  private int maxProgressPeriod;
-
-  /** Number of milliseconds to sleep for between loop iterations. Must be less
-      than report interval.
-    */
-  private int sleepInterval;
-
-  /** Number of milliseconds between calls to Reporter.progress(). Should be a multiple
-      of the sleepInterval.
-    */
-  private int reportInterval;
-
-  public static final String MAX_PROGRESS_PERIOD_KEY = "sqoop.mapred.auto.progress.max";
-  public static final String SLEEP_INTERVAL_KEY = "sqoop.mapred.auto.progress.sleep";
-  public static final String REPORT_INTERVAL_KEY = "sqoop.mapred.auto.progress.report";
-
-  // Sleep for 10 seconds at a time.
-  static final int DEFAULT_SLEEP_INTERVAL = 10000;
-
-  // Report progress every 30 seconds.
-  static final int DEFAULT_REPORT_INTERVAL = 30000;
-
-  // Disable max progress, by default.
-  static final int DEFAULT_MAX_PROGRESS = 0;
-
-  private class ProgressThread extends Thread {
-
-    private volatile boolean keepGoing; // while this is true, thread runs.
-
-    private Context context;
-    private long startTimeMillis;
-    private long lastReportMillis;
-
-    public ProgressThread(final Context ctxt) {
-      this.context = ctxt;
-      this.keepGoing = true;
-    }
-
-    public void signalShutdown() {
-      this.keepGoing = false; // volatile update.
-      this.interrupt();
-    }
-
-    public void run() {
-      this.lastReportMillis = System.currentTimeMillis();
-      this.startTimeMillis = this.lastReportMillis;
-
-      final long MAX_PROGRESS = AutoProgressMapper.this.maxProgressPeriod;
-      final long REPORT_INTERVAL = AutoProgressMapper.this.reportInterval;
-      final long SLEEP_INTERVAL = AutoProgressMapper.this.sleepInterval;
-
-      // in a loop:
-      //   * Check that we haven't run for too long (maxProgressPeriod)
-      //   * If it's been a report interval since we last made progress, make more.
-      //   * Sleep for a bit.
-      //   * If the parent thread has signaled for exit, do so.
-      while (this.keepGoing) {
-        long curTimeMillis = System.currentTimeMillis();
-
-        if (MAX_PROGRESS != 0 && curTimeMillis - this.startTimeMillis > MAX_PROGRESS) {
-          this.keepGoing = false;
-          LOG.info("Auto-progress thread exiting after " + MAX_PROGRESS + " ms.");
-          break;
-        }
-
-        if (curTimeMillis - this.lastReportMillis > REPORT_INTERVAL) {
-          // It's been a full report interval -- claim progress.
-          LOG.debug("Auto-progress thread reporting progress");
-          this.context.progress();
-          this.lastReportMillis = curTimeMillis;
-        }
-
-        // Unless we got an interrupt while we were working,
-        // sleep a bit before doing more work.
-        if (!this.interrupted()) {
-          try {
-            Thread.sleep(SLEEP_INTERVAL);
-          } catch (InterruptedException ie) {
-            // we were notified on something; not necessarily an error.
-          }
-        }
-      }
-
-      LOG.info("Auto-progress thread is finished. keepGoing=" + this.keepGoing);
-    }
-  }
-
-  /**
-   * Set configuration parameters for the auto-progress thread.
-   */
-  private final void configureAutoProgress(Configuration job) {
-    this.maxProgressPeriod = job.getInt(MAX_PROGRESS_PERIOD_KEY, DEFAULT_MAX_PROGRESS);
-    this.sleepInterval = job.getInt(SLEEP_INTERVAL_KEY, DEFAULT_SLEEP_INTERVAL);
-    this.reportInterval = job.getInt(REPORT_INTERVAL_KEY, DEFAULT_REPORT_INTERVAL);
-
-    if (this.reportInterval < 1) {
-      LOG.warn("Invalid " + REPORT_INTERVAL_KEY + "; setting to " + DEFAULT_REPORT_INTERVAL);
-      this.reportInterval = DEFAULT_REPORT_INTERVAL;
-    }
-
-    if (this.sleepInterval > this.reportInterval || this.sleepInterval < 1) {
-      LOG.warn("Invalid " + SLEEP_INTERVAL_KEY + "; setting to " + DEFAULT_SLEEP_INTERVAL);
-      this.sleepInterval = DEFAULT_SLEEP_INTERVAL;
-    }
-
-    if (this.maxProgressPeriod < 0) {
-      LOG.warn("Invalid " + MAX_PROGRESS_PERIOD_KEY + "; setting to " + DEFAULT_MAX_PROGRESS);
-      this.maxProgressPeriod = DEFAULT_MAX_PROGRESS;
-    }
-  }
-
-
-  // map() method intentionally omitted; Mapper.map() is the identity mapper.
-
-
-  /**
-   * Run the mapping process for this task, wrapped in an auto-progress system.
-   */
-  public void run(Context context) throws IOException, InterruptedException {
-    configureAutoProgress(context.getConfiguration());
-    ProgressThread thread = this.new ProgressThread(context);
-
-    try {
-      thread.setDaemon(true);
-      thread.start();
-
-      // use default run() method to actually drive the mapping.
-      super.run(context);
-    } finally {
-      // Tell the progress thread to exit..
-      LOG.debug("Instructing auto-progress thread to quit.");
-      thread.signalShutdown();
-      try {
-        // And wait for that to happen.
-        LOG.debug("Waiting for progress thread shutdown...");
-        thread.join();
-        LOG.debug("Progress thread shutdown detected.");
-      } catch (InterruptedException ie) {
-        LOG.warn("Interrupted when waiting on auto-progress thread: " + ie.toString());
-      }
-    }
-  }
-
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/DataDrivenImportJob.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/DataDrivenImportJob.java
deleted file mode 100644
index d6e5a2f..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/DataDrivenImportJob.java
+++ /dev/null
@@ -1,219 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapreduce.Counters;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.db.DBConfiguration;
-import org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat;
-import org.apache.hadoop.mapreduce.lib.db.DBWritable;
-
-import org.apache.hadoop.sqoop.ConnFactory;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.orm.TableClassName;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-import org.apache.hadoop.sqoop.util.ImportException;
-import org.apache.hadoop.sqoop.util.PerfCounters;
-
-/**
- * Actually runs a jdbc import job using the ORM files generated by the sqoop.orm package.
- * Uses DataDrivenDBInputFormat
- */
-public class DataDrivenImportJob {
-
-  public static final Log LOG = LogFactory.getLog(DataDrivenImportJob.class.getName());
-
-  private final SqoopOptions options;
-  private final Class<Mapper> mapperClass;
-
-  // For dependency-injection purposes, we can specify a mapper class
-  // to use during tests.
-  public final static String DATA_DRIVEN_MAPPER_KEY =
-      "sqoop.data.driven.mapper.class";
-
-  @SuppressWarnings("unchecked")
-  public DataDrivenImportJob(final SqoopOptions opts) {
-    this.options = opts;
-    this.mapperClass = (Class<Mapper>) opts.getConf().getClass(
-        DATA_DRIVEN_MAPPER_KEY, null);
-  }
-
-
-  /**
-   * Run an import job to read a table in to HDFS
-   *
-   * @param tableName  the database table to read
-   * @param ormJarFile the Jar file to insert into the dcache classpath. (may be null)
-   * @param splitByCol the column of the database table to use to split the import
-   * @param conf A fresh Hadoop Configuration to use to build an MR job.
-   * @throws IOException if the job encountered an IO problem
-   * @throws ImportException if the job failed unexpectedly or was misconfigured.
-   */
-  public void runImport(String tableName, String ormJarFile, String splitByCol,
-      Configuration conf) throws IOException, ImportException {
-
-    LOG.info("Beginning data-driven import of " + tableName);
-
-    String tableClassName = new TableClassName(options).getClassForTable(tableName);
-
-    boolean isLocal = "local".equals(conf.get("mapreduce.jobtracker.address"))
-        || "local".equals(conf.get("mapred.job.tracker"));
-    ClassLoader prevClassLoader = null;
-    if (isLocal) {
-      // If we're using the LocalJobRunner, then instead of using the compiled jar file
-      // as the job source, we're running in the current thread. Push on another classloader
-      // that loads from that jar in addition to everything currently on the classpath.
-      prevClassLoader = ClassLoaderStack.addJarFile(ormJarFile, tableClassName);
-    }
-
-    try {
-      Job job = new Job(conf);
-
-      // Set the external jar to use for the job.
-      job.getConfiguration().set("mapred.jar", ormJarFile);
-
-      String hdfsWarehouseDir = options.getWarehouseDir();
-      Path outputPath;
-
-      if (null != hdfsWarehouseDir) {
-        Path hdfsWarehousePath = new Path(hdfsWarehouseDir);
-        hdfsWarehousePath.makeQualified(FileSystem.get(job.getConfiguration()));
-        outputPath = new Path(hdfsWarehousePath, tableName);
-      } else {
-        outputPath = new Path(tableName);
-      }
-
-      if (options.getFileLayout() == SqoopOptions.FileLayout.TextFile) {
-        job.setOutputFormatClass(RawKeyTextOutputFormat.class);
-        if (null == mapperClass) {
-          job.setMapperClass(TextImportMapper.class);
-        } else {
-          job.setMapperClass(mapperClass);
-        }
-        job.setOutputKeyClass(Text.class);
-        job.setOutputValueClass(NullWritable.class);
-        if (options.shouldUseCompression()) {
-          FileOutputFormat.setCompressOutput(job, true);
-          FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
-        }
-      } else if (options.getFileLayout() == SqoopOptions.FileLayout.SequenceFile) {
-        job.setOutputFormatClass(SequenceFileOutputFormat.class);
-        if (null == mapperClass) {
-          job.setMapperClass(AutoProgressMapper.class);
-        } else {
-          job.setMapperClass(mapperClass);
-        }
-        if (options.shouldUseCompression()) {
-          SequenceFileOutputFormat.setCompressOutput(job, true);
-          SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
-        }
-        job.getConfiguration().set("mapred.output.value.class", tableClassName);
-      } else {
-        LOG.warn("Unknown file layout specified: " + options.getFileLayout() + "; using text.");
-      }
-
-      int numMapTasks = options.getNumMappers();
-      if (numMapTasks < 1) {
-        numMapTasks = SqoopOptions.DEFAULT_NUM_MAPPERS;
-        LOG.warn("Invalid mapper count; using " + numMapTasks + " mappers.");
-      }
-      job.getConfiguration().setInt("mapred.map.tasks", numMapTasks);
-      job.setNumReduceTasks(0);
-
-      job.setInputFormatClass(DataDrivenDBInputFormat.class);
-
-      FileOutputFormat.setOutputPath(job, outputPath);
-
-      ConnManager mgr = new ConnFactory(conf).getManager(options);
-      String username = options.getUsername();
-      if (null == username || username.length() == 0) {
-        DBConfiguration.configureDB(job.getConfiguration(), mgr.getDriverClass(),
-            options.getConnectString());
-      } else {
-        DBConfiguration.configureDB(job.getConfiguration(), mgr.getDriverClass(),
-            options.getConnectString(), username, options.getPassword());
-      }
-
-      String [] colNames = options.getColumns();
-      if (null == colNames) {
-        colNames = mgr.getColumnNames(tableName);
-      }
-
-      String [] sqlColNames = null;
-      if (null != colNames) {
-        sqlColNames = new String[colNames.length];
-        for (int i = 0; i < colNames.length; i++) {
-          sqlColNames[i] = mgr.escapeColName(colNames[i]);
-        }
-      }
-
-      // It's ok if the where clause is null in DBInputFormat.setInput.
-      String whereClause = options.getWhereClause();
-
-      // We can't set the class properly in here, because we may not have the
-      // jar loaded in this JVM. So we start by calling setInput() with DBWritable,
-      // and then overriding the string manually.
-      DataDrivenDBInputFormat.setInput(job, DBWritable.class,
-          mgr.escapeTableName(tableName), whereClause,
-          mgr.escapeColName(splitByCol), sqlColNames);
-      job.getConfiguration().set(DBConfiguration.INPUT_CLASS_PROPERTY, tableClassName);
-
-      PerfCounters counters = new PerfCounters();
-      counters.startClock();
-
-      boolean success;
-      try {
-        success = job.waitForCompletion(false);
-      } catch (InterruptedException ie) {
-        throw new IOException(ie);
-      } catch (ClassNotFoundException cnfe) {
-        throw new IOException(cnfe);
-      }
-
-      counters.stopClock();
-      counters.addBytes(job.getCounters().getGroup("FileSystemCounters")
-          .findCounter("HDFS_BYTES_WRITTEN").getValue());
-      LOG.info("Transferred " + counters.toString());
-      if (!success) {
-        throw new ImportException("import job failed!");
-      }
-    } finally {
-      if (isLocal && null != prevClassLoader) {
-        // unload the special classloader for this jar.
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/ExportJob.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/ExportJob.java
deleted file mode 100644
index 84eb8f7..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/ExportJob.java
+++ /dev/null
@@ -1,218 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.db.DBConfiguration;
-import org.apache.hadoop.mapreduce.lib.db.DBOutputFormat;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-
-import org.apache.hadoop.sqoop.ConnFactory;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.ExportJobContext;
-import org.apache.hadoop.sqoop.orm.TableClassName;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-import org.apache.hadoop.sqoop.util.ExportException;
-
-/**
- * Actually runs a jdbc export job using the ORM files generated by the sqoop.orm package.
- * Uses DBOutputFormat
- */
-public class ExportJob {
-
-  public static final Log LOG = LogFactory.getLog(ExportJob.class.getName());
-
-  public static final String SQOOP_EXPORT_TABLE_CLASS_KEY = "sqoop.export.table.class";
-
-  private ExportJobContext context;
-
-  public ExportJob(final ExportJobContext ctxt) {
-    this.context = ctxt;
-  }
-
-  /**
-   * @return true if p is a SequenceFile, or a directory containing
-   * SequenceFiles.
-   */
-  private boolean isSequenceFiles(Path p) throws IOException {
-    Configuration conf = context.getOptions().getConf();
-    FileSystem fs = p.getFileSystem(conf);
-
-    try {
-      FileStatus stat = fs.getFileStatus(p);
-
-      if (null == stat) {
-        // Couldn't get the item.
-        LOG.warn("Input path " + p + " does not exist");
-        return false;
-      }
-
-      if (stat.isDir()) {
-        FileStatus [] subitems = fs.listStatus(p);
-        if (subitems == null || subitems.length == 0) {
-          LOG.warn("Input path " + p + " contains no files");
-          return false; // empty dir.
-        }
-
-        // Pick a random child entry to examine instead.
-        stat = subitems[0];
-      }
-
-      if (null == stat) {
-        LOG.warn("null FileStatus object in isSequenceFiles(); assuming false.");
-        return false;
-      }
-
-      Path target = stat.getPath();
-      // Test target's header to see if it contains magic numbers indicating it's
-      // a SequenceFile.
-      byte [] header = new byte[3];
-      FSDataInputStream is = null;
-      try {
-        is = fs.open(target);
-        is.readFully(header);
-      } catch (IOException ioe) {
-        // Error reading header or EOF; assume not a SequenceFile.
-        LOG.warn("IOException checking SequenceFile header: " + ioe);
-        return false;
-      } finally {
-        try {
-          if (null != is) {
-            is.close();
-          }
-        } catch (IOException ioe) {
-          // ignore; closing.
-          LOG.warn("IOException closing input stream: " + ioe + "; ignoring.");
-        }
-      }
-
-      // Return true (isSequenceFile) iff the magic number sticks.
-      return header[0] == 'S' && header[1] == 'E' && header[2] == 'Q';
-    } catch (FileNotFoundException fnfe) {
-      LOG.warn("Input path " + p + " does not exist");
-      return false; // doesn't exist!
-    }
-  }
-
-  /**
-   * Run an export job to dump a table from HDFS to a database
-   * @throws IOException if the export job encounters an IO error
-   * @throws ExportException if the job fails unexpectedly or is misconfigured.
-   */
-  public void runExport() throws ExportException, IOException {
-
-    SqoopOptions options = context.getOptions();
-    Configuration conf = options.getConf();
-    String tableName = context.getTableName();
-    String tableClassName = new TableClassName(options).getClassForTable(tableName);
-    String ormJarFile = context.getJarFile();
-
-    LOG.info("Beginning export of " + tableName);
-
-    boolean isLocal = "local".equals(conf.get("mapreduce.jobtracker.address"))
-        || "local".equals(conf.get("mapred.job.tracker"));
-    ClassLoader prevClassLoader = null;
-    if (isLocal) {
-      // If we're using the LocalJobRunner, then instead of using the compiled jar file
-      // as the job source, we're running in the current thread. Push on another classloader
-      // that loads from that jar in addition to everything currently on the classpath.
-      prevClassLoader = ClassLoaderStack.addJarFile(ormJarFile, tableClassName);
-    }
-
-    try {
-      Job job = new Job(conf);
-
-      // Set the external jar to use for the job.
-      job.getConfiguration().set("mapred.jar", ormJarFile);
-
-      Path inputPath = new Path(context.getOptions().getExportDir());
-      inputPath = inputPath.makeQualified(FileSystem.get(conf));
-
-      if (isSequenceFiles(inputPath)) {
-        job.setInputFormatClass(SequenceFileInputFormat.class);
-        job.setMapperClass(SequenceFileExportMapper.class);
-      } else {
-        job.setInputFormatClass(TextInputFormat.class);
-        job.setMapperClass(TextExportMapper.class);
-      }
-
-      FileInputFormat.addInputPath(job, inputPath);
-      job.setNumReduceTasks(0);
-
-      // Concurrent writes of the same records would be problematic.
-      JobConf jobConf = (JobConf) job.getConfiguration();
-      jobConf.setMapSpeculativeExecution(false);
-
-      ConnManager mgr = new ConnFactory(conf).getManager(options);
-      String username = options.getUsername();
-      if (null == username || username.length() == 0) {
-        DBConfiguration.configureDB(job.getConfiguration(), mgr.getDriverClass(),
-            options.getConnectString());
-      } else {
-        DBConfiguration.configureDB(job.getConfiguration(), mgr.getDriverClass(),
-            options.getConnectString(), username, options.getPassword());
-      }
-
-      String [] colNames = options.getColumns();
-      if (null == colNames) {
-        colNames = mgr.getColumnNames(tableName);
-      }
-      DBOutputFormat.setOutput(job, tableName, colNames);
-
-      job.setOutputFormatClass(DBOutputFormat.class);
-      job.getConfiguration().set(SQOOP_EXPORT_TABLE_CLASS_KEY, tableClassName);
-      job.setMapOutputKeyClass(SqoopRecord.class);
-      job.setMapOutputValueClass(NullWritable.class);
-
-      try {
-        boolean success = job.waitForCompletion(false);
-        if (!success) {
-          throw new ExportException("Export job failed!");
-        }
-      } catch (InterruptedException ie) {
-        throw new IOException(ie);
-      } catch (ClassNotFoundException cnfe) {
-        throw new IOException(cnfe);
-      }
-    } finally {
-      if (isLocal && null != prevClassLoader) {
-        // unload the special classloader for this jar.
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/RawKeyTextOutputFormat.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/RawKeyTextOutputFormat.java
deleted file mode 100644
index c216ac6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/RawKeyTextOutputFormat.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.util.*;
-
-/** An {@link OutputFormat} that writes plain text files.
- * Only writes the key. Does not write any delimiter/newline after the key.
- */
-public class RawKeyTextOutputFormat<K, V> extends FileOutputFormat<K, V> {
-
-  protected static class RawKeyRecordWriter<K, V> extends RecordWriter<K, V> {
-    private static final String utf8 = "UTF-8";
-
-    protected DataOutputStream out;
-
-    public RawKeyRecordWriter(DataOutputStream out) {
-      this.out = out;
-    }
-
-    /**
-     * Write the object to the byte stream, handling Text as a special
-     * case.
-     * @param o the object to print
-     * @throws IOException if the write throws, we pass it on
-     */
-    private void writeObject(Object o) throws IOException {
-      if (o instanceof Text) {
-        Text to = (Text) o;
-        out.write(to.getBytes(), 0, to.getLength());
-      } else {
-        out.write(o.toString().getBytes(utf8));
-      }
-    }
-
-    public synchronized void write(K key, V value) throws IOException {
-      writeObject(key);
-    }
-
-    public synchronized void close(TaskAttemptContext context) throws IOException {
-      out.close();
-    }
-  }
-
-  public RecordWriter<K, V> getRecordWriter(TaskAttemptContext context)
-      throws IOException {
-    boolean isCompressed = getCompressOutput(context);
-    Configuration conf = context.getConfiguration();
-    String ext = "";
-    CompressionCodec codec = null;
-
-    if (isCompressed) {
-      // create the named codec
-      Class<? extends CompressionCodec> codecClass =
-        getOutputCompressorClass(context, GzipCodec.class);
-      codec = ReflectionUtils.newInstance(codecClass, conf);
-
-      ext = codec.getDefaultExtension();
-    }
-
-    Path file = getDefaultWorkFile(context, ext);
-    FileSystem fs = file.getFileSystem(conf);
-    FSDataOutputStream fileOut = fs.create(file, false);
-    DataOutputStream ostream = fileOut;
-
-    if (isCompressed) {
-      ostream = new DataOutputStream(codec.createOutputStream(fileOut));
-    }
-
-    return new RawKeyRecordWriter<K, V>(ostream);
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/SequenceFileExportMapper.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/SequenceFileExportMapper.java
deleted file mode 100644
index 15a9dd4..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/SequenceFileExportMapper.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-
-/**
- * Reads a SqoopRecord from the SequenceFile in which it's packed and emits
- * that DBWritable to the DBOutputFormat for writeback to the database.
- */
-public class SequenceFileExportMapper
-    extends AutoProgressMapper<LongWritable, SqoopRecord, SqoopRecord, NullWritable> {
-
-  public SequenceFileExportMapper() {
-  }
-
-  public void map(LongWritable key, SqoopRecord val, Context context)
-      throws IOException, InterruptedException {
-    context.write(val, NullWritable.get());
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextExportMapper.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextExportMapper.java
deleted file mode 100644
index cb1d002..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextExportMapper.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-import org.apache.hadoop.mapreduce.lib.db.DBWritable;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.lib.RecordParser;
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-
-/**
- * Converts an input record from a string representation to a parsed Sqoop record
- * and emits that DBWritable to the DBOutputFormat for writeback to the database.
- */
-public class TextExportMapper
-    extends AutoProgressMapper<LongWritable, Text, SqoopRecord, NullWritable> {
-
-  private SqoopRecord recordImpl;
-
-  public TextExportMapper() {
-  }
-
-  protected void setup(Context context) throws IOException, InterruptedException {
-    super.setup(context);
-
-    Configuration conf = context.getConfiguration();
-
-    // Instantiate a copy of the user's class to hold and parse the record.
-    String recordClassName = conf.get(ExportJob.SQOOP_EXPORT_TABLE_CLASS_KEY);
-    if (null == recordClassName) {
-      throw new IOException("Export table class name ("
-          + ExportJob.SQOOP_EXPORT_TABLE_CLASS_KEY
-          + ") is not set!");
-    }
-
-    try {
-      Class cls = Class.forName(recordClassName, true,
-          Thread.currentThread().getContextClassLoader());
-      recordImpl = (SqoopRecord) ReflectionUtils.newInstance(cls, conf);
-    } catch (ClassNotFoundException cnfe) {
-      throw new IOException(cnfe);
-    }
-
-    if (null == recordImpl) {
-      throw new IOException("Could not instantiate object of type " + recordClassName);
-    }
-  }
-
-
-  public void map(LongWritable key, Text val, Context context)
-      throws IOException, InterruptedException {
-    try {
-      recordImpl.parse(val);
-      context.write(recordImpl, NullWritable.get());
-    } catch (RecordParser.ParseError pe) {
-      throw new IOException("Could not parse record: " + val, pe);
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextImportMapper.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextImportMapper.java
deleted file mode 100644
index 3970105..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/mapreduce/TextImportMapper.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-import org.apache.hadoop.mapreduce.lib.db.DBWritable;
-
-/**
- * Converts an input record into a string representation and emit it.
- */
-public class TextImportMapper
-    extends AutoProgressMapper<LongWritable, DBWritable, Text, NullWritable> {
-
-  private Text outkey;
-
-  public TextImportMapper() {
-    outkey = new Text();
-  }
-
-  public void map(LongWritable key, DBWritable val, Context context)
-      throws IOException, InterruptedException {
-    outkey.set(val.toString());
-    context.write(outkey, NullWritable.get());
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/ClassWriter.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/ClassWriter.java
deleted file mode 100644
index 611e0b0..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/ClassWriter.java
+++ /dev/null
@@ -1,854 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.orm;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.SqlManager;
-import org.apache.hadoop.sqoop.lib.BigDecimalSerializer;
-import org.apache.hadoop.sqoop.lib.JdbcWritableBridge;
-import org.apache.hadoop.sqoop.lib.FieldFormatter;
-import org.apache.hadoop.sqoop.lib.RecordParser;
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-import java.util.HashSet;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Creates an ORM class to represent a table from a database
- */
-public class ClassWriter {
-
-  public static final Log LOG = LogFactory.getLog(ClassWriter.class.getName());
-
-  // The following are keywords and cannot be used for class, method, or field
-  // names.
-  public static final HashSet<String> JAVA_RESERVED_WORDS;
-
-  static {
-    JAVA_RESERVED_WORDS = new HashSet<String>();
-
-    JAVA_RESERVED_WORDS.add("abstract");
-    JAVA_RESERVED_WORDS.add("else");
-    JAVA_RESERVED_WORDS.add("int");
-    JAVA_RESERVED_WORDS.add("strictfp");
-    JAVA_RESERVED_WORDS.add("assert");
-    JAVA_RESERVED_WORDS.add("enum");
-    JAVA_RESERVED_WORDS.add("interface");
-    JAVA_RESERVED_WORDS.add("super");
-    JAVA_RESERVED_WORDS.add("boolean");
-    JAVA_RESERVED_WORDS.add("extends");
-    JAVA_RESERVED_WORDS.add("long");
-    JAVA_RESERVED_WORDS.add("switch");
-    JAVA_RESERVED_WORDS.add("break");
-    JAVA_RESERVED_WORDS.add("false");
-    JAVA_RESERVED_WORDS.add("native");
-    JAVA_RESERVED_WORDS.add("synchronized");
-    JAVA_RESERVED_WORDS.add("byte");
-    JAVA_RESERVED_WORDS.add("final");
-    JAVA_RESERVED_WORDS.add("new");
-    JAVA_RESERVED_WORDS.add("this");
-    JAVA_RESERVED_WORDS.add("case");
-    JAVA_RESERVED_WORDS.add("finally");
-    JAVA_RESERVED_WORDS.add("null");
-    JAVA_RESERVED_WORDS.add("throw");
-    JAVA_RESERVED_WORDS.add("catch");
-    JAVA_RESERVED_WORDS.add("float");
-    JAVA_RESERVED_WORDS.add("package");
-    JAVA_RESERVED_WORDS.add("throws");
-    JAVA_RESERVED_WORDS.add("char");
-    JAVA_RESERVED_WORDS.add("for");
-    JAVA_RESERVED_WORDS.add("private");
-    JAVA_RESERVED_WORDS.add("transient");
-    JAVA_RESERVED_WORDS.add("class");
-    JAVA_RESERVED_WORDS.add("goto");
-    JAVA_RESERVED_WORDS.add("protected");
-    JAVA_RESERVED_WORDS.add("true");
-    JAVA_RESERVED_WORDS.add("const");
-  }
-
-  /**
-   * This version number is injected into all generated Java classes to denote
-   * which version of the ClassWriter's output format was used to generate the
-   * class.
-   *
-   *  If the way that we generate classes, bump this number.
-   */
-  public static final int CLASS_WRITER_VERSION = 2;
-
-  private SqoopOptions options;
-  private ConnManager connManager;
-  private String tableName;
-  private CompilationManager compileManager;
-
-  /**
-   * Creates a new ClassWriter to generate an ORM class for a table.
-   * @param opts program-wide options
-   * @param connMgr the connection manager used to describe the table.
-   * @param table the name of the table to read.
-   */
-  public ClassWriter(final SqoopOptions opts, final ConnManager connMgr,
-      final String table, final CompilationManager compMgr) {
-    this.options = opts;
-    this.connManager = connMgr;
-    this.tableName = table;
-    this.compileManager = compMgr;
-  }
-
-  /**
-   * Given some character that can't be in an identifier,
-   * try to map it to a string that can.
-   *
-   * @param c a character that can't be in a Java identifier
-   * @return a string of characters that can, or null if there's
-   * no good translation.
-   */
-  static String getIdentifierStrForChar(char c) {
-    if (Character.isJavaIdentifierPart(c)) {
-      return "" + c;
-    } else if (Character.isWhitespace(c)) {
-      // Eliminate whitespace.
-      return null;
-    } else {
-      // All other characters map to underscore.
-      return "_";
-    }
-  }
-
-  /**
-   * @param word a word to test.
-   * @return true if 'word' is reserved the in Java language.
-   */
-  private static boolean isReservedWord(String word) {
-    return JAVA_RESERVED_WORDS.contains(word);
-  }
-
-  /**
-   * Coerce a candidate name for an identifier into one which will
-   * definitely compile.
-   *
-   * Ensures that the returned identifier matches [A-Za-z_][A-Za-z0-9_]*
-   * and is not a reserved word.
-   *
-   * @param candidate A string we want to use as an identifier
-   * @return A string naming an identifier which compiles and is
-   *   similar to the candidate.
-   */
-  public static String toIdentifier(String candidate) {
-    StringBuilder sb = new StringBuilder();
-    boolean first = true;
-    for (char c : candidate.toCharArray()) {
-      if (Character.isJavaIdentifierStart(c) && first) {
-        // Ok for this to be the first character of the identifier.
-        sb.append(c);
-        first = false;
-      } else if (Character.isJavaIdentifierPart(c) && !first) {
-        // Ok for this character to be in the output identifier.
-        sb.append(c);
-      } else {
-        // We have a character in the original that can't be
-        // part of this identifier we're building.
-        // If it's just not allowed to be the first char, add a leading '_'.
-        // If we have a reasonable translation (e.g., '-' -> '_'), do that.
-        // Otherwise, drop it.
-        if (first && Character.isJavaIdentifierPart(c)
-            && !Character.isJavaIdentifierStart(c)) {
-          sb.append("_");
-          sb.append(c);
-          first = false;
-        } else {
-          // Try to map this to a different character or string.
-          // If we can't just give up.
-          String translated = getIdentifierStrForChar(c);
-          if (null != translated) {
-            sb.append(translated);
-            first = false;
-          }
-        }
-      }
-    }
-
-    String output = sb.toString();
-    if (isReservedWord(output)) {
-      // e.g., 'class' -> '_class';
-      return "_" + output;
-    }
-
-    return output;
-  }
-
-  /**
-   * @param javaType
-   * @return the name of the method of JdbcWritableBridge to read an entry with a given java type.
-   */
-  private String dbGetterForType(String javaType) {
-    // All Class-based types (e.g., java.math.BigDecimal) are handled with
-    // "readBar" where some.package.foo.Bar is the canonical class name.
-    // Turn the javaType string into the getter type string.
-
-    String [] parts = javaType.split("\\.");
-    if (parts.length == 0) {
-      LOG.error("No ResultSet method for Java type " + javaType);
-      return null;
-    }
-
-    String lastPart = parts[parts.length - 1];
-    try {
-      String getter = "read" + Character.toUpperCase(lastPart.charAt(0)) + lastPart.substring(1);
-      return getter;
-    } catch (StringIndexOutOfBoundsException oob) {
-      // lastPart.*() doesn't work on empty strings.
-      LOG.error("Could not infer JdbcWritableBridge getter for Java type " + javaType);
-      return null;
-    }
-  }
-
-  /**
-   * @param javaType
-   * @return the name of the method of JdbcWritableBridge to write an entry with a given java type.
-   */
-  private String dbSetterForType(String javaType) {
-    // TODO(aaron): Lots of unit tests needed here.
-    // See dbGetterForType() for the logic used here; it's basically the same.
-
-    String [] parts = javaType.split("\\.");
-    if (parts.length == 0) {
-      LOG.error("No PreparedStatement Set method for Java type " + javaType);
-      return null;
-    }
-
-    String lastPart = parts[parts.length - 1];
-    try {
-      String setter = "write" + Character.toUpperCase(lastPart.charAt(0)) + lastPart.substring(1);
-      return setter;
-    } catch (StringIndexOutOfBoundsException oob) {
-      // lastPart.*() doesn't work on empty strings.
-      LOG.error("Could not infer PreparedStatement setter for Java type " + javaType);
-      return null;
-    }
-  }
-
-  private String stringifierForType(String javaType, String colName) {
-    if (javaType.equals("String")) {
-      return colName;
-    } else {
-      // this is an object type -- just call its toString() in a null-safe way.
-      return "\"\" + " + colName;
-    }
-  }
-
-  /**
-   * @param javaType the type to read
-   * @param inputObj the name of the DataInput to read from
-   * @param colName the column name to read
-   * @return the line of code involving a DataInput object to read an entry with a given java type.
-   */
-  private String rpcGetterForType(String javaType, String inputObj, String colName) {
-    if (javaType.equals("Integer")) {
-      return "    this." + colName + " = Integer.valueOf(" + inputObj + ".readInt());\n";
-    } else if (javaType.equals("Long")) {
-      return "    this." + colName + " = Long.valueOf(" + inputObj + ".readLong());\n";
-    } else if (javaType.equals("Float")) {
-      return "    this." + colName + " = Float.valueOf(" + inputObj + ".readFloat());\n";
-    } else if (javaType.equals("Double")) {
-      return "    this." + colName + " = Double.valueOf(" + inputObj + ".readDouble());\n";
-    } else if (javaType.equals("Boolean")) {
-      return "    this." + colName + " = Boolean.valueOf(" + inputObj + ".readBoolean());\n";
-    } else if (javaType.equals("String")) {
-      return "    this." + colName + " = Text.readString(" + inputObj + ");\n";
-    } else if (javaType.equals("java.sql.Date")) {
-      return "    this." + colName + " = new Date(" + inputObj + ".readLong());\n";
-    } else if (javaType.equals("java.sql.Time")) {
-      return "    this." + colName + " = new Time(" + inputObj + ".readLong());\n";
-    } else if (javaType.equals("java.sql.Timestamp")) {
-      return "    this." + colName + " = new Timestamp(" + inputObj + ".readLong());\n"
-          + "    this." + colName + ".setNanos(" + inputObj + ".readInt());\n";
-    } else if (javaType.equals("java.math.BigDecimal")) {
-      return "    this." + colName + " = " + BigDecimalSerializer.class.getCanonicalName()
-          + ".readFields(" + inputObj + ");\n";
-    } else {
-      LOG.error("No ResultSet method for Java type " + javaType);
-      return null;
-    }
-  }
-
-  /**
-   * Deserialize a possibly-null value from the DataInput stream
-   * @param javaType name of the type to deserialize if it's not null.
-   * @param inputObj name of the DataInput to read from
-   * @param colName the column name to read.
-   * @return
-   */
-  private String rpcGetterForMaybeNull(String javaType, String inputObj, String colName) {
-    return "    if (" + inputObj + ".readBoolean()) { \n"
-        + "        this." + colName + " = null;\n"
-        + "    } else {\n"
-        + rpcGetterForType(javaType, inputObj, colName)
-        + "    }\n";
-  }
-
-  /**
-   * @param javaType the type to write
-   * @param inputObj the name of the DataOutput to write to
-   * @param colName the column name to write
-   * @return the line of code involving a DataOutput object to write an entry with
-   *         a given java type.
-   */
-  private String rpcSetterForType(String javaType, String outputObj, String colName) {
-    if (javaType.equals("Integer")) {
-      return "    " + outputObj + ".writeInt(this." + colName + ");\n";
-    } else if (javaType.equals("Long")) {
-      return "    " + outputObj + ".writeLong(this." + colName + ");\n";
-    } else if (javaType.equals("Boolean")) {
-      return "    " + outputObj + ".writeBoolean(this." + colName + ");\n";
-    } else if (javaType.equals("Float")) {
-      return "    " + outputObj + ".writeFloat(this." + colName + ");\n";
-    } else if (javaType.equals("Double")) {
-      return "    " + outputObj + ".writeDouble(this." + colName + ");\n";
-    } else if (javaType.equals("String")) {
-      return "    Text.writeString(" + outputObj + ", " + colName + ");\n";
-    } else if (javaType.equals("java.sql.Date")) {
-      return "    " + outputObj + ".writeLong(this." + colName + ".getTime());\n";
-    } else if (javaType.equals("java.sql.Time")) {
-      return "    " + outputObj + ".writeLong(this." + colName + ".getTime());\n";
-    } else if (javaType.equals("java.sql.Timestamp")) {
-      return "    " + outputObj + ".writeLong(this." + colName + ".getTime());\n"
-          + "    " + outputObj + ".writeInt(this." + colName + ".getNanos());\n";
-    } else if (javaType.equals("java.math.BigDecimal")) {
-      return "    " + BigDecimalSerializer.class.getCanonicalName()
-          + ".write(this." + colName + ", " + outputObj + ");\n";
-    } else {
-      LOG.error("No ResultSet method for Java type " + javaType);
-      return null;
-    }
-  }
-
-  /**
-   * Serialize a possibly-null value to the DataOutput stream. First a boolean
-   * isNull is written, followed by the contents itself (if not null).
-   * @param javaType name of the type to deserialize if it's not null.
-   * @param inputObj name of the DataInput to read from
-   * @param colName the column name to read.
-   * @return
-   */
-  private String rpcSetterForMaybeNull(String javaType, String outputObj, String colName) {
-    return "    if (null == this." + colName + ") { \n"
-        + "        " + outputObj + ".writeBoolean(true);\n"
-        + "    } else {\n"
-        + "        " + outputObj + ".writeBoolean(false);\n"
-        + rpcSetterForType(javaType, outputObj, colName)
-        + "    }\n";
-  }
-
-  /**
-   * Generate a member field and getter method for each column
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateFields(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    for (String col : colNames) {
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("Cannot resolve SQL type " + sqlType);
-        continue;
-      }
-
-      sb.append("  private " + javaType + " " + col + ";\n");
-      sb.append("  public " + javaType + " get_" + col + "() {\n");
-      sb.append("    return " + col + ";\n");
-      sb.append("  }\n");
-    }
-  }
-
-  /**
-   * Generate the readFields() method used by the database
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateDbRead(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    sb.append("  public void readFields(ResultSet __dbResults) throws SQLException {\n");
-
-    int fieldNum = 0;
-
-    for (String col : colNames) {
-      fieldNum++;
-
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("No Java type for SQL type " + sqlType);
-        continue;
-      }
-
-      String getterMethod = dbGetterForType(javaType);
-      if (null == getterMethod) {
-        LOG.error("No db getter method for Java type " + javaType);
-        continue;
-      }
-
-      sb.append("    this." + col + " = JdbcWritableBridge." +  getterMethod
-          + "(" + fieldNum + ", __dbResults);\n");
-    }
-
-    sb.append("  }\n");
-  }
-
-
-  /**
-   * Generate the write() method used by the database
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateDbWrite(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    sb.append("  public void write(PreparedStatement __dbStmt) throws SQLException {\n");
-
-    int fieldNum = 0;
-
-    for (String col : colNames) {
-      fieldNum++;
-
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("No Java type for SQL type " + sqlType);
-        continue;
-      }
-
-      String setterMethod = dbSetterForType(javaType);
-      if (null == setterMethod) {
-        LOG.error("No db setter method for Java type " + javaType);
-        continue;
-      }
-
-      sb.append("    JdbcWritableBridge." + setterMethod + "(" + col + ", "
-          + fieldNum + ", " + sqlType + ", __dbStmt);\n");
-    }
-
-    sb.append("  }\n");
-  }
-
-
-  /**
-   * Generate the readFields() method used by the Hadoop RPC system
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateHadoopRead(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    sb.append("  public void readFields(DataInput __dataIn) throws IOException {\n");
-
-    for (String col : colNames) {
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("No Java type for SQL type " + sqlType);
-        continue;
-      }
-
-      String getterMethod = rpcGetterForMaybeNull(javaType, "__dataIn", col);
-      if (null == getterMethod) {
-        LOG.error("No RPC getter method for Java type " + javaType);
-        continue;
-      }
-
-      sb.append(getterMethod);
-    }
-
-    sb.append("  }\n");
-  }
-
-  /**
-   * Generate the toString() method
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateToString(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    // Embed the delimiters into the class, as characters...
-    sb.append("  private static final char __OUTPUT_FIELD_DELIM_CHAR = " +
-        + (int)options.getOutputFieldDelim() + ";\n");
-    sb.append("  private static final char __OUTPUT_RECORD_DELIM_CHAR = " 
-        + (int)options.getOutputRecordDelim() + ";\n");
-
-    // as strings...
-    sb.append("  private static final String __OUTPUT_FIELD_DELIM = \"\" + (char) "
-        + (int) options.getOutputFieldDelim() + ";\n");
-    sb.append("  private static final String __OUTPUT_RECORD_DELIM = \"\" + (char) " 
-        + (int) options.getOutputRecordDelim() + ";\n");
-    sb.append("  private static final String __OUTPUT_ENCLOSED_BY = \"\" + (char) " 
-        + (int) options.getOutputEnclosedBy() + ";\n");
-    sb.append("  private static final String __OUTPUT_ESCAPED_BY = \"\" + (char) " 
-        + (int) options.getOutputEscapedBy() + ";\n");
-
-    // and some more options.
-    sb.append("  private static final boolean __OUTPUT_ENCLOSE_REQUIRED = " 
-        + options.isOutputEncloseRequired() + ";\n");
-    sb.append("  private static final char [] __OUTPUT_DELIMITER_LIST = { "
-        + "__OUTPUT_FIELD_DELIM_CHAR, __OUTPUT_RECORD_DELIM_CHAR };\n\n");
-
-    // The actual toString() method itself follows.
-    sb.append("  public String toString() {\n");
-    sb.append("    StringBuilder __sb = new StringBuilder();\n");
-
-    boolean first = true;
-    for (String col : colNames) {
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("No Java type for SQL type " + sqlType);
-        continue;
-      }
-
-      if (!first) {
-        // print inter-field tokens.
-        sb.append("    __sb.append(__OUTPUT_FIELD_DELIM);\n");
-      }
-
-      first = false;
-
-      String stringExpr = stringifierForType(javaType, col);
-      if (null == stringExpr) {
-        LOG.error("No toString method for Java type " + javaType);
-        continue;
-      }
-
-      sb.append("    __sb.append(FieldFormatter.escapeAndEnclose(" + stringExpr 
-          + ", __OUTPUT_ESCAPED_BY, __OUTPUT_ENCLOSED_BY, __OUTPUT_DELIMITER_LIST, "
-          + "__OUTPUT_ENCLOSE_REQUIRED));\n");
-
-    }
-
-    sb.append("    __sb.append(__OUTPUT_RECORD_DELIM);\n");
-    sb.append("    return __sb.toString();\n");
-    sb.append("  }\n");
-  }
-
-
-
-  /**
-   * Helper method for generateParser(). Writes out the parse() method for one particular
-   * type we support as an input string-ish type.
-   */
-  private void generateParseMethod(String typ, StringBuilder sb) {
-    sb.append("  public void parse(" + typ + " __record) throws RecordParser.ParseError {\n");
-    sb.append("    if (null == this.__parser) {\n");
-    sb.append("      this.__parser = new RecordParser(__INPUT_FIELD_DELIM_CHAR, ");
-    sb.append("__INPUT_RECORD_DELIM_CHAR, __INPUT_ENCLOSED_BY_CHAR, __INPUT_ESCAPED_BY_CHAR, ");
-    sb.append("__INPUT_ENCLOSE_REQUIRED);\n");
-    sb.append("    }\n");
-    sb.append("    List<String> __fields = this.__parser.parseRecord(__record);\n");
-    sb.append("    __loadFromFields(__fields);\n");
-    sb.append("  }\n\n");
-  }
-
-  /**
-   * Helper method for parseColumn(). Interpret the string 'null' as a null
-   * for a particular column.
-   */
-  private void parseNullVal(String colName, StringBuilder sb) {
-    sb.append("    if (__cur_str.equals(\"null\")) { this.");
-    sb.append(colName);
-    sb.append(" = null; } else {\n");
-  }
-
-  /**
-   * Helper method for generateParser(). Generates the code that loads one field of
-   * a specified name and type from the next element of the field strings list.
-   */
-  private void parseColumn(String colName, int colType, StringBuilder sb) {
-    // assume that we have __it and __cur_str vars, based on __loadFromFields() code.
-    sb.append("    __cur_str = __it.next();\n");
-    String javaType = connManager.toJavaType(colType);
-
-    parseNullVal(colName, sb);
-    if (javaType.equals("String")) {
-      // TODO(aaron): Distinguish between 'null' and null. Currently they both set the
-      // actual object to null.
-      sb.append("      this." + colName + " = __cur_str;\n");
-    } else if (javaType.equals("Integer")) {
-      sb.append("      this." + colName + " = Integer.valueOf(__cur_str);\n");
-    } else if (javaType.equals("Long")) {
-      sb.append("      this." + colName + " = Long.valueOf(__cur_str);\n");
-    } else if (javaType.equals("Float")) {
-      sb.append("      this." + colName + " = Float.valueOf(__cur_str);\n");
-    } else if (javaType.equals("Double")) {
-      sb.append("      this." + colName + " = Double.valueOf(__cur_str);\n");
-    } else if (javaType.equals("Boolean")) {
-      sb.append("      this." + colName + " = Boolean.valueOf(__cur_str);\n");
-    } else if (javaType.equals("java.sql.Date")) {
-      sb.append("      this." + colName + " = java.sql.Date.valueOf(__cur_str);\n");
-    } else if (javaType.equals("java.sql.Time")) {
-      sb.append("      this." + colName + " = java.sql.Time.valueOf(__cur_str);\n");
-    } else if (javaType.equals("java.sql.Timestamp")) {
-      sb.append("      this." + colName + " = java.sql.Timestamp.valueOf(__cur_str);\n");
-    } else if (javaType.equals("java.math.BigDecimal")) {
-      sb.append("      this." + colName + " = new java.math.BigDecimal(__cur_str);\n");
-    } else {
-      LOG.error("No parser available for Java type " + javaType);
-    }
-
-    sb.append("    }\n\n"); // the closing '{' based on code in parseNullVal();
-  }
-
-  /**
-   * Generate the parse() method
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateParser(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    // Embed into the class the delimiter characters to use when parsing input records.
-    // Note that these can differ from the delims to use as output via toString(), if
-    // the user wants to use this class to convert one format to another.
-    sb.append("  private static final char __INPUT_FIELD_DELIM_CHAR = " +
-        + (int)options.getInputFieldDelim() + ";\n");
-    sb.append("  private static final char __INPUT_RECORD_DELIM_CHAR = " 
-        + (int)options.getInputRecordDelim() + ";\n");
-    sb.append("  private static final char __INPUT_ENCLOSED_BY_CHAR = " 
-        + (int)options.getInputEnclosedBy() + ";\n");
-    sb.append("  private static final char __INPUT_ESCAPED_BY_CHAR = " 
-        + (int)options.getInputEscapedBy() + ";\n");
-    sb.append("  private static final boolean __INPUT_ENCLOSE_REQUIRED = " 
-        + options.isInputEncloseRequired() + ";\n");
-
-
-    // The parser object which will do the heavy lifting for field splitting.
-    sb.append("  private RecordParser __parser;\n"); 
-
-    // Generate wrapper methods which will invoke the parser.
-    generateParseMethod("Text", sb);
-    generateParseMethod("CharSequence", sb);
-    generateParseMethod("byte []", sb);
-    generateParseMethod("char []", sb);
-    generateParseMethod("ByteBuffer", sb);
-    generateParseMethod("CharBuffer", sb);
-
-    // The wrapper methods call __loadFromFields() to actually interpret the raw
-    // field data as string, int, boolean, etc. The generation of this method is
-    // type-dependent for the fields.
-    sb.append("  private void __loadFromFields(List<String> fields) {\n");
-    sb.append("    Iterator<String> __it = fields.listIterator();\n");
-    sb.append("    String __cur_str;\n");
-    for (String colName : colNames) {
-      int colType = columnTypes.get(colName);
-      parseColumn(colName, colType, sb);
-    }
-    sb.append("  }\n\n");
-  }
-
-  /**
-   * Generate the write() method used by the Hadoop RPC system
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @param sb - StringBuilder to append code to
-   */
-  private void generateHadoopWrite(Map<String, Integer> columnTypes, String [] colNames,
-      StringBuilder sb) {
-
-    sb.append("  public void write(DataOutput __dataOut) throws IOException {\n");
-
-    for (String col : colNames) {
-      int sqlType = columnTypes.get(col);
-      String javaType = connManager.toJavaType(sqlType);
-      if (null == javaType) {
-        LOG.error("No Java type for SQL type " + sqlType);
-        continue;
-      }
-
-      String setterMethod = rpcSetterForMaybeNull(javaType, "__dataOut", col);
-      if (null == setterMethod) {
-        LOG.error("No RPC setter method for Java type " + javaType);
-        continue;
-      }
-
-      sb.append(setterMethod);
-    }
-
-    sb.append("  }\n");
-  }
-  /**
-   * Generate the ORM code for the class.
-   */
-  public void generate() throws IOException {
-    Map<String, Integer> columnTypes = connManager.getColumnTypes(tableName);
-
-    String [] colNames = options.getColumns();
-    if (null == colNames) {
-      colNames = connManager.getColumnNames(tableName);
-    }
-
-    // Translate all the column names into names that are safe to
-    // use as identifiers.
-    String [] cleanedColNames = new String[colNames.length];
-    for (int i = 0; i < colNames.length; i++) {
-      String col = colNames[i];
-      String identifier = toIdentifier(col);
-      cleanedColNames[i] = identifier;
-
-      // make sure the col->type mapping holds for the 
-      // new identifier name, too.
-      columnTypes.put(identifier, columnTypes.get(col));
-    }
-
-    // Generate the Java code
-    StringBuilder sb = generateClassForColumns(columnTypes, cleanedColNames);
-
-    // Write this out to a file.
-    String codeOutDir = options.getCodeOutputDir();
-
-    // Get the class name to generate, which includes package components
-    String className = new TableClassName(options).getClassForTable(tableName);
-    // convert the '.' characters to '/' characters
-    String sourceFilename = className.replace('.', File.separatorChar) + ".java";
-    String filename = codeOutDir + sourceFilename;
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Writing source file: " + filename);
-      LOG.debug("Table name: " + tableName);
-      StringBuilder sbColTypes = new StringBuilder();
-      for (String col : colNames) {
-        Integer colType = columnTypes.get(col);
-        sbColTypes.append(col + ":" + colType + ", ");
-      }
-      String colTypeStr = sbColTypes.toString();
-      LOG.debug("Columns: " + colTypeStr);
-      LOG.debug("sourceFilename is " + sourceFilename);
-    }
-
-    compileManager.addSourceFile(sourceFilename);
-
-    // Create any missing parent directories.
-    File file = new File(filename);
-    String dirname = file.getParent();
-    if (null != dirname) {
-      boolean mkdirSuccess = new File(dirname).mkdirs();
-      if (!mkdirSuccess) {
-        LOG.debug("Could not create directory tree for " + dirname);
-      }
-    }
-
-    OutputStream ostream = null;
-    Writer writer = null;
-    try {
-      ostream = new FileOutputStream(filename);
-      writer = new OutputStreamWriter(ostream);
-      writer.append(sb.toString());
-    } finally {
-      if (null != writer) {
-        try {
-          writer.close();
-        } catch (IOException ioe) {
-          // ignored because we're closing.
-        }
-      }
-
-      if (null != ostream) {
-        try {
-          ostream.close();
-        } catch (IOException ioe) {
-          // ignored because we're closing.
-        }
-      }
-    }
-  }
-
-  /**
-   * Generate the ORM code for a table object containing the named columns
-   * @param columnTypes - mapping from column names to sql types
-   * @param colNames - ordered list of column names for table.
-   * @return - A StringBuilder that contains the text of the class code.
-   */
-  public StringBuilder generateClassForColumns(Map<String, Integer> columnTypes,
-      String [] colNames) {
-    StringBuilder sb = new StringBuilder();
-    sb.append("// ORM class for " + tableName + "\n");
-    sb.append("// WARNING: This class is AUTO-GENERATED. Modify at your own risk.\n");
-
-    TableClassName tableNameInfo = new TableClassName(options);
-
-    String packageName = tableNameInfo.getPackageForTable();
-    if (null != packageName) {
-      sb.append("package ");
-      sb.append(packageName);
-      sb.append(";\n");
-    }
-
-    sb.append("import org.apache.hadoop.io.Text;\n");
-    sb.append("import org.apache.hadoop.io.Writable;\n");
-    sb.append("import org.apache.hadoop.mapred.lib.db.DBWritable;\n");
-    sb.append("import " + JdbcWritableBridge.class.getCanonicalName() + ";\n");
-    sb.append("import " + FieldFormatter.class.getCanonicalName() + ";\n");
-    sb.append("import " + RecordParser.class.getCanonicalName() + ";\n");
-    sb.append("import " + SqoopRecord.class.getCanonicalName() + ";\n");
-    sb.append("import java.sql.PreparedStatement;\n");
-    sb.append("import java.sql.ResultSet;\n");
-    sb.append("import java.sql.SQLException;\n");
-    sb.append("import java.io.DataInput;\n");
-    sb.append("import java.io.DataOutput;\n");
-    sb.append("import java.io.IOException;\n");
-    sb.append("import java.nio.ByteBuffer;\n");
-    sb.append("import java.nio.CharBuffer;\n");
-    sb.append("import java.sql.Date;\n");
-    sb.append("import java.sql.Time;\n");
-    sb.append("import java.sql.Timestamp;\n");
-    sb.append("import java.util.Iterator;\n");
-    sb.append("import java.util.List;\n");
-
-    String className = tableNameInfo.getShortClassForTable(tableName);
-    sb.append("public class " + className + " implements DBWritable, SqoopRecord, Writable {\n");
-    sb.append("  public static final int PROTOCOL_VERSION = " + CLASS_WRITER_VERSION + ";\n");
-    generateFields(columnTypes, colNames, sb);
-    generateDbRead(columnTypes, colNames, sb);
-    generateDbWrite(columnTypes, colNames, sb);
-    generateHadoopRead(columnTypes, colNames, sb);
-    generateHadoopWrite(columnTypes, colNames, sb);
-    generateToString(columnTypes, colNames, sb);
-    generateParser(columnTypes, colNames, sb);
-    // TODO(aaron): Generate hashCode(), compareTo(), equals() so it can be a WritableComparable
-
-    sb.append("}\n");
-
-    return sb;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/CompilationManager.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/CompilationManager.java
deleted file mode 100644
index 99630e6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/CompilationManager.java
+++ /dev/null
@@ -1,385 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.orm;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.net.URL;
-import java.net.URLDecoder;
-import java.util.ArrayList;
-import java.util.Enumeration;
-import java.util.List;
-import java.util.jar.JarOutputStream;
-import java.util.zip.ZipEntry;
-
-import javax.tools.JavaCompiler;
-import javax.tools.JavaFileObject;
-import javax.tools.StandardJavaFileManager;
-import javax.tools.ToolProvider;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapred.JobConf;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.util.FileListing;
-
-/**
- * Manages the compilation of a bunch of .java files into .class files
- * and eventually a jar.
- *
- * Also embeds this program's jar into the lib/ directory inside the compiled jar
- * to ensure that the job runs correctly.
- *
- * 
- *
- */
-public class CompilationManager {
-
-  public static final Log LOG = LogFactory.getLog(CompilationManager.class.getName());
-
-  private SqoopOptions options;
-  private List<String> sources;
-
-  public CompilationManager(final SqoopOptions opts) {
-    options = opts;
-    sources = new ArrayList<String>();
-  }
-
-  public void addSourceFile(String sourceName) {
-    sources.add(sourceName);
-  }
-
-  /**
-   * locate the hadoop-*-core.jar in $HADOOP_HOME or --hadoop-home.
-   * If that doesn't work, check our classpath.
-   * @return the filename of the hadoop-*-core.jar file.
-   */
-  private String findHadoopCoreJar() {
-    String hadoopHome = options.getHadoopHome();
-
-    if (null == hadoopHome) {
-      LOG.info("$HADOOP_HOME is not set");
-      return findJarForClass(JobConf.class);
-    }
-
-    if (!hadoopHome.endsWith(File.separator)) {
-      hadoopHome = hadoopHome + File.separator;
-    }
-
-    File hadoopHomeFile = new File(hadoopHome);
-    LOG.info("HADOOP_HOME is " + hadoopHomeFile.getAbsolutePath());
-    File [] entries = hadoopHomeFile.listFiles();
-
-    if (null == entries) {
-      LOG.warn("HADOOP_HOME appears empty or missing");
-      return findJarForClass(JobConf.class);
-    }
-
-    for (File f : entries) {
-      if (f.getName().startsWith("hadoop-") && f.getName().endsWith("-core.jar")) {
-        LOG.info("Found hadoop core jar at: " + f.getAbsolutePath());
-        return f.getAbsolutePath();
-      }
-    }
-
-    return findJarForClass(JobConf.class);
-  }
-
-  /**
-   * Compile the .java files into .class files via embedded javac call.
-   */
-  public void compile() throws IOException {
-    List<String> args = new ArrayList<String>();
-
-    // ensure that the jar output dir exists.
-    String jarOutDir = options.getJarOutputDir();
-    File jarOutDirObj = new File(jarOutDir);
-    if (!jarOutDirObj.exists()) {
-      boolean mkdirSuccess = jarOutDirObj.mkdirs();
-      if (!mkdirSuccess) {
-        LOG.debug("Warning: Could not make directories for " + jarOutDir);
-      }
-    } else if (LOG.isDebugEnabled()) {
-      LOG.debug("Found existing " + jarOutDir);
-    }
-
-    // find hadoop-*-core.jar for classpath.
-    String coreJar = findHadoopCoreJar();
-    if (null == coreJar) {
-      // Couldn't find a core jar to insert into the CP for compilation.
-      // If, however, we're running this from a unit test, then the path
-      // to the .class files might be set via the hadoop.alt.classpath property
-      // instead. Check there first.
-      String coreClassesPath = System.getProperty("hadoop.alt.classpath");
-      if (null == coreClassesPath) {
-        // no -- we're out of options. Fail.
-        throw new IOException("Could not find hadoop core jar!");
-      } else {
-        coreJar = coreClassesPath;
-      }
-    }
-
-    // find sqoop jar for compilation classpath
-    String sqoopJar = findThisJar();
-    if (null != sqoopJar) {
-      sqoopJar = File.pathSeparator + sqoopJar;
-    } else {
-      LOG.warn("Could not find sqoop jar; child compilation may fail");
-      sqoopJar = "";
-    }
-
-    String curClasspath = System.getProperty("java.class.path");
-
-    String srcOutDir = new File(options.getCodeOutputDir()).getAbsolutePath();
-    if (!srcOutDir.endsWith(File.separator)) {
-      srcOutDir = srcOutDir + File.separator;
-    }
-
-    args.add("-sourcepath");
-    args.add(srcOutDir);
-
-    args.add("-d");
-    args.add(jarOutDir);
-
-    args.add("-classpath");
-    args.add(curClasspath + File.pathSeparator + coreJar + sqoopJar);
-
-    JavaCompiler compiler = ToolProvider.getSystemJavaCompiler();
-    StandardJavaFileManager fileManager =
-        compiler.getStandardFileManager(null, null, null);
-
-    ArrayList<String> srcFileNames = new ArrayList<String>();
-    for (String srcfile : sources) {
-      srcFileNames.add(srcOutDir + srcfile);
-      LOG.debug("Adding source file: " + srcOutDir + srcfile);
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Invoking javac with args:");
-      for (String arg : args) {
-        LOG.debug("  " + arg);
-      }
-    }
-
-    Iterable<? extends JavaFileObject> srcFileObjs =
-        fileManager.getJavaFileObjectsFromStrings(srcFileNames);
-    JavaCompiler.CompilationTask task = compiler.getTask(
-        null, // Write to stderr
-        fileManager,
-        null, // No special diagnostic handling
-        args,
-        null, // Compile all classes in the source compilation units
-        srcFileObjs);
-
-    boolean result = task.call();
-    if (!result) {
-      throw new IOException("Error returned by javac");
-    }
-  }
-
-  /**
-   * @return the complete filename of the .jar file to generate */
-  public String getJarFilename() {
-    String jarOutDir = options.getJarOutputDir();
-    String tableName = options.getTableName();
-    if (null != tableName && tableName.length() > 0) {
-      return jarOutDir + tableName + ".jar";
-    } else if (this.sources.size() == 1) {
-      // if we only have one source file, find it's base name,
-      // turn "foo.java" into "foo", and then return jarDir + "foo" + ".jar"
-      String srcFileName = this.sources.get(0);
-      String basename = new File(srcFileName).getName();
-      String [] parts = basename.split("\\.");
-      String preExtPart = parts[0];
-      return jarOutDir + preExtPart + ".jar";
-    } else {
-      return jarOutDir + "sqoop.jar";
-    }
-  }
-
-  /**
-   * Searches through a directory and its children for .class
-   * files to add to a jar.
-   *
-   * @param dir - The root directory to scan with this algorithm.
-   * @param jstream - The JarOutputStream to write .class files to.
-   */
-  private void addClassFilesFromDir(File dir, JarOutputStream jstream)
-      throws IOException {
-    LOG.debug("Scanning for .class files in directory: " + dir);
-    List<File> dirEntries = FileListing.getFileListing(dir);
-    String baseDirName = dir.getAbsolutePath();
-    if (!baseDirName.endsWith(File.separator)) {
-      baseDirName = baseDirName + File.separator;
-    }
-
-    // for each input class file, create a zipfile entry for it,
-    // read the file into a buffer, and write it to the jar file.
-    for (File entry : dirEntries) {
-      if (!entry.isDirectory()) {
-        // chomp off the portion of the full path that is shared
-        // with the base directory where class files were put;
-        // we only record the subdir parts in the zip entry.
-        String fullPath = entry.getAbsolutePath();
-        String chompedPath = fullPath.substring(baseDirName.length());
-
-        boolean include = chompedPath.endsWith(".class")
-            && sources.contains(
-            chompedPath.substring(0, chompedPath.length() - ".class".length()) + ".java");
-
-        if (include) {
-          // include this file.
-          LOG.debug("Got classfile: " + entry.getPath() + " -> " + chompedPath);
-          ZipEntry ze = new ZipEntry(chompedPath);
-          jstream.putNextEntry(ze);
-          copyFileToStream(entry, jstream);
-          jstream.closeEntry();
-        }
-      }
-    }
-  }
-
-  /**
-   * Create an output jar file to use when executing MapReduce jobs
-   */
-  public void jar() throws IOException {
-    String jarOutDir = options.getJarOutputDir();
-
-    String jarFilename = getJarFilename();
-
-    LOG.info("Writing jar file: " + jarFilename);
-
-    File jarFileObj = new File(jarFilename);
-    if (jarFileObj.exists()) {
-      LOG.debug("Found existing jar (" + jarFilename + "); removing.");
-      if (!jarFileObj.delete()) {
-        LOG.warn("Could not remove existing jar file: " + jarFilename);
-      }
-    }
-
-    FileOutputStream fstream = null;
-    JarOutputStream jstream = null;
-    try {
-      fstream = new FileOutputStream(jarFilename);
-      jstream = new JarOutputStream(fstream);
-
-      addClassFilesFromDir(new File(jarOutDir), jstream);
-
-      // put our own jar in there in its lib/ subdir
-      String thisJarFile = findThisJar();
-      if (null != thisJarFile) {
-        File thisJarFileObj = new File(thisJarFile);
-        String thisJarBasename = thisJarFileObj.getName();
-        String thisJarEntryName = "lib" + File.separator + thisJarBasename;
-        ZipEntry ze = new ZipEntry(thisJarEntryName);
-        jstream.putNextEntry(ze);
-        copyFileToStream(thisJarFileObj, jstream);
-        jstream.closeEntry();
-      } else {
-        // couldn't find our own jar (we were running from .class files?)
-        LOG.warn("Could not find jar for Sqoop; MapReduce jobs may not run correctly.");
-      }
-
-      jstream.finish();
-    } finally {
-      if (null != jstream) {
-        try {
-          jstream.close();
-        } catch (IOException ioe) {
-          LOG.warn("IOException closing jar stream: " + ioe.toString());
-        }
-      }
-
-      if (null != fstream) {
-        try {
-          fstream.close();
-        } catch (IOException ioe) {
-          LOG.warn("IOException closing file stream: " + ioe.toString());
-        }
-      }
-    }
-
-    LOG.debug("Finished writing jar file " + jarFilename);
-  }
-
-
-  private static final int BUFFER_SZ = 4096;
-
-  /**
-   * utility method to copy a .class file into the jar stream.
-   * @param f
-   * @param ostream
-   * @throws IOException
-   */
-  private void copyFileToStream(File f, OutputStream ostream) throws IOException {
-    FileInputStream fis = new FileInputStream(f);
-    byte [] buffer = new byte[BUFFER_SZ];
-    try {
-      while (true) {
-        int bytesReceived = fis.read(buffer);
-        if (bytesReceived < 1) {
-          break;
-        }
-
-        ostream.write(buffer, 0, bytesReceived);
-      }
-    } finally {
-      fis.close();
-    }
-  }
-
-  private String findThisJar() {
-    return findJarForClass(CompilationManager.class);
-  }
-
-  // method mostly cloned from o.a.h.mapred.JobConf.findContainingJar()
-  private String findJarForClass(Class<? extends Object> classObj) {
-    ClassLoader loader = classObj.getClassLoader();
-    String classFile = classObj.getName().replaceAll("\\.", "/") + ".class";
-    try {
-      for (Enumeration<URL> itr = loader.getResources(classFile);
-          itr.hasMoreElements();) {
-        URL url = (URL) itr.nextElement();
-        if ("jar".equals(url.getProtocol())) {
-          String toReturn = url.getPath();
-          if (toReturn.startsWith("file:")) {
-            toReturn = toReturn.substring("file:".length());
-          }
-          // URLDecoder is a misnamed class, since it actually decodes
-          // x-www-form-urlencoded MIME type rather than actual
-          // URL encoding (which the file path has). Therefore it would
-          // decode +s to ' 's which is incorrect (spaces are actually
-          // either unencoded or encoded as "%20"). Replace +s first, so
-          // that they are kept sacred during the decoding process.
-          toReturn = toReturn.replaceAll("\\+", "%2B");
-          toReturn = URLDecoder.decode(toReturn, "UTF-8");
-          return toReturn.replaceAll("!.*$", "");
-        }
-      }
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-    return null;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/TableClassName.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/TableClassName.java
deleted file mode 100644
index 2d766f7..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/orm/TableClassName.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.orm;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Reconciles the table name being imported with the class naming information
- * specified in SqoopOptions to determine the actual package and class name
- * to use for a table.
- */
-public class TableClassName {
-
-  public static final Log LOG = LogFactory.getLog(TableClassName.class.getName());
-
-  private final SqoopOptions options;
-
-  public TableClassName(final SqoopOptions opts) {
-    if (null == opts) {
-      throw new NullPointerException("Cannot instantiate a TableClassName on null options.");
-    } else {
-      this.options = opts;
-    }
-  }
-
-  /**
-   * Taking into account --class-name and --package-name, return the actual
-   * package-part which will be used for a class. The actual table name being
-   * generated-for is irrelevant; so not an argument.
-   *
-   * @return the package where generated ORM classes go. Will be null for top-level.
-   */
-  public String getPackageForTable() {
-    String predefinedClass = options.getClassName();
-    if (null != predefinedClass) {
-      // if the predefined classname contains a package-part, return that.
-      int lastDot = predefinedClass.lastIndexOf('.');
-      if (-1 == lastDot) {
-        // no package part.
-        return null;
-      } else {
-        // return the string up to but not including the last dot.
-        return predefinedClass.substring(0, lastDot);
-      }
-    } else {
-      // If the user has specified a package name, return it.
-      // This will be null if the user hasn't specified one -- as we expect.
-      return options.getPackageName();
-    }
-  }
-
-  /**
-   * @param tableName the name of the table being imported
-   * @return the full name of the class to generate/use to import a table
-   */
-  public String getClassForTable(String tableName) {
-    if (null == tableName) {
-      return null;
-    }
-
-    String predefinedClass = options.getClassName();
-    if (predefinedClass != null) {
-      // The user's chosen a specific class name for this job.
-      return predefinedClass;
-    }
-
-    String packageName = options.getPackageName();
-    if (null != packageName) {
-      // return packageName.tableName.
-      return packageName + "." + tableName;
-    }
-
-    // no specific class; no specific package.
-    // Just make sure it's a legal identifier.
-    return ClassWriter.toIdentifier(tableName);
-  }
-
-  /**
-   * @return just the last spegment of the class name -- all package info stripped. 
-   */
-  public String getShortClassForTable(String tableName) {
-    String fullClass = getClassForTable(tableName);
-    if (null == fullClass) {
-      return null;
-    }
-
-    int lastDot = fullClass.lastIndexOf('.');
-    if (-1 == lastDot) {
-      return fullClass;
-    } else {
-      return fullClass.substring(lastDot + 1, fullClass.length());
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/AsyncSink.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/AsyncSink.java
deleted file mode 100644
index dca6017..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/AsyncSink.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.InputStream;
-
-/**
- * An interface describing a factory class for a Thread class that handles
- * input from some sort of stream.
- *
- * When the stream is closed, the thread should terminate.
- */
-public abstract class AsyncSink {
-  
-  /**
-   * Create and run a thread to handle input from the provided InputStream.
-   * When processStream returns, the thread should be running; it should
-   * continue to run until the InputStream is exhausted.
-   */
-  public abstract void processStream(InputStream is);
-
-  /**
-   * Wait until the stream has been processed.
-   * @return a status code indicating success or failure. 0 is typical for success.
-   */
-  public abstract int join() throws InterruptedException;
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ClassLoaderStack.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ClassLoaderStack.java
deleted file mode 100644
index bf30376..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ClassLoaderStack.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.File;
-import java.io.IOException;
-import java.net.URL;
-import java.net.URLClassLoader;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Allows you to add and remove jar-files from the running JVM by
- * instantiating classloaders for them.
- *
- * 
- *
- */
-public final class ClassLoaderStack {
-
-  public static final Log LOG = LogFactory.getLog(ClassLoaderStack.class.getName());
-
-  private ClassLoaderStack() {
-  }
-
-  /**
-   * Sets the classloader for the current thread
-   */
-  public static void setCurrentClassLoader(ClassLoader cl) {
-    LOG.info("Restoring classloader: " + cl.toString());
-    Thread.currentThread().setContextClassLoader(cl);
-  }
-
-  /**
-   * Adds a ClassLoader to the top of the stack that will load from the Jar file
-   * of your choice. Returns the previous classloader so you can restore it
-   * if need be, later.
-   *
-   * @param jarFile The filename of a jar file that you want loaded into this JVM
-   * @param tableClassName The name of the class to load immediately (optional)
-   */
-  public static ClassLoader addJarFile(String jarFile, String testClassName)
-      throws IOException {
-
-    // load the classes from the ORM JAR file into the current VM
-    ClassLoader prevClassLoader = Thread.currentThread().getContextClassLoader();
-    String urlPath = "jar:file://" + new File(jarFile).getAbsolutePath() + "!/";
-    LOG.debug("Attempting to load jar through URL: " + urlPath);
-    LOG.debug("Previous classloader is " + prevClassLoader);
-    URL [] jarUrlArray = {new URL(urlPath)};
-    URLClassLoader cl = URLClassLoader.newInstance(jarUrlArray, prevClassLoader);
-    try {
-      if (null != testClassName) {
-        // try to load a class from the jar to force loading now.
-        Class.forName(testClassName, true, cl);
-      }
-      LOG.info("Loaded jar into current JVM: " + urlPath);
-    } catch (ClassNotFoundException cnfe) {
-      throw new IOException("Could not load jar " + jarFile + " into JVM. (Could not find class "
-          + testClassName + ".)", cnfe);
-    }
-
-    LOG.info("Added classloader for jar " + jarFile + ": " + cl);
-    Thread.currentThread().setContextClassLoader(cl);
-    return prevClassLoader;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/DirectImportUtils.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/DirectImportUtils.java
deleted file mode 100644
index 5c13063..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/DirectImportUtils.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.IOException;
-import java.io.File;
-import java.net.InetAddress;
-import java.net.UnknownHostException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.io.SplittingOutputStream;
-import org.apache.hadoop.sqoop.io.SplittableBufferedWriter;
-import org.apache.hadoop.util.Shell;
-
-/**
- * Utility methods that are common to various the direct import managers.
- */
-public final class DirectImportUtils {
-
-  public static final Log LOG = LogFactory.getLog(
-      DirectImportUtils.class.getName());
-
-  private DirectImportUtils() {
-  }
-
-  /**
-   * Executes chmod on the specified file, passing in the mode string 'modstr'
-   * which may be e.g. "a+x" or "0600", etc.
-   * @throws IOException if chmod failed.
-   */
-  public static void setFilePermissions(File file, String modstr)
-      throws IOException {
-    // Set this file to be 0600. Java doesn't have a built-in mechanism for this
-    // so we need to go out to the shell to execute chmod.
-    try {
-      Shell.execCommand("chmod", modstr, file.toString());
-    } catch (IOException ioe) {
-      // Shell.execCommand will throw IOException on exit code != 0.
-      LOG.error("Could not chmod " + modstr + " " + file.toString());
-      throw new IOException("Could not ensure password file security.", ioe);
-    }
-  }
-
-  /**
-   * Open a file in HDFS for write to hold the data associated with a table.
-   * Creates any necessary directories, and returns the OutputStream to write
-   * to. The caller is responsible for calling the close() method on the
-   * returned stream.
-   */
-  public static SplittableBufferedWriter createHdfsSink(Configuration conf,
-      SqoopOptions options, String tableName) throws IOException {
-
-    FileSystem fs = FileSystem.get(conf);
-    String warehouseDir = options.getWarehouseDir();
-    Path destDir = null;
-    if (null != warehouseDir) {
-      destDir = new Path(new Path(warehouseDir), tableName);
-    } else {
-      destDir = new Path(tableName);
-    }
-
-    LOG.debug("Writing to filesystem: " + conf.get("fs.default.name"));
-    LOG.debug("Creating destination directory " + destDir);
-    fs.mkdirs(destDir);
-
-    // This Writer will be closed by the caller.
-    return new SplittableBufferedWriter(
-        new SplittingOutputStream(conf, destDir, "data-",
-        options.getDirectSplitSize(), options.shouldUseCompression()));
-  }
-
-  /** @return true if someHost refers to localhost.
-   */
-  public static boolean isLocalhost(String someHost) {
-    if (null == someHost) {
-      return false;
-    }
-
-    try {
-      InetAddress localHostAddr = InetAddress.getLocalHost();
-      InetAddress someAddr = InetAddress.getByName(someHost);
-
-      return localHostAddr.equals(someAddr);
-    } catch (UnknownHostException uhe) {
-      return false;
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableAsyncSink.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableAsyncSink.java
deleted file mode 100644
index 2a1f002..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableAsyncSink.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.InputStream;
-
-/**
- * Partial implementation of AsyncSink that relies on ErrorableThread to
- * provide a status bit for the join() method.
- */
-public abstract class ErrorableAsyncSink extends AsyncSink {
-
-  protected ErrorableThread child;
-
-  public int join() throws InterruptedException {
-    child.join();
-    if (child.isErrored()) {
-      return 1;
-    } else {
-      return 0;
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableThread.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableThread.java
deleted file mode 100644
index 94d1e58..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ErrorableThread.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-/**
- * A thread which has an error bit which can be set from within the thread.
- */
-public abstract class ErrorableThread extends Thread {
-
-  private volatile boolean error;
-
-  public ErrorableThread() {
-    this.error = false;
-  }
-
-  protected void setError() {
-    this.error = true;
-  }
-
-  public boolean isErrored() {
-    return this.error;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/Executor.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/Executor.java
deleted file mode 100644
index 769eb88..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/Executor.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Runs a process via Runtime.exec() and allows handling of stdout/stderr to be
- * deferred to other threads.
- *
- */
-public final class Executor {
-  
-  public static final Log LOG = LogFactory.getLog(Executor.class.getName());
-
-  private Executor() {
-  }
-
-  /**
-   * Execute a program defined by the args array with default stream sinks
-   * that consume the program's output (to prevent it from blocking on buffers)
-   * and then ignore said output.
-   */
-  public static int exec(String [] args) throws IOException {
-    NullAsyncSink s = new NullAsyncSink();
-    return exec(args, s, s);
-  }
-
-  /**
-   * Run a command via Runtime.exec(), with its stdout and stderr streams
-   * directed to be handled by threads generated by AsyncSinks.
-   * Block until the child process terminates. 
-   *
-   * @return the exit status of the ran program
-   */
-  public static int exec(String [] args, AsyncSink outSink,
-      AsyncSink errSink) throws IOException {
-    return exec(args, null, outSink, errSink);
-  }
-
-
-  /**
-   * Run a command via Runtime.exec(), with its stdout and stderr streams
-   * directed to be handled by threads generated by AsyncSinks.
-   * Block until the child process terminates. Allows the programmer to
-   * specify an environment for the child program.
-   *
-   * @return the exit status of the ran program
-   */
-  public static int exec(String [] args, String [] envp, AsyncSink outSink,
-      AsyncSink errSink) throws IOException {
-
-    // launch the process.
-    Process p = Runtime.getRuntime().exec(args, envp);
-
-    // dispatch its stdout and stderr to stream sinks if available.
-    if (null != outSink) {
-      outSink.processStream(p.getInputStream());
-    } 
-
-    if (null != errSink) {
-      errSink.processStream(p.getErrorStream());
-    }
-
-    // wait for the return value.
-    while (true) {
-      try {
-        int ret = p.waitFor();
-        return ret;
-      } catch (InterruptedException ie) {
-        continue;
-      }
-    }
-  }
-
-
-  /**
-   * @return An array formatted correctly for use as an envp based on the
-   * current environment for this program.
-   */
-  public static List<String> getCurEnvpStrings() {
-    Map<String, String> curEnv = System.getenv();
-    ArrayList<String> array = new ArrayList<String>();
-
-    if (null == curEnv) {
-      return null;
-    }
-
-    for (Map.Entry<String, String> entry : curEnv.entrySet()) {
-      array.add(entry.getKey() + "=" + entry.getValue());
-    }
-
-    return array;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ExportException.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ExportException.java
deleted file mode 100644
index ffc0e75..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ExportException.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-/**
- * General error during export process.
- */
-@SuppressWarnings("serial")
-public class ExportException extends Exception {
-
-  public ExportException() {
-    super("ExportException");
-  }
-
-  public ExportException(final String message) {
-    super(message);
-  }
-
-  public ExportException(final Throwable cause) {
-    super(cause);
-  }
-
-  public ExportException(final String message, final Throwable cause) {
-    super(message, cause);
-  }
-
-  @Override
-  public String toString() {
-    String msg = getMessage();
-    return (null == msg) ? "ExportException" : msg;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/FileListing.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/FileListing.java
deleted file mode 100644
index b172fa6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/FileListing.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-/**
-* Recursive file listing under a specified directory.
-*
-* Taken from http://www.javapractices.com/topic/TopicAction.do?Id=68
-* Used under the terms of the CC Attribution license:
-* http://creativecommons.org/licenses/by/3.0/
-*
-* Method by Alex Wong (javapractices.com)
-*/
-public final class FileListing {
-
-  private FileListing() { }
-
-  /**
-  * Demonstrate use.
-  *
-  * @param aArgs - <tt>aArgs[0]</tt> is the full name of an existing
-  * directory that can be read.
-  */
-  public static void main(String... aArgs) throws FileNotFoundException {
-    File startingDirectory = new File(aArgs[0]);
-    List<File> files = FileListing.getFileListing(startingDirectory);
-
-    //print out all file names, in the the order of File.compareTo()
-    for (File file : files) {
-      System.out.println(file);
-    }
-  }
-
-  /**
-  * Recursively walk a directory tree and return a List of all
-  * Files found; the List is sorted using File.compareTo().
-  *
-  * @param aStartingDir is a valid directory, which can be read.
-  */
-  public static List<File> getFileListing(File aStartingDir) throws FileNotFoundException {
-    validateDirectory(aStartingDir);
-    List<File> result = getFileListingNoSort(aStartingDir);
-    Collections.sort(result);
-    return result;
-  }
-
-  // PRIVATE //
-  private static List<File> getFileListingNoSort(File aStartingDir) throws FileNotFoundException {
-    List<File> result = new ArrayList<File>();
-    File[] filesAndDirs = aStartingDir.listFiles();
-    List<File> filesDirs = Arrays.asList(filesAndDirs);
-    for (File file : filesDirs) {
-      result.add(file); //always add, even if directory
-      if (!file.isFile()) {
-        //must be a directory
-        //recursive call!
-        List<File> deeperList = getFileListingNoSort(file);
-        result.addAll(deeperList);
-      }
-    }
-    return result;
-  }
-
-  /**
-  * Directory is valid if it exists, does not represent a file, and can be read.
-  */
-  private static void validateDirectory(File aDirectory) throws FileNotFoundException {
-    if (aDirectory == null) {
-      throw new IllegalArgumentException("Directory should not be null.");
-    }
-    if (!aDirectory.exists()) {
-      throw new FileNotFoundException("Directory does not exist: " + aDirectory);
-    }
-    if (!aDirectory.isDirectory()) {
-      throw new IllegalArgumentException("Is not a directory: " + aDirectory);
-    }
-    if (!aDirectory.canRead()) {
-      throw new IllegalArgumentException("Directory cannot be read: " + aDirectory);
-    }
-  }
-
-  /**
-   * Recursively delete a directory and all its children
-   * @param aStartingDir is a valid directory.
-   */
-  public static void recursiveDeleteDir(File dir) throws IOException {
-    if (!dir.exists()) {
-      throw new FileNotFoundException(dir.toString() + " does not exist");
-    }
-
-    if (dir.isDirectory()) {
-      // recursively descend into all children and delete them.
-      File [] children = dir.listFiles();
-      for (File child : children) {
-        recursiveDeleteDir(child);
-      }
-    }
-
-    if (!dir.delete()) {
-      throw new IOException("Could not remove: " + dir);
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportException.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportException.java
deleted file mode 100644
index c6381a2..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ImportException.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-/**
- * General error during import process.
- *
- * 
- */
-@SuppressWarnings("serial")
-public class ImportException extends Exception {
-
-  public ImportException() {
-    super("ImportException");
-  }
-
-  public ImportException(final String message) {
-    super(message);
-  }
-
-  public ImportException(final Throwable cause) {
-    super(cause);
-  }
-
-  public ImportException(final String message, final Throwable cause) {
-    super(message, cause);
-  }
-
-  @Override
-  public String toString() {
-    String msg = getMessage();
-    return (null == msg) ? "ImportException" : msg;
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/JdbcUrl.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/JdbcUrl.java
deleted file mode 100644
index de923c6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/JdbcUrl.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.net.MalformedURLException;
-import java.net.URL;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Some utilities for parsing JDBC URLs which may not be tolerated
- * by Java's java.net.URL class.
- * java.net.URL does not support multi:part:scheme:// components, which
- * virtually all JDBC connect string URLs have.
- */
-public final class JdbcUrl {
-
-  public static final Log LOG = LogFactory.getLog(JdbcUrl.class.getName());
-
-  private JdbcUrl() {
-  }
-
-  /**
-   * @return the database name from the connect string, which is typically the 'path'
-   * component, or null if we can't.
-   */
-  public static String getDatabaseName(String connectString) {
-    try {
-      String sanitizedString = null;
-      int schemeEndOffset = connectString.indexOf("://");
-      if (-1 == schemeEndOffset) {
-        // couldn't find one? try our best here.
-        sanitizedString = "http://" + connectString;
-        LOG.warn("Could not find database access scheme in connect string " + connectString);
-      } else {
-        sanitizedString = "http" + connectString.substring(schemeEndOffset);
-      }
-
-      URL connectUrl = new URL(sanitizedString);
-      String databaseName = connectUrl.getPath();
-      if (null == databaseName) {
-        return null;
-      }
-
-      // This is taken from a 'path' part of a URL, which may have leading '/'
-      // characters; trim them off.
-      while (databaseName.startsWith("/")) {
-        databaseName = databaseName.substring(1);
-      }
-
-      return databaseName;
-    } catch (MalformedURLException mue) {
-      LOG.error("Malformed connect string URL: " + connectString
-          + "; reason is " + mue.toString());
-      return null;
-    }
-  }
-
-  /**
-   * @return the hostname from the connect string, or null if we can't.
-   */
-  public static String getHostName(String connectString) {
-    try {
-      String sanitizedString = null;
-      int schemeEndOffset = connectString.indexOf("://");
-      if (-1 == schemeEndOffset) {
-        // couldn't find one? ok, then there's no problem, it should work as a URL.
-        sanitizedString = connectString;
-      } else {
-        sanitizedString = "http" + connectString.substring(schemeEndOffset);
-      }
-
-      URL connectUrl = new URL(sanitizedString);
-      return connectUrl.getHost();
-    } catch (MalformedURLException mue) {
-      LOG.error("Malformed connect string URL: " + connectString
-          + "; reason is " + mue.toString());
-      return null;
-    }
-  }
-
-  /**
-   * @return the port from the connect string, or -1 if we can't.
-   */
-  public static int getPort(String connectString) {
-    try {
-      String sanitizedString = null;
-      int schemeEndOffset = connectString.indexOf("://");
-      if (-1 == schemeEndOffset) {
-        // couldn't find one? ok, then there's no problem, it should work as a URL.
-        sanitizedString = connectString;
-      } else {
-        sanitizedString = "http" + connectString.substring(schemeEndOffset);
-      }
-
-      URL connectUrl = new URL(sanitizedString);
-      return connectUrl.getPort();
-    } catch (MalformedURLException mue) {
-      LOG.error("Malformed connect string URL: " + connectString
-          + "; reason is " + mue.toString());
-      return -1;
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/LoggingAsyncSink.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/LoggingAsyncSink.java
deleted file mode 100644
index 514b4e1..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/LoggingAsyncSink.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.BufferedReader;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * An AsyncSink that takes the contents of a stream and writes
- * it to log4j.
- */
-public class LoggingAsyncSink extends AsyncSink {
-
-  public static final Log LOG = LogFactory.getLog(LoggingAsyncSink.class.getName());
-
-  private Log contextLog;
-
-  public LoggingAsyncSink(final Log context) {
-    if (null == context) {
-      this.contextLog = LOG;
-    } else {
-      this.contextLog = context;
-    }
-  }
-
-  private Thread child;
-
-  public void processStream(InputStream is) {
-    child = new LoggingThread(is);
-    child.start();
-  }
-
-  public int join() throws InterruptedException {
-    child.join();
-    return 0; // always successful.
-  }
-
-  /**
-   * Run a background thread that copies the contents of the stream
-   * to the output context log.
-   */
-  private class LoggingThread extends Thread {
-
-    private InputStream stream;
-
-    LoggingThread(final InputStream is) {
-      this.stream = is;
-    }
-
-    public void run() {
-      InputStreamReader isr = new InputStreamReader(this.stream);
-      BufferedReader r = new BufferedReader(isr);
-
-      try {
-        while (true) {
-          String line = r.readLine();
-          if (null == line) {
-            break; // stream was closed by remote end.
-          }
-
-          LoggingAsyncSink.this.contextLog.info(line);
-        }
-      } catch (IOException ioe) {
-        LOG.error("IOException reading from stream: " + ioe.toString());
-      }
-
-      try {
-        r.close();
-      } catch (IOException ioe) {
-        LOG.warn("Error closing stream in LoggingAsyncSink: " + ioe.toString());
-      }
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/NullAsyncSink.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/NullAsyncSink.java
deleted file mode 100644
index 357dcb5..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/NullAsyncSink.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.BufferedReader;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * An AsyncSink that takes the contents of a stream and ignores it.
- */
-public class NullAsyncSink extends AsyncSink {
-
-  public static final Log LOG = LogFactory.getLog(NullAsyncSink.class.getName());
-
-  private Thread child;
-
-  public void processStream(InputStream is) {
-    child = new IgnoringThread(is);
-    child.start();
-  }
-
-  public int join() throws InterruptedException {
-    child.join();
-    return 0; // always successful.
-  }
-
-  /**
-   * Run a background thread that reads and ignores the
-   * contents of the stream.
-   */
-  private class IgnoringThread extends Thread {
-
-    private InputStream stream;
-
-    IgnoringThread(final InputStream is) {
-      this.stream = is;
-    }
-
-    public void run() {
-      InputStreamReader isr = new InputStreamReader(this.stream);
-      BufferedReader r = new BufferedReader(isr);
-
-      try {
-        while (true) {
-          String line = r.readLine();
-          if (null == line) {
-            break; // stream was closed by remote end.
-          }
-        }
-      } catch (IOException ioe) {
-        LOG.warn("IOException reading from (ignored) stream: " + ioe.toString());
-      }
-
-      try {
-        r.close();
-      } catch (IOException ioe) {
-        LOG.warn("Error closing stream in NullAsyncSink: " + ioe.toString());
-      }
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/PerfCounters.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/PerfCounters.java
deleted file mode 100644
index 979a1c6..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/PerfCounters.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.text.NumberFormat;
-
-/**
- * A quick set of performance counters for reporting import speed.
- */
-public class PerfCounters {
-
-  private long bytes;
-  private long nanoseconds;
-
-  private long startTime;
-
-  public PerfCounters() {
-  }
-
-  public void addBytes(long more) {
-    bytes += more;
-  }
-
-  public void startClock() {
-    startTime = System.nanoTime();
-  }
-
-  public void stopClock() {
-    nanoseconds = System.nanoTime() - startTime;
-  }
-
-  private static final double ONE_BILLION = 1000.0 * 1000.0 * 1000.0;
-
-  /** maximum number of digits after the decimal place */
-  private static final int MAX_PLACES = 4;
-
-  /**
-   * @return A value in nanoseconds scaled to report in seconds
-   */
-  private Double inSeconds(long nanos) {
-    return (double) nanos / ONE_BILLION;
-  }
-
-  private static final long ONE_GB = 1024 * 1024 * 1024;
-  private static final long ONE_MB = 1024 * 1024;
-  private static final long ONE_KB = 1024;
-
-
-  /**
-   * @return a string of the form "xxxx bytes" or "xxxxx KB" or "xxxx GB", scaled
-   * as is appropriate for the current value.
-   */
-  private String formatBytes() {
-    double val;
-    String scale;
-    if (bytes > ONE_GB) {
-      val = (double) bytes / (double) ONE_GB;
-      scale = "GB";
-    } else if (bytes > ONE_MB) {
-      val = (double) bytes / (double) ONE_MB;
-      scale = "MB";
-    } else if (bytes > ONE_KB) {
-      val = (double) bytes / (double) ONE_KB;
-      scale = "KB";
-    } else {
-      val = (double) bytes;
-      scale = "bytes";
-    }
-
-    NumberFormat fmt = NumberFormat.getInstance();
-    fmt.setMaximumFractionDigits(MAX_PLACES);
-    return fmt.format(val) + " " + scale;
-  }
-
-  private String formatTimeInSeconds() {
-    NumberFormat fmt = NumberFormat.getInstance();
-    fmt.setMaximumFractionDigits(MAX_PLACES);
-    return fmt.format(inSeconds(this.nanoseconds)) + " seconds";
-  }
-
-  /**
-   * @return a string of the form "xxx bytes/sec" or "xxx KB/sec" scaled as is
-   * appropriate for the current value.
-   */
-  private String formatSpeed() {
-    NumberFormat fmt = NumberFormat.getInstance();
-    fmt.setMaximumFractionDigits(MAX_PLACES);
-
-    Double seconds = inSeconds(this.nanoseconds);
-
-    double speed = (double) bytes / seconds;
-    double val;
-    String scale;
-    if (speed > ONE_GB) {
-      val = speed / (double) ONE_GB;
-      scale = "GB";
-    } else if (speed > ONE_MB) {
-      val = speed / (double) ONE_MB;
-      scale = "MB";
-    } else if (speed > ONE_KB) {
-      val = speed / (double) ONE_KB;
-      scale = "KB";
-    } else {
-      val = speed;
-      scale = "bytes";
-    }
-
-    return fmt.format(val) + " " + scale + "/sec";
-  }
-
-  public String toString() {
-    return formatBytes() + " in " + formatTimeInSeconds() + " (" + formatSpeed() + ")";
-  }
-}
-
diff --git a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ResultSetPrinter.java b/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ResultSetPrinter.java
deleted file mode 100644
index b90e326..0000000
--- a/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/util/ResultSetPrinter.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.util;
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.nio.ByteBuffer;
-import java.nio.CharBuffer;
-import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
-import java.sql.SQLException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Utility methods to format and print ResultSet objects
- * 
- *
- */
-public class ResultSetPrinter {
-
-  public static final Log LOG = LogFactory.getLog(ResultSetPrinter.class.getName());
-
-  // max output width to allocate to any column of the printed results.
-  private static final int MAX_COL_WIDTH = 20;
-
-  // length of the byte buffer, in bytes, to allocate.
-  private static final int BUFFER_LEN = 4096;
-
-  // maximum number of characters to deserialize from the stringbuilder
-  // into the byte buffer at a time. Factor of 2 off b/c of Unicode.
-  private static final int MAX_CHARS = 2048;
-
-  private ByteBuffer bytebuf;
-  private char [] charArray;
-
-  public ResultSetPrinter() {
-    bytebuf = ByteBuffer.allocate(BUFFER_LEN);
-    charArray = new char[MAX_CHARS];
-  }
-
-  /**
-   * Print 'str' to the string builder, padded to 'width' chars
-   */
-  private static void printPadded(StringBuilder sb, String str, int width) {
-    int numPad;
-    if (null == str) {
-      sb.append("(null)");
-      numPad = width - "(null)".length();
-    } else {
-      sb.append(str);
-      numPad = width - str.length();
-    }
-
-    for (int i = 0; i < numPad; i++) {
-      sb.append(' ');
-    }
-  }
-
-
-  /**
-   * Takes the contents of the StringBuilder and prints it on the OutputStream
-   */
-  private void sendToStream(StringBuilder sb, OutputStream os) throws IOException {
-
-    int pos = 0;  // current pos in the string builder
-    int len = sb.length(); // total length (in characters) to send to os.
-    CharBuffer charbuf = bytebuf.asCharBuffer();
-
-    while (pos < len) {
-      int copyLen = Math.min(sb.length(), MAX_CHARS);
-      sb.getChars(pos, copyLen, charArray, 0);
-
-      charbuf.put(charArray, 0, copyLen);
-      os.write(bytebuf.array());
-
-      pos += copyLen;
-    }
-
-  }
-
-  private static final String COL_SEPARATOR = " | ";
-
-  /**
-   * Format the contents of the ResultSet into something that could be printed
-   * neatly; the results are appended to the supplied StringBuilder.
-   */
-  public final void printResultSet(OutputStream os, ResultSet results) throws IOException {
-    try {
-      StringBuilder sbNames = new StringBuilder();
-      int cols = results.getMetaData().getColumnCount();
-
-      int [] colWidths = new int[cols];
-      ResultSetMetaData metadata = results.getMetaData();
-      for (int i = 1; i < cols + 1; i++) {
-        String colName = metadata.getColumnName(i);
-        colWidths[i - 1] = Math.min(metadata.getColumnDisplaySize(i), MAX_COL_WIDTH);
-        if (colName == null || colName.equals("")) {
-          colName = metadata.getColumnLabel(i) + "*";
-        }
-        printPadded(sbNames, colName, colWidths[i - 1]);
-        sbNames.append(COL_SEPARATOR);
-      }
-      sbNames.append('\n');
-
-      StringBuilder sbPad = new StringBuilder();
-      for (int i = 0; i < cols; i++) {
-        for (int j = 0; j < COL_SEPARATOR.length() + colWidths[i]; j++) {
-          sbPad.append('-');
-        }
-      }
-      sbPad.append('\n');
-
-      sendToStream(sbPad, os);
-      sendToStream(sbNames, os);
-      sendToStream(sbPad, os);
-
-      while (results.next())  {
-        StringBuilder sb = new StringBuilder();
-        for (int i = 1; i < cols + 1; i++) {
-          printPadded(sb, results.getString(i), colWidths[i - 1]);
-          sb.append(COL_SEPARATOR);
-        }
-        sb.append('\n');
-        sendToStream(sb, os);
-      }
-
-      sendToStream(sbPad, os);
-    } catch (SQLException sqlException) {
-      LOG.error("Error reading from database: " + sqlException.toString());
-    }
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/AllTests.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/AllTests.java
deleted file mode 100644
index 333828a..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/AllTests.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.hadoop.sqoop.mapred.MapredTests;
-
-import junit.framework.Test;
-import junit.framework.TestSuite;
-
-/**
- * All tests for Sqoop (org.apache.hadoop.sqoop)
- */
-public final class AllTests {
-
-  private AllTests() { }
-
-  public static Test suite() {
-    TestSuite suite = new TestSuite("All tests for org.apache.hadoop.sqoop");
-
-    suite.addTest(SmokeTests.suite());
-    suite.addTest(ThirdPartyTests.suite());
-    suite.addTest(MapredTests.suite());
-
-    return suite;
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/SmokeTests.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/SmokeTests.java
deleted file mode 100644
index 5f1059a..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/SmokeTests.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.hadoop.sqoop.hive.TestHiveImport;
-import org.apache.hadoop.sqoop.hive.TestTableDefWriter;
-import org.apache.hadoop.sqoop.io.TestSplittableBufferedWriter;
-import org.apache.hadoop.sqoop.lib.TestFieldFormatter;
-import org.apache.hadoop.sqoop.lib.TestRecordParser;
-import org.apache.hadoop.sqoop.manager.TestHsqldbManager;
-import org.apache.hadoop.sqoop.manager.TestSqlManager;
-import org.apache.hadoop.sqoop.mapred.MapredTests;
-import org.apache.hadoop.sqoop.mapreduce.MapreduceTests;
-import org.apache.hadoop.sqoop.orm.TestClassWriter;
-import org.apache.hadoop.sqoop.orm.TestParseMethods;
-
-import junit.framework.Test;
-import junit.framework.TestSuite;
-
-/**
- * Smoke tests for Sqoop (org.apache.hadoop.sqoop)
- */
-public final class SmokeTests {
-
-  private SmokeTests() { }
-
-  public static Test suite() {
-    TestSuite suite = new TestSuite("Smoke tests for org.apache.hadoop.sqoop");
-
-    suite.addTestSuite(TestAllTables.class);
-    suite.addTestSuite(TestHsqldbManager.class);
-    suite.addTestSuite(TestSqlManager.class);
-    suite.addTestSuite(TestClassWriter.class);
-    suite.addTestSuite(TestColumnTypes.class);
-    suite.addTestSuite(TestExport.class);
-    suite.addTestSuite(TestMultiCols.class);
-    suite.addTestSuite(TestMultiMaps.class);
-    suite.addTestSuite(TestSplitBy.class);
-    suite.addTestSuite(TestWhere.class);
-    suite.addTestSuite(TestHiveImport.class);
-    suite.addTestSuite(TestRecordParser.class);
-    suite.addTestSuite(TestFieldFormatter.class);
-    suite.addTestSuite(TestSqoopOptions.class);
-    suite.addTestSuite(TestParseMethods.class);
-    suite.addTestSuite(TestConnFactory.class);
-    suite.addTestSuite(TestSplittableBufferedWriter.class);
-    suite.addTestSuite(TestTableDefWriter.class);
-    suite.addTest(MapreduceTests.suite());
-
-    return suite;
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java
deleted file mode 100644
index 31c5167..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java
+++ /dev/null
@@ -1,136 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.sql.SQLException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.junit.Before;
-
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test the --all-tables functionality that can import multiple tables.
- */
-public class TestAllTables extends ImportJobTestCase {
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  private String [] getArgv(boolean includeHadoopFlags) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--all-tables");
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--num-mappers");
-    args.add("1");
-    args.add("--escaped-by");
-    args.add("\\");
-
-    return args.toArray(new String[0]);
-  }
-
-  /** the names of the tables we're creating. */
-  private List<String> tableNames;
-
-  /** The strings to inject in the (ordered) tables */
-  private List<String> expectedStrings;
-
-  @Before
-  public void setUp() {
-    // start the server
-    super.setUp();
-
-    // throw away TWOINTTABLE and things we don't care about.
-    try {
-      this.getTestServer().dropExistingSchema();
-    } catch (SQLException sqlE) {
-      fail(sqlE.toString());
-    }
-
-    this.tableNames = new ArrayList<String>();
-    this.expectedStrings = new ArrayList<String>();
-
-    // create two tables.
-    this.expectedStrings.add("A winner");
-    this.expectedStrings.add("is you!");
-    this.expectedStrings.add(null);
-
-    int i = 0;
-    for (String expectedStr: this.expectedStrings) {
-      String wrappedStr = null;
-      if (expectedStr != null) {
-        wrappedStr = "'" + expectedStr + "'";
-      }
-
-      String [] types = { "INT NOT NULL PRIMARY KEY", "VARCHAR(32)" };
-      String [] vals = { Integer.toString(i++) , wrappedStr };
-      this.createTableWithColTypes(types, vals);
-      this.tableNames.add(this.getTableName());
-      this.removeTableDir();
-      incrementTableNum();
-    }
-  }
-
-  public void testMultiTableImport() throws IOException {
-    String [] argv = getArgv(true);
-    runImport(argv);
-
-    Path warehousePath = new Path(this.getWarehouseDir());
-    int i = 0;
-    for (String tableName : this.tableNames) {
-      Path tablePath = new Path(warehousePath, tableName);
-      Path filePath = new Path(tablePath, "part-m-00000");
-
-      // dequeue the expected value for this table. This
-      // list has the same order as the tableNames list.
-      String expectedVal = Integer.toString(i++) + ","
-          + this.expectedStrings.get(0);
-      this.expectedStrings.remove(0);
-
-      BufferedReader reader = new BufferedReader(
-          new InputStreamReader(new FileInputStream(new File(filePath.toString()))));
-      try {
-        String line = reader.readLine();
-        assertEquals("Table " + tableName + " expected a different string",
-            expectedVal, line);
-      } finally {
-        IOUtils.closeStream(reader);
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestColumnTypes.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestColumnTypes.java
deleted file mode 100644
index 3a5e1dd..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestColumnTypes.java
+++ /dev/null
@@ -1,302 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.Test;
-
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test that each of the different SQL Column types that we support
- * can, in fact, be imported into HDFS. Test that the writable
- * that we expect to work, does.
- *
- * This requires testing:
- * - That we can pull from the database into HDFS:
- *    readFields(ResultSet), toString()
- * - That we can pull from mapper to reducer:
- *    write(DataOutput), readFields(DataInput)
- * - And optionally, that we can push to the database:
- *    write(PreparedStatement)
- */
-public class TestColumnTypes extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestColumnTypes.class.getName());
-
-  /**
-   * Do a full verification test on the singleton value of a given type.
-   * @param colType  The SQL type to instantiate the column
-   * @param insertVal The SQL text to insert a value into the database
-   * @param returnVal The string representation of the value as extracted from the db
-   */
-  private void verifyType(String colType, String insertVal, String returnVal) {
-    verifyType(colType, insertVal, returnVal, returnVal);
-  }
-
-  /**
-   * Do a full verification test on the singleton value of a given type.
-   * @param colType  The SQL type to instantiate the column
-   * @param insertVal The SQL text to insert a value into the database
-   * @param returnVal The string representation of the value as extracted from the db
-   * @param seqFileVal The string representation of the value as extracted through
-   *        the DBInputFormat, serialized, and injected into a SequenceFile and put
-   *        through toString(). This may be slightly different than what ResultSet.getString()
-   *        returns, which is used by returnVal.
-   */
-  private void verifyType(String colType, String insertVal, String returnVal, String seqFileVal) {
-    createTableForColType(colType, insertVal);
-    verifyReadback(1, returnVal);
-    verifyImport(seqFileVal, null);
-  }
-
-  static final String STRING_VAL_IN = "'this is a short string'";
-  static final String STRING_VAL_OUT = "this is a short string";
-
-  @Test
-  public void testStringCol1() {
-    verifyType("VARCHAR(32)", STRING_VAL_IN, STRING_VAL_OUT);
-  }
-
-  @Test
-  public void testStringCol2() {
-    verifyType("CHAR(32)", STRING_VAL_IN, STRING_VAL_OUT);
-  }
-
-  @Test
-  public void testEmptyStringCol() {
-    verifyType("VARCHAR(32)", "''", "");
-  }
-
-  @Test
-  public void testNullStringCol() {
-    verifyType("VARCHAR(32)", "NULL", null);
-  }
-
-  @Test
-  public void testInt() {
-    verifyType("INTEGER", "42", "42");
-  }
-
-  @Test
-  public void testNullInt() {
-    verifyType("INTEGER", "NULL", null);
-  }
-
-  @Test
-  public void testBit1() {
-    verifyType("BIT", "1", "true");
-  }
-
-  @Test
-  public void testBit2() {
-    verifyType("BIT", "0", "false");
-  }
-
-  @Test
-  public void testBit3() {
-    verifyType("BIT", "false", "false");
-  }
-
-  @Test
-  public void testTinyInt1() {
-    verifyType("TINYINT", "0", "0");
-  }
-
-  @Test
-  public void testTinyInt2() {
-    verifyType("TINYINT", "42", "42");
-  }
-
-  @Test
-  public void testSmallInt1() {
-    verifyType("SMALLINT", "-1024", "-1024");
-  }
-
-  @Test
-  public void testSmallInt2() {
-    verifyType("SMALLINT", "2048", "2048");
-  }
-
-  @Test
-  public void testBigInt1() {
-    verifyType("BIGINT", "10000000000", "10000000000");
-  }
-
-  @Test
-  public void testReal1() {
-    verifyType("REAL", "256", "256.0");
-  }
-
-  @Test
-  public void testReal2() {
-    verifyType("REAL", "256.45", "256.45");
-  }
-
-  @Test
-  public void testFloat1() {
-    verifyType("FLOAT", "256", "256.0");
-  }
-
-  @Test
-  public void testFloat2() {
-    verifyType("FLOAT", "256.45", "256.45");
-  }
-
-  @Test
-  public void testDouble1() {
-    verifyType("DOUBLE", "-256", "-256.0");
-  }
-
-  @Test
-  public void testDouble2() {
-    verifyType("DOUBLE", "256.45", "256.45");
-  }
-
-  @Test
-  public void testDate1() {
-    verifyType("DATE", "'2009-1-12'", "2009-01-12");
-  }
-
-  @Test
-  public void testDate2() {
-    verifyType("DATE", "'2009-01-12'", "2009-01-12");
-  }
-
-  @Test
-  public void testDate3() {
-    verifyType("DATE", "'2009-04-24'", "2009-04-24");
-  }
-
-  @Test
-  public void testTime1() {
-    verifyType("TIME", "'12:24:00'", "12:24:00");
-  }
-
-  @Test
-  public void testTime2() {
-    verifyType("TIME", "'06:24:00'", "06:24:00");
-  }
-
-  @Test
-  public void testTime3() {
-    verifyType("TIME", "'6:24:00'", "06:24:00");
-  }
-
-  @Test
-  public void testTime4() {
-    verifyType("TIME", "'18:24:00'", "18:24:00");
-  }
-
-  @Test
-  public void testTimestamp1() {
-    verifyType("TIMESTAMP", "'2009-04-24 18:24:00'",
-        "2009-04-24 18:24:00.000000000",
-        "2009-04-24 18:24:00.0");
-  }
-
-  @Test
-  public void testTimestamp2() {
-    try {
-    LOG.debug("Beginning testTimestamp2");
-    verifyType("TIMESTAMP", "'2009-04-24 18:24:00.0002'",
-        "2009-04-24 18:24:00.000200000",
-        "2009-04-24 18:24:00.0002");
-    } finally {
-      LOG.debug("End testTimestamp2");
-    }
-  }
-
-  @Test
-  public void testTimestamp3() {
-    try {
-    LOG.debug("Beginning testTimestamp3");
-    verifyType("TIMESTAMP", "null", null);
-    } finally {
-      LOG.debug("End testTimestamp3");
-    }
-  }
-
-  @Test
-  public void testNumeric1() {
-    verifyType("NUMERIC", "1", "1");
-  }
-
-  @Test
-  public void testNumeric2() {
-    verifyType("NUMERIC", "-10", "-10");
-  }
-
-  @Test
-  public void testNumeric3() {
-    verifyType("NUMERIC", "3.14159", "3.14159");
-  }
-
-  @Test
-  public void testNumeric4() {
-    verifyType("NUMERIC", "30000000000000000000000000.14159", "30000000000000000000000000.14159");
-  }
-
-  @Test
-  public void testNumeric5() {
-    verifyType("NUMERIC", "999999999999999999999999999999.14159", "999999999999999999999999999999.14159");
-  }
-
-  @Test
-  public void testNumeric6() {
-    verifyType("NUMERIC", "-999999999999999999999999999999.14159", "-999999999999999999999999999999.14159");
-  }
-
-  @Test
-  public void testDecimal1() {
-    verifyType("DECIMAL", "1", "1");
-  }
-
-  @Test
-  public void testDecimal2() {
-    verifyType("DECIMAL", "-10", "-10");
-  }
-
-  @Test
-  public void testDecimal3() {
-    verifyType("DECIMAL", "3.14159", "3.14159");
-  }
-
-  @Test
-  public void testDecimal4() {
-    verifyType("DECIMAL", "30000000000000000000000000.14159", "30000000000000000000000000.14159");
-  }
-
-  @Test
-  public void testDecimal5() {
-    verifyType("DECIMAL", "999999999999999999999999999999.14159", "999999999999999999999999999999.14159");
-  }
-
-  @Test
-  public void testDecimal6() {
-    verifyType("DECIMAL", "-999999999999999999999999999999.14159", "-999999999999999999999999999999.14159");
-  }
-
-  @Test
-  public void testLongVarChar() {
-    verifyType("LONGVARCHAR", "'this is a long varchar'", "this is a long varchar");
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestConnFactory.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestConnFactory.java
deleted file mode 100644
index 6af7ba3..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestConnFactory.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.ImportJobContext;
-import org.apache.hadoop.sqoop.manager.ManagerFactory;
-
-import junit.framework.TestCase;
-
-import java.io.IOException; 
-import java.util.Map;
-import java.sql.Connection;
-import java.sql.ResultSet;
-
-/**
- * Test the ConnFactory implementation and its ability to delegate to multiple
- * different ManagerFactory implementations using reflection.
- */
-public class TestConnFactory extends TestCase {
-
-  public void testCustomFactory() throws IOException {
-    Configuration conf = new Configuration();
-    conf.set(ConnFactory.FACTORY_CLASS_NAMES_KEY, AlwaysDummyFactory.class.getName());
-
-    ConnFactory factory = new ConnFactory(conf);
-    ConnManager manager = factory.getManager(new SqoopOptions());
-    assertNotNull("No manager returned", manager);
-    assertTrue("Expected a DummyManager", manager instanceof DummyManager);
-  }
-
-  public void testExceptionForNoManager() {
-    Configuration conf = new Configuration();
-    conf.set(ConnFactory.FACTORY_CLASS_NAMES_KEY, EmptyFactory.class.getName());
-
-    ConnFactory factory = new ConnFactory(conf);
-    try {
-      ConnManager manager = factory.getManager(new SqoopOptions());
-      fail("factory.getManager() expected to throw IOException");
-    } catch (IOException ioe) {
-      // Expected this. Test passes.
-    }
-  }
-
-  public void testMultipleManagers() throws IOException {
-    Configuration conf = new Configuration();
-    // The AlwaysDummyFactory is second in this list. Nevertheless, since
-    // we know the first factory in the list will return null, we should still
-    // get a DummyManager out.
-    String classNames = EmptyFactory.class.getName()
-        + "," + AlwaysDummyFactory.class.getName();
-    conf.set(ConnFactory.FACTORY_CLASS_NAMES_KEY, classNames);
-
-    ConnFactory factory = new ConnFactory(conf);
-    ConnManager manager = factory.getManager(new SqoopOptions());
-    assertNotNull("No manager returned", manager);
-    assertTrue("Expected a DummyManager", manager instanceof DummyManager);
-  }
-
-  ////// mock classes used for test cases above //////
-
-  public static class AlwaysDummyFactory extends ManagerFactory {
-    public ConnManager accept(SqoopOptions opts) {
-      // Always return a new DummyManager
-      return new DummyManager();
-    }
-  }
-
-  public static class EmptyFactory extends ManagerFactory {
-    public ConnManager accept(SqoopOptions opts) {
-      // Never instantiate a proper ConnManager;
-      return null;
-    }
-  }
-
-  /**
-   * This implementation doesn't do anything special.
-   */
-  public static class DummyManager extends ConnManager {
-    public void close() {
-    }
-
-    public String [] listDatabases() {
-      return null; 
-    }
-
-    public String [] listTables() {
-      return null;
-    }
-
-    public String [] getColumnNames(String tableName) {
-      return null;
-    }
-
-    public String getPrimaryKey(String tableName) {
-      return null;
-    }
-
-    /**
-    * Default implementation
-    * @param sqlType     sql data type
-    * @return            java data type
-    */
-    public String toJavaType(int sqlType) {
-      return null;
-    }
-
-    /**
-    * Default implementation
-    * @param sqlType     sql data type
-    * @return            hive data type
-    */
-    public String toHiveType(int sqlType) {
-      return null;
-    }
-
-    public Map<String, Integer> getColumnTypes(String tableName) {
-      return null;
-    }
-
-    public ResultSet readTable(String tableName, String [] columns) {
-      return null;
-    }
-
-    public Connection getConnection() {
-      return null;
-    }
-
-    public String getDriverClass() {
-      return null;
-    }
-
-    public void execAndPrint(String s) {
-    }
-
-    public void importTable(ImportJobContext context) {
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestExport.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestExport.java
deleted file mode 100644
index 14c8550..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestExport.java
+++ /dev/null
@@ -1,539 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionCodecFactory;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.lib.RecordParser;
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-import org.apache.hadoop.sqoop.testutil.ExportJobTestCase;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-import org.junit.Before;
-
-/**
- * Test that we can export data from HDFS into databases.
- */
-public class TestExport extends ExportJobTestCase {
-
-  @Before
-  public void setUp() {
-    // start the server
-    super.setUp();
-
-    // throw away any existing data that might be in the database.
-    try {
-      this.getTestServer().dropExistingSchema();
-    } catch (SQLException sqlE) {
-      fail(sqlE.toString());
-    }
-  }
-
-  private String getRecordLine(int recordNum, ColumnGenerator... extraCols) {
-    String idStr = Integer.toString(recordNum);
-    StringBuilder sb = new StringBuilder();
-
-    sb.append(idStr);
-    sb.append("\t");
-    sb.append(getMsgPrefix());
-    sb.append(idStr);
-    for (ColumnGenerator gen : extraCols) {
-      sb.append("\t");
-      sb.append(gen.getExportText(recordNum));
-    }
-    sb.append("\n");
-
-    return sb.toString();
-  }
-
-  /** When generating data for export tests, each column is generated
-      according to a ColumnGenerator. Methods exist for determining
-      what to put into text strings in the files to export, as well
-      as what the string representation of the column as returned by
-      the database should look like.
-    */
-  interface ColumnGenerator {
-    /** for a row with id rowNum, what should we write into that
-        line of the text file to export?
-      */
-    public String getExportText(int rowNum);
-
-    /** for a row with id rowNum, what should the database return
-        for the given column's value?
-      */
-    public String getVerifyText(int rowNum);
-
-    /** Return the column type to put in the CREATE TABLE statement */
-    public String getType();
-  }
-
-  /**
-   * Create a data file that gets exported to the db
-   * @param fileNum the number of the file (for multi-file export)
-   * @param numRecords how many records to write to the file.
-   * @param gzip is true if the file should be gzipped.
-   */
-  private void createTextFile(int fileNum, int numRecords, boolean gzip,
-      ColumnGenerator... extraCols) throws IOException {
-    int startId = fileNum * numRecords;
-
-    String ext = ".txt";
-    if (gzip) {
-      ext = ext + ".gz";
-    }
-    Path tablePath = getTablePath();
-    Path filePath = new Path(tablePath, "part" + fileNum + ext);
-
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    FileSystem fs = FileSystem.get(conf);
-    fs.mkdirs(tablePath);
-    OutputStream os = fs.create(filePath);
-    if (gzip) {
-      CompressionCodecFactory ccf = new CompressionCodecFactory(conf);
-      CompressionCodec codec = ccf.getCodec(filePath);
-      os = codec.createOutputStream(os);
-    }
-    BufferedWriter w = new BufferedWriter(new OutputStreamWriter(os));
-    for (int i = 0; i < numRecords; i++) {
-      w.write(getRecordLine(startId + i, extraCols));
-    }
-    w.close();
-    os.close();
-
-    if (gzip) {
-      verifyCompressedFile(filePath, numRecords);
-    }
-  }
-
-  private void verifyCompressedFile(Path f, int expectedNumLines) throws IOException {
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    FileSystem fs = FileSystem.get(conf);
-    InputStream is = fs.open(f);
-    CompressionCodecFactory ccf = new CompressionCodecFactory(conf);
-    CompressionCodec codec = ccf.getCodec(f);
-    LOG.info("gzip check codec is " + codec);
-    Decompressor decompressor = CodecPool.getDecompressor(codec);
-    if (null == decompressor) {
-      LOG.info("Verifying gzip sanity with null decompressor");
-    } else {
-      LOG.info("Verifying gzip sanity with decompressor: " + decompressor.toString());
-    }
-    is = codec.createInputStream(is, decompressor);
-    BufferedReader r = new BufferedReader(new InputStreamReader(is));
-    int numLines = 0;
-    while (true) {
-      String ln = r.readLine();
-      if (ln == null) {
-        break;
-      }
-      numLines++;
-    }
-
-    r.close();
-    assertEquals("Did not read back correct number of lines",
-        expectedNumLines, numLines);
-    LOG.info("gzip sanity check returned " + numLines + " lines; ok.");
-  }
-
-  /**
-   * Create a data file in SequenceFile format that gets exported to the db
-   * @param fileNum the number of the file (for multi-file export).
-   * @param numRecords how many records to write to the file.
-   * @param className the table class name to instantiate and populate
-   *          for each record.
-   */
-  private void createSequenceFile(int fileNum, int numRecords, String className)
-      throws IOException {
-
-    try {
-      // Instantiate the value record object via reflection. 
-      Class cls = Class.forName(className, true,
-          Thread.currentThread().getContextClassLoader());
-      SqoopRecord record = (SqoopRecord) ReflectionUtils.newInstance(cls, new Configuration());
-
-      // Create the SequenceFile.
-      Configuration conf = new Configuration();
-      conf.set("fs.default.name", "file:///");
-      FileSystem fs = FileSystem.get(conf);
-      Path tablePath = getTablePath();
-      Path filePath = new Path(tablePath, "part" + fileNum);
-      fs.mkdirs(tablePath);
-      SequenceFile.Writer w =
-          SequenceFile.createWriter(fs, conf, filePath, LongWritable.class, cls);
-
-      // Now write the data.
-      int startId = fileNum * numRecords;
-      for (int i = 0; i < numRecords; i++) {
-        record.parse(getRecordLine(startId + i));
-        w.append(new LongWritable(startId + i), record);
-      }
-
-      w.close();
-    } catch (ClassNotFoundException cnfe) {
-      throw new IOException(cnfe);
-    } catch (RecordParser.ParseError pe) {
-      throw new IOException(pe);
-    }
-  }
-
-  /** Return the column name for a column index.
-   *  Each table contains two columns named 'id' and 'msg', and then an
-   *  arbitrary number of additional columns defined by ColumnGenerators.
-   *  These columns are referenced by idx 0, 1, 2...
-   *  @param idx the index of the ColumnGenerator in the array passed to
-   *   createTable().
-   *  @return the name of the column
-   */
-  protected String forIdx(int idx) {
-    return "col" + idx;
-  }
-
-  /** Create the table definition to export to, removing any prior table.
-      By specifying ColumnGenerator arguments, you can add extra columns
-      to the table of arbitrary type.
-   */
-  public void createTable(ColumnGenerator... extraColumns) throws SQLException {
-    Connection conn = getTestServer().getConnection();
-    PreparedStatement statement = conn.prepareStatement(
-        "DROP TABLE " + getTableName() + " IF EXISTS",
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    statement.executeUpdate();
-    conn.commit();
-    statement.close();
-
-    StringBuilder sb = new StringBuilder();
-    sb.append("CREATE TABLE ");
-    sb.append(getTableName());
-    sb.append(" (id INT NOT NULL PRIMARY KEY, msg VARCHAR(64)");
-    int colNum = 0;
-    for (ColumnGenerator gen : extraColumns) {
-      sb.append(", " + forIdx(colNum++) + " " + gen.getType());
-    }
-    sb.append(")");
-
-    statement = conn.prepareStatement(sb.toString(),
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    statement.executeUpdate();
-    conn.commit();
-    statement.close();
-  }
-
-  /** Removing an existing table directory from the filesystem */
-  private void removeTablePath() throws IOException {
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    FileSystem fs = FileSystem.get(conf);
-    fs.delete(getTablePath(), true);
-  }
-
-  /** Verify that on a given row, a column has a given value.
-   * @param id the id column specifying the row to test.
-   */
-  private void assertColValForRowId(int id, String colName, String expectedVal)
-      throws SQLException {
-    Connection conn = getTestServer().getConnection();
-    LOG.info("Verifying column " + colName + " has value " + expectedVal);
-
-    PreparedStatement statement = conn.prepareStatement(
-        "SELECT " + colName + " FROM " + getTableName() + " WHERE id = " + id,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    ResultSet rs = statement.executeQuery();
-    rs.next();
-
-    String actualVal = rs.getString(1);
-    rs.close();
-    statement.close();
-
-    assertEquals("Got unexpected column value", expectedVal, actualVal);
-  }
-
-  /** Verify that for the max and min values of the 'id' column, the values
-      for a given column meet the expected values.
-   */
-  private void assertColMinAndMax(String colName, ColumnGenerator generator)
-      throws SQLException {
-    int minId = getMinRowId();
-    int maxId = getMaxRowId();
-
-    LOG.info("Checking min/max for column " + colName + " with type " + generator.getType());
-
-    String expectedMin = generator.getVerifyText(minId);
-    String expectedMax = generator.getVerifyText(maxId);
-
-    assertColValForRowId(minId, colName, expectedMin);
-    assertColValForRowId(maxId, colName, expectedMax);
-  }
-
-  /** Export 10 rows, make sure they load in correctly */
-  public void testTextExport() throws IOException, SQLException {
-
-    final int TOTAL_RECORDS = 10;
-
-    createTextFile(0, TOTAL_RECORDS, false);
-    createTable();
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-  }
-
-  /** Export 10 rows from gzipped text files. */
-  public void testGzipExport() throws IOException, SQLException {
-
-    LOG.info("Beginning gzip export test");
-
-    final int TOTAL_RECORDS = 10;
-
-    createTextFile(0, TOTAL_RECORDS, true);
-    createTable();
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-    LOG.info("Complete gzip export test");
-  }
-
-  /** Run 2 mappers, make sure all records load in correctly */
-  public void testMultiMapTextExport() throws IOException, SQLException {
-
-    final int RECORDS_PER_MAP = 10;
-    final int NUM_FILES = 2;
-
-    for (int f = 0; f < NUM_FILES; f++) {
-      createTextFile(f, RECORDS_PER_MAP, false);
-    }
-
-    createTable();
-    runExport(getArgv(true));
-    verifyExport(RECORDS_PER_MAP * NUM_FILES);
-  }
-
-
-  /** Export some rows from a SequenceFile, make sure they import correctly */
-  public void testSequenceFileExport() throws IOException, SQLException {
-
-    final int TOTAL_RECORDS = 10;
-
-    // First, generate class and jar files that represent the table we're exporting to.
-    LOG.info("Creating initial schema for SeqFile test");
-    createTable();
-    LOG.info("Generating code..."); 
-    List<String> generatedJars = runExport(getArgv(true, "--generate-only"));
-
-    // Now, wipe the created table so we can export on top of it again.
-    LOG.info("Resetting schema and data...");
-    createTable();
-
-    // Wipe the directory we use when creating files to export to ensure
-    // it's ready for new SequenceFiles.
-    removeTablePath();
-
-    assertNotNull(generatedJars);
-    assertEquals("Expected 1 generated jar file", 1, generatedJars.size());
-    String jarFileName = generatedJars.get(0);
-    // Sqoop generates jars named "foo.jar"; by default, this should contain a
-    // class named 'foo'. Extract the class name.
-    Path jarPath = new Path(jarFileName);
-    String jarBaseName = jarPath.getName();
-    assertTrue(jarBaseName.endsWith(".jar"));
-    assertTrue(jarBaseName.length() > ".jar".length());
-    String className = jarBaseName.substring(0, jarBaseName.length() - ".jar".length());
-
-    LOG.info("Using jar filename: " + jarFileName);
-    LOG.info("Using class name: " + className);
-
-    ClassLoader prevClassLoader = null;
-
-    try {
-      if (null != jarFileName) {
-        prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, className);
-      }
-
-      // Now use this class and jar name to create a sequence file.
-      LOG.info("Writing data to SequenceFiles");
-      createSequenceFile(0, TOTAL_RECORDS, className);
-
-      // Now run and verify the export.
-      LOG.info("Exporting SequenceFile-based data");
-      runExport(getArgv(true, "--class-name", className, "--jar-file", jarFileName));
-      verifyExport(TOTAL_RECORDS);
-    } finally {
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  public void testIntCol() throws IOException, SQLException {
-    final int TOTAL_RECORDS = 10;
-
-    // generate a column equivalent to rownum.
-    ColumnGenerator gen = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        return "" + rowNum;
-      }
-      public String getVerifyText(int rowNum) {
-        return "" + rowNum;
-      }
-      public String getType() {
-        return "INTEGER";
-      }
-    };
-
-    createTextFile(0, TOTAL_RECORDS, false, gen);
-    createTable(gen);
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-    assertColMinAndMax(forIdx(0), gen);
-  }
-
-  public void testBigIntCol() throws IOException, SQLException {
-    final int TOTAL_RECORDS = 10;
-
-    // generate a column that won't fit in a normal int.
-    ColumnGenerator gen = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        long val = (long) rowNum * 1000000000;
-        return "" + val;
-      }
-      public String getVerifyText(int rowNum) {
-        long val = (long) rowNum * 1000000000;
-        return "" + val;
-      }
-      public String getType() {
-        return "BIGINT";
-      }
-    };
-
-    createTextFile(0, TOTAL_RECORDS, false, gen);
-    createTable(gen);
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-    assertColMinAndMax(forIdx(0), gen);
-  }
-
-  private String pad(int n) {
-    if (n <= 9) {
-      return "0" + n;
-    } else {
-      return String.valueOf(n);
-    }
-  }
-
-  public void testDatesAndTimes() throws IOException, SQLException {
-    final int TOTAL_RECORDS = 10;
-
-    ColumnGenerator genDate = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        int day = rowNum + 1;
-        return "2009-10-" + day;
-      }
-      public String getVerifyText(int rowNum) {
-        int day = rowNum + 1;
-        return "2009-10-" + pad(day);
-      }
-      public String getType() {
-        return "DATE";
-      }
-    };
-
-    ColumnGenerator genTime = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        return "10:01:" + rowNum;
-      }
-      public String getVerifyText(int rowNum) {
-        return "10:01:" + pad(rowNum);
-      }
-      public String getType() {
-        return "TIME";
-      }
-    };
-
-    createTextFile(0, TOTAL_RECORDS, false, genDate, genTime);
-    createTable(genDate, genTime);
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-    assertColMinAndMax(forIdx(0), genDate);
-    assertColMinAndMax(forIdx(1), genTime);
-  }
-
-  public void testNumericTypes() throws IOException, SQLException {
-    final int TOTAL_RECORDS = 10;
-
-    // Check floating point values
-    ColumnGenerator genFloat = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        double v = 3.141 * (double) rowNum;
-        return "" + v;
-      }
-      public String getVerifyText(int rowNum) {
-        double v = 3.141 * (double) rowNum;
-        return "" + v;
-      }
-      public String getType() {
-        return "FLOAT";
-      }
-    };
-
-    // Check precise decimal placement. The first of ten
-    // rows will be 2.7181; the last of ten rows will be
-    // 2.71810.
-    ColumnGenerator genNumeric = new ColumnGenerator() {
-      public String getExportText(int rowNum) {
-        int digit = rowNum + 1;
-        return "2.718" + digit;
-      }
-      public String getVerifyText(int rowNum) {
-        int digit = rowNum + 1;
-        return "2.718" + digit;
-      }
-      public String getType() {
-        return "NUMERIC";
-      }
-    };
-
-    createTextFile(0, TOTAL_RECORDS, false, genFloat, genNumeric);
-    createTable(genFloat, genNumeric);
-    runExport(getArgv(true));
-    verifyExport(TOTAL_RECORDS);
-    assertColMinAndMax(forIdx(0), genFloat);
-    assertColMinAndMax(forIdx(1), genNumeric);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiCols.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiCols.java
deleted file mode 100644
index aee4153..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiCols.java
+++ /dev/null
@@ -1,212 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test cases that import rows containing multiple columns,
- * some of which may contain null values.
- *
- * Also test loading only selected columns from the db.
- */
-public class TestMultiCols extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestMultiCols.class.getName());
-
-  /**
-   * Do a full import verification test on a table containing one row
-   * @param types the types of the columns to insert
-   * @param insertVals the SQL text to use to insert each value
-   * @param validateVals the text to expect when retrieving each value from
-   * the db
-   * @param validateLine the text to expect as a toString() of the entire row,
-   * as imported by the tool
-   * @param importColumns The list of columns to import
-   */
-  private void verifyTypes(String [] types , String [] insertVals,
-      String validateVals [], String validateLine) {
-    verifyTypes(types, insertVals, validateVals, validateLine, null);
-  }
-
-  private void verifyTypes(String [] types , String [] insertVals,
-      String validateVals [], String validateLine, String [] importColumns) {
-
-    createTableWithColTypes(types, insertVals);
-
-    int i = 0;
-    for (String val : validateVals) {
-      verifyReadback(++i, val);
-      LOG.debug("Verified column " + i + " as value: " + val);
-    }
-
-    verifyImport(validateLine, importColumns);
-    LOG.debug("Verified input line as " + validateLine + " -- ok!");
-  }
-
-  public void testThreeStrings() {
-    String [] types = { "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)" };
-    String [] insertVals = { "'foo'", "'bar'", "'baz'" };
-    String [] validateVals = { "foo", "bar", "baz" };
-    String validateLine = "foo,bar,baz";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testStringsWithNull1() {
-    String [] types = { "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)" };
-    String [] insertVals = { "'foo'", "null", "'baz'" };
-    String [] validateVals = { "foo", null, "baz" };
-    String validateLine = "foo,null,baz";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testStringsWithNull2() {
-    String [] types = { "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)" };
-    String [] insertVals = { "null", "'foo'", "'baz'" };
-    String [] validateVals = { null, "foo", "baz" };
-    String validateLine = "null,foo,baz";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testStringsWithNull3() {
-    String [] types = { "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)" };
-    String [] insertVals = { "'foo'", "'baz'", "null"};
-    String [] validateVals = { "foo", "baz", null };
-    String validateLine = "foo,baz,null";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testThreeInts() {
-    String [] types = { "INTEGER", "INTEGER", "INTEGER" };
-    String [] insertVals = { "1", "2", "3" };
-    String [] validateVals = { "1", "2", "3" };
-    String validateLine = "1,2,3";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testIntsWithNulls() {
-    String [] types = { "INTEGER", "INTEGER", "INTEGER" };
-    String [] insertVals = { "1", "null", "3" };
-    String [] validateVals = { "1", null, "3" };
-    String validateLine = "1,null,3";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed1() {
-    String [] types = { "INTEGER", "VARCHAR(32)", "DATE" };
-    String [] insertVals = { "1", "'meep'", "'2009-12-31'" };
-    String [] validateVals = { "1", "meep", "2009-12-31" };
-    String validateLine = "1,meep,2009-12-31";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed2() {
-    String [] types = { "INTEGER", "VARCHAR(32)", "DATE" };
-    String [] insertVals = { "null", "'meep'", "'2009-12-31'" };
-    String [] validateVals = { null, "meep", "2009-12-31" };
-    String validateLine = "null,meep,2009-12-31";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed3() {
-    String [] types = { "INTEGER", "VARCHAR(32)", "DATE" };
-    String [] insertVals = { "1", "'meep'", "null" };
-    String [] validateVals = { "1", "meep", null };
-    String validateLine = "1,meep,null";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed4() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "-42", "17", "33333333333333333333333.1714" };
-    String [] validateVals = { "-42", "17", "33333333333333333333333.1714" };
-    String validateLine = "-42,17,33333333333333333333333.1714";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed5() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "null", "17", "33333333333333333333333.0" };
-    String [] validateVals = { null, "17", "33333333333333333333333.0" };
-    String validateLine = "null,17,33333333333333333333333.0";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  public void testMixed6() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "33333333333333333333333", "17", "-42"};
-    String [] validateVals = { "33333333333333333333333", "17", "-42" };
-    String validateLine = "33333333333333333333333,17,-42";
-
-    verifyTypes(types, insertVals, validateVals, validateLine);
-  }
-
-  //////////////////////////////////////////////////////////////////////////
-  // the tests below here test the --columns parameter and ensure that
-  // we can selectively import only certain columns.
-  //////////////////////////////////////////////////////////////////////////
-
-  public void testSkipFirstCol() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "33333333333333333333333", "17", "-42"};
-    String [] validateVals = { "33333333333333333333333", "17", "-42" };
-    String validateLine = "17,-42";
-
-    String [] loadCols = {"DATA_COL1", "DATA_COL2"};
-
-    verifyTypes(types, insertVals, validateVals, validateLine, loadCols);
-  }
-
-  public void testSkipSecondCol() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "33333333333333333333333", "17", "-42"};
-    String [] validateVals = { "33333333333333333333333", "17", "-42" };
-    String validateLine = "33333333333333333333333,-42";
-
-    String [] loadCols = {"DATA_COL0", "DATA_COL2"};
-
-    verifyTypes(types, insertVals, validateVals, validateLine, loadCols);
-  }
-
-  public void testSkipThirdCol() {
-    String [] types = { "NUMERIC", "INTEGER", "NUMERIC" };
-    String [] insertVals = { "33333333333333333333333", "17", "-42"};
-    String [] validateVals = { "33333333333333333333333", "17", "-42" };
-    String validateLine = "33333333333333333333333,17";
-
-    String [] loadCols = {"DATA_COL0", "DATA_COL1"};
-
-    verifyTypes(types, insertVals, validateVals, validateLine, loadCols);
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiMaps.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiMaps.java
deleted file mode 100644
index b9a2974..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestMultiMaps.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.testutil.SeqFileReader;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-/**
- * Test that using multiple mapper splits works.
- */
-public class TestMultiMaps extends ImportJobTestCase {
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  private String [] getArgv(boolean includeHadoopFlags, String [] colNames, String splitByCol) {
-    String columnsString = "";
-    for (String col : colNames) {
-      columnsString += col + ",";
-    }
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(HsqldbTestServer.getTableName());
-    args.add("--columns");
-    args.add(columnsString);
-    args.add("--split-by");
-    args.add(splitByCol);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--as-sequencefile");
-    args.add("--num-mappers");
-    args.add("2");
-
-    return args.toArray(new String[0]);
-  }
-
-  // this test just uses the two int table.
-  protected String getTableName() {
-    return HsqldbTestServer.getTableName();
-  }
-
-  /** @return a list of Path objects for each data file */
-  protected List<Path> getDataFilePaths() throws IOException {
-    List<Path> paths = new ArrayList<Path>();
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    FileSystem fs = FileSystem.get(conf);
-
-    FileStatus [] stats = fs.listStatus(getTablePath());
-    for (FileStatus stat : stats) {
-      paths.add(stat.getPath());
-    }
-
-    return paths;
-  }
-
-  /**
-   * Given a comma-delimited list of integers, grab and parse the first int
-   * @param str a comma-delimited list of values, the first of which is an int.
-   * @return the first field in the string, cast to int
-   */
-  private int getFirstInt(String str) {
-    String [] parts = str.split(",");
-    return Integer.parseInt(parts[0]);
-  }
-
-  public void runMultiMapTest(String splitByCol, int expectedSum)
-      throws IOException {
-
-    String [] columns = HsqldbTestServer.getFieldNames();
-    ClassLoader prevClassLoader = null;
-    SequenceFile.Reader reader = null;
-
-    String [] argv = getArgv(true, columns, splitByCol);
-    runImport(argv);
-    try {
-      SqoopOptions opts = new SqoopOptions();
-      opts.parse(getArgv(false, columns, splitByCol));
-
-      CompilationManager compileMgr = new CompilationManager(opts);
-      String jarFileName = compileMgr.getJarFilename();
-
-      prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, getTableName());
-
-      List<Path> paths = getDataFilePaths();
-      Configuration conf = new Configuration();
-      int curSum = 0;
-
-      assertTrue("Found only " + paths.size() + " path(s); expected > 1.", paths.size() > 1);
-
-      // We expect multiple files. We need to open all the files and sum up the
-      // first column across all of them.
-      for (Path p : paths) {
-        reader = SeqFileReader.getSeqFileReader(p.toString());
-
-        // here we can actually instantiate (k, v) pairs.
-        Object key = ReflectionUtils.newInstance(reader.getKeyClass(), conf);
-        Object val = ReflectionUtils.newInstance(reader.getValueClass(), conf);
-
-        // We know that these values are two ints separated by a ',' character.
-        // Since this is all dynamic, though, we don't want to actually link against
-        // the class and use its methods. So we just parse this back into int fields manually.
-        // Sum them up and ensure that we get the expected total for the first column, to
-        // verify that we got all the results from the db into the file.
-
-        // now sum up everything in the file.
-        while (reader.next(key) != null) {
-          reader.getCurrentValue(val);
-          curSum += getFirstInt(val.toString());
-        }
-
-        IOUtils.closeStream(reader);
-        reader = null;
-      }
-
-      assertEquals("Total sum of first db column mismatch", expectedSum, curSum);
-    } catch (InvalidOptionsException ioe) {
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(reader);
-
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  public void testSplitByFirstCol() throws IOException {
-    runMultiMapTest("INTFIELD1", HsqldbTestServer.getFirstColSum());
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSplitBy.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSplitBy.java
deleted file mode 100644
index 842d59d..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSplitBy.java
+++ /dev/null
@@ -1,150 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.testutil.SeqFileReader;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-/**
- * Test that --split-by works
- */
-public class TestSplitBy extends ImportJobTestCase {
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  private String [] getArgv(boolean includeHadoopFlags, String [] colNames, String splitByCol) {
-    String columnsString = "";
-    for (String col : colNames) {
-      columnsString += col + ",";
-    }
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(HsqldbTestServer.getTableName());
-    args.add("--columns");
-    args.add(columnsString);
-    args.add("--split-by");
-    args.add(splitByCol);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--as-sequencefile");
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  // this test just uses the two int table.
-  protected String getTableName() {
-    return HsqldbTestServer.getTableName();
-  }
-
-
-  /**
-   * Given a comma-delimited list of integers, grab and parse the first int
-   * @param str a comma-delimited list of values, the first of which is an int.
-   * @return the first field in the string, cast to int
-   */
-  private int getFirstInt(String str) {
-    String [] parts = str.split(",");
-    return Integer.parseInt(parts[0]);
-  }
-
-  public void runSplitByTest(String splitByCol, int expectedSum)
-      throws IOException {
-
-    String [] columns = HsqldbTestServer.getFieldNames();
-    ClassLoader prevClassLoader = null;
-    SequenceFile.Reader reader = null;
-
-    String [] argv = getArgv(true, columns, splitByCol);
-    runImport(argv);
-    try {
-      SqoopOptions opts = new SqoopOptions();
-      opts.parse(getArgv(false, columns, splitByCol));
-
-      CompilationManager compileMgr = new CompilationManager(opts);
-      String jarFileName = compileMgr.getJarFilename();
-
-      prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, getTableName());
-
-      reader = SeqFileReader.getSeqFileReader(getDataFilePath().toString());
-
-      // here we can actually instantiate (k, v) pairs.
-      Configuration conf = new Configuration();
-      Object key = ReflectionUtils.newInstance(reader.getKeyClass(), conf);
-      Object val = ReflectionUtils.newInstance(reader.getValueClass(), conf);
-
-      // We know that these values are two ints separated by a ',' character.
-      // Since this is all dynamic, though, we don't want to actually link against
-      // the class and use its methods. So we just parse this back into int fields manually.
-      // Sum them up and ensure that we get the expected total for the first column, to
-      // verify that we got all the results from the db into the file.
-
-      // Sum up everything in the file.
-      int curSum = 0;
-      while (reader.next(key) != null) {
-        reader.getCurrentValue(val);
-        curSum += getFirstInt(val.toString());
-      }
-
-      assertEquals("Total sum of first db column mismatch", expectedSum, curSum);
-    } catch (InvalidOptionsException ioe) {
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(reader);
-
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  public void testSplitByFirstCol() throws IOException {
-    String splitByCol = "INTFIELD1";
-    runSplitByTest(splitByCol, HsqldbTestServer.getFirstColSum());
-  }
-
-  public void testSplitBySecondCol() throws IOException {
-    String splitByCol = "INTFIELD2";
-    runSplitByTest(splitByCol, HsqldbTestServer.getFirstColSum());
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSqoopOptions.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSqoopOptions.java
deleted file mode 100644
index 4f42455..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestSqoopOptions.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import junit.framework.TestCase;
-
-
-/**
- * Test aspects of the SqoopOptions class
- */
-public class TestSqoopOptions extends TestCase {
-
-  // tests for the toChar() parser
-  public void testNormalChar() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('a', SqoopOptions.toChar("a"));
-  }
-
-  public void testEmptyString() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar("");
-      fail("Expected exception");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expect this.
-    }
-  }
-
-  public void testNullString() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar(null);
-      fail("Expected exception");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expect this.
-    }
-  }
-
-  public void testTooLong() throws SqoopOptions.InvalidOptionsException {
-    // Should just use the first character and log a warning.
-    assertEquals('x', SqoopOptions.toChar("xyz"));
-  }
-
-  public void testHexChar1() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(0xF, SqoopOptions.toChar("\\0xf"));
-  }
-
-  public void testHexChar2() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(0xF, SqoopOptions.toChar("\\0xF"));
-  }
-
-  public void testHexChar3() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(0xF0, SqoopOptions.toChar("\\0xf0"));
-  }
-
-  public void testHexChar4() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(0xF0, SqoopOptions.toChar("\\0Xf0"));
-  }
-
-  public void testEscapeChar1() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\n', SqoopOptions.toChar("\\n"));
-  }
-
-  public void testEscapeChar2() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\\', SqoopOptions.toChar("\\\\"));
-  }
-
-  public void testEscapeChar3() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\\', SqoopOptions.toChar("\\"));
-  }
-
-  public void testUnknownEscape1() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar("\\Q");
-      fail("Expected exception");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expect this.
-    }
-  }
-
-  public void testUnknownEscape2() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar("\\nn");
-      fail("Expected exception");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expect this.
-    }
-  }
-
-  public void testEscapeNul1() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\000', SqoopOptions.toChar("\\0"));
-  }
-
-  public void testEscapeNul2() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\000', SqoopOptions.toChar("\\00"));
-  }
-
-  public void testEscapeNul3() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\000', SqoopOptions.toChar("\\0000"));
-  }
-
-  public void testEscapeNul4() throws SqoopOptions.InvalidOptionsException {
-    assertEquals('\000', SqoopOptions.toChar("\\0x0"));
-  }
-
-  public void testOctalChar1() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(04, SqoopOptions.toChar("\\04"));
-  }
-
-  public void testOctalChar2() throws SqoopOptions.InvalidOptionsException {
-    assertEquals(045, SqoopOptions.toChar("\\045"));
-  }
-
-  public void testErrOctalChar() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar("\\095");
-      fail("Expected exception");
-    } catch (NumberFormatException nfe) {
-      // expected.
-    }
-  }
-
-  public void testErrHexChar() throws SqoopOptions.InvalidOptionsException {
-    try {
-      SqoopOptions.toChar("\\0x9K5");
-      fail("Expected exception");
-    } catch (NumberFormatException nfe) {
-      // expected.
-    }
-  }
-
-  // test that setting output delimiters also sets input delimiters 
-  public void testDelimitersInherit() throws SqoopOptions.InvalidOptionsException {
-    String [] args = {
-        "--fields-terminated-by",
-        "|"
-    };
-
-    SqoopOptions opts = new SqoopOptions();
-    opts.parse(args);
-    assertEquals('|', opts.getInputFieldDelim());
-    assertEquals('|', opts.getOutputFieldDelim());
-  }
-
-  // test that setting output delimiters and setting input delims separately works
-  public void testDelimOverride1() throws SqoopOptions.InvalidOptionsException {
-    String [] args = {
-        "--fields-terminated-by",
-        "|",
-        "--input-fields-terminated-by",
-        "*"
-    };
-
-    SqoopOptions opts = new SqoopOptions();
-    opts.parse(args);
-    assertEquals('*', opts.getInputFieldDelim());
-    assertEquals('|', opts.getOutputFieldDelim());
-  }
-
-  // test that the order in which delims are specified doesn't matter
-  public void testDelimOverride2() throws SqoopOptions.InvalidOptionsException {
-    String [] args = {
-        "--input-fields-terminated-by",
-        "*",
-        "--fields-terminated-by",
-        "|"
-    };
-
-    SqoopOptions opts = new SqoopOptions();
-    opts.parse(args);
-    assertEquals('*', opts.getInputFieldDelim());
-    assertEquals('|', opts.getOutputFieldDelim());
-  }
-
-  public void testBadNumMappers1() {
-    String [] args = {
-      "--num-mappers",
-      "x"
-    };
-
-    try {
-      SqoopOptions opts = new SqoopOptions();
-      opts.parse(args);
-      fail("Expected InvalidOptionsException");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expected.
-    }
-  }
-
-  public void testBadNumMappers2() {
-    String [] args = {
-      "-m",
-      "x"
-    };
-
-    try {
-      SqoopOptions opts = new SqoopOptions();
-      opts.parse(args);
-      fail("Expected InvalidOptionsException");
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      // expected.
-    }
-  }
-
-  public void testGoodNumMappers() throws SqoopOptions.InvalidOptionsException {
-    String [] args = {
-      "-m",
-      "4"
-    };
-
-    SqoopOptions opts = new SqoopOptions();
-    opts.parse(args);
-    assertEquals(4, opts.getNumMappers());
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestWhere.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestWhere.java
deleted file mode 100644
index 8f5d700..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestWhere.java
+++ /dev/null
@@ -1,166 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.testutil.SeqFileReader;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-/**
- * Test that --where works in Sqoop.
- * Methods essentially copied out of the other Test* classes.
- * TODO(kevin or aaron): Factor out these common test methods
- * so that every new Test* class doesn't need to copy the code.
- */
-public class TestWhere extends ImportJobTestCase {
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  private String [] getArgv(boolean includeHadoopFlags, String [] colNames, String whereClause) {
-    String columnsString = "";
-    for (String col : colNames) {
-      columnsString += col + ",";
-    }
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(HsqldbTestServer.getTableName());
-    args.add("--columns");
-    args.add(columnsString);
-    args.add("--where");
-    args.add(whereClause);
-    args.add("--split-by");
-    args.add("INTFIELD1");
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--as-sequencefile");
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  // this test just uses the two int table.
-  protected String getTableName() {
-    return HsqldbTestServer.getTableName();
-  }
-
-
-  /**
-   * Given a comma-delimited list of integers, grab and parse the first int
-   * @param str a comma-delimited list of values, the first of which is an int.
-   * @return the first field in the string, cast to int
-   */
-  private int getFirstInt(String str) {
-    String [] parts = str.split(",");
-    return Integer.parseInt(parts[0]);
-  }
-
-  public void runWhereTest(String whereClause, String firstValStr, int numExpectedResults,
-      int expectedSum) throws IOException {
-
-    String [] columns = HsqldbTestServer.getFieldNames();
-    ClassLoader prevClassLoader = null;
-    SequenceFile.Reader reader = null;
-
-    String [] argv = getArgv(true, columns, whereClause);
-    runImport(argv);
-    try {
-      SqoopOptions opts = new SqoopOptions();
-      opts.parse(getArgv(false, columns, whereClause));
-
-      CompilationManager compileMgr = new CompilationManager(opts);
-      String jarFileName = compileMgr.getJarFilename();
-
-      prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, getTableName());
-
-      reader = SeqFileReader.getSeqFileReader(getDataFilePath().toString());
-
-      // here we can actually instantiate (k, v) pairs.
-      Configuration conf = new Configuration();
-      Object key = ReflectionUtils.newInstance(reader.getKeyClass(), conf);
-      Object val = ReflectionUtils.newInstance(reader.getValueClass(), conf);
-
-      if (reader.next(key) == null) {
-        fail("Empty SequenceFile during import");
-      }
-
-      // make sure that the value we think should be at the top, is.
-      reader.getCurrentValue(val);
-      assertEquals("Invalid ordering within sorted SeqFile", firstValStr, val.toString());
-
-      // We know that these values are two ints separated by a ',' character.
-      // Since this is all dynamic, though, we don't want to actually link against
-      // the class and use its methods. So we just parse this back into int fields manually.
-      // Sum them up and ensure that we get the expected total for the first column, to
-      // verify that we got all the results from the db into the file.
-      int curSum = getFirstInt(val.toString());
-      int totalResults = 1;
-
-      // now sum up everything else in the file.
-      while (reader.next(key) != null) {
-        reader.getCurrentValue(val);
-        curSum += getFirstInt(val.toString());
-        totalResults++;
-      }
-
-      assertEquals("Total sum of first db column mismatch", expectedSum, curSum);
-      assertEquals("Incorrect number of results for query", numExpectedResults, totalResults);
-    } catch (InvalidOptionsException ioe) {
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(reader);
-
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  public void testSingleClauseWhere() throws IOException {
-    String whereClause = "INTFIELD2 > 4";
-    runWhereTest(whereClause, "1,8\n", 2, 4);
-  }
-
-  public void testMultiClauseWhere() throws IOException {
-    String whereClause = "INTFIELD1 > 4 AND INTFIELD2 < 3";
-    runWhereTest(whereClause, "7,2\n", 1, 7);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/ThirdPartyTests.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/ThirdPartyTests.java
deleted file mode 100644
index fb645a8..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/ThirdPartyTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop;
-
-import junit.framework.Test;
-import junit.framework.TestCase;
-import junit.framework.TestSuite;
-
-import org.apache.hadoop.sqoop.manager.LocalMySQLTest;
-import org.apache.hadoop.sqoop.manager.MySQLAuthTest;
-import org.apache.hadoop.sqoop.manager.OracleManagerTest;
-import org.apache.hadoop.sqoop.manager.PostgresqlTest;
-
-/**
- * Test battery including all tests of vendor-specific ConnManager implementations.
- * These tests likely aren't run by Apache Hudson, because they require configuring
- * and using Oracle, MySQL, etc., which may have incompatible licenses with Apache.
- */
-public final class ThirdPartyTests extends TestCase {
-
-  private ThirdPartyTests() { }
-
-  public static Test suite() {
-    TestSuite suite = new TestSuite("Tests vendor-specific ConnManager "
-      + "implementations in Sqoop");
-    suite.addTestSuite(LocalMySQLTest.class);
-    suite.addTestSuite(MySQLAuthTest.class);
-    suite.addTestSuite(OracleManagerTest.class);
-    suite.addTestSuite(PostgresqlTest.class);
-
-    return suite;
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestHiveImport.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestHiveImport.java
deleted file mode 100644
index 9ce05e6..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestHiveImport.java
+++ /dev/null
@@ -1,172 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.hive;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.Path;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test HiveImport capability after an import to HDFS.
- */
-public class TestHiveImport extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestHiveImport.class.getName());
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  protected String [] getArgv(boolean includeHadoopFlags, String [] moreArgs) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(getTableName());
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--hive-import");
-    args.add("--split-by");
-    args.add(getColNames()[0]);
-    args.add("--num-mappers");
-    args.add("1");
-
-    if (null != moreArgs) {
-      for (String arg: moreArgs) {
-        args.add(arg);
-      }
-    }
-
-    return args.toArray(new String[0]);
-  }
-
-  private SqoopOptions getSqoopOptions(String [] extraArgs) {
-    SqoopOptions opts = new SqoopOptions();
-    try {
-      opts.parse(getArgv(false, extraArgs));
-    } catch (SqoopOptions.InvalidOptionsException ioe) {
-      fail("Invalid options: " + ioe.toString());
-    }
-
-    return opts;
-  }
-
-  private void runImportTest(String tableName, String [] types, String [] values,
-      String verificationScript, String [] extraArgs) throws IOException {
-
-    // create a table and populate it with a row...
-    setCurTableName(tableName);
-    createTableWithColTypes(types, values);
-    
-    // set up our mock hive shell to compare our generated script
-    // against the correct expected one.
-    SqoopOptions options = getSqoopOptions(extraArgs);
-    String hiveHome = options.getHiveHome();
-    assertNotNull("hive.home was not set", hiveHome);
-    Path testDataPath = new Path(new Path(hiveHome), "scripts/" + verificationScript);
-    System.setProperty("expected.script", testDataPath.toString());
-
-    // verify that we can import it correctly into hive.
-    runImport(getArgv(true, extraArgs));
-  }
-
-  /** Test that strings and ints are handled in the normal fashion */
-  @Test
-  public void testNormalHiveImport() throws IOException {
-    String [] types = { "VARCHAR(32)", "INTEGER", "CHAR(64)" };
-    String [] vals = { "'test'", "42", "'somestring'" };
-    runImportTest("NORMAL_HIVE_IMPORT", types, vals, "normalImport.q", null);
-  }
-
-  /** Test that table is created in hive with no data import */
-  @Test
-  public void testCreateOnlyHiveImport() throws IOException {
-    String [] types = { "VARCHAR(32)", "INTEGER", "CHAR(64)" };
-    String [] vals = { "'test'", "42", "'somestring'" };
-    String [] extraArgs = {"--hive-create-only"};
-    runImportTest("CREATE_ONLY_HIVE_IMPORT", types, vals, "createOnlyImport.q", extraArgs);
-  }
-
-  /** Test that table is created in hive and replaces the existing table if any */
-  @Test
-  public void testCreateOverwriteHiveImport() throws IOException {
-    String [] types = { "VARCHAR(32)", "INTEGER", "CHAR(64)" };
-    String [] vals = { "'test'", "42", "'somestring'" };
-    String [] extraArgs = {"--hive-create-only", "--hive-overwrite"};
-    runImportTest("CREATE_OVERWRITE_HIVE_IMPORT", types, vals, "createOverwriteImport.q", extraArgs);
-  }
-
-  /** Test that dates are coerced properly to strings */
-  @Test
-  public void testDate() throws IOException {
-    String [] types = { "VARCHAR(32)", "DATE" };
-    String [] vals = { "'test'", "'2009-05-12'" };
-    runImportTest("DATE_HIVE_IMPORT", types, vals, "dateImport.q", null);
-  }
-
-  /** Test that NUMERICs are coerced to doubles */
-  @Test
-  public void testNumeric() throws IOException {
-    String [] types = { "NUMERIC", "CHAR(64)" };
-    String [] vals = { "3.14159", "'foo'" };
-    runImportTest("NUMERIC_HIVE_IMPORT", types, vals, "numericImport.q", null);
-  }
-
-  /** If bin/hive returns an error exit status, we should get an IOException */
-  @Test
-  public void testHiveExitFails() {
-    // The expected script is different than the one which would be generated
-    // by this, so we expect an IOException out.
-    String [] types = { "NUMERIC", "CHAR(64)" };
-    String [] vals = { "3.14159", "'foo'" };
-    try {
-      runImportTest("FAILING_HIVE_IMPORT", types, vals, "failingImport.q", null);
-      // If we get here, then the run succeeded -- which is incorrect.
-      fail("FAILING_HIVE_IMPORT test should have thrown IOException");
-    } catch (IOException ioe) {
-      // expected; ok.
-    }
-  }
-
-  /** Test that we can set delimiters how we want them */
-  @Test
-  public void testCustomDelimiters() throws IOException {
-    String [] types = { "VARCHAR(32)", "INTEGER", "CHAR(64)" };
-    String [] vals = { "'test'", "42", "'somestring'" };
-    String [] extraArgs = { "--fields-terminated-by", ",", "--lines-terminated-by", "|" };
-    runImportTest("CUSTOM_DELIM_IMPORT", types, vals, "customDelimImport.q", extraArgs);
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestTableDefWriter.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestTableDefWriter.java
deleted file mode 100644
index 98b2294..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/hive/TestTableDefWriter.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.hive;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.junit.Test;
-
-import junit.framework.TestCase;
-
-/**
- * Test Hive DDL statement generation.
- */
-public class TestTableDefWriter extends TestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestTableDefWriter.class.getName());
-
-  // Test getHiveOctalCharCode and expect an IllegalArgumentException.
-  private void expectExceptionInCharCode(int charCode) {
-    try {
-      TableDefWriter.getHiveOctalCharCode(charCode);
-      fail("Expected IllegalArgumentException");
-    } catch (IllegalArgumentException iae) {
-      // Expected; ok.
-    }
-  }
-
-  public void testHiveOctalCharCode() {
-    assertEquals("\\000", TableDefWriter.getHiveOctalCharCode(0));
-    assertEquals("\\001", TableDefWriter.getHiveOctalCharCode(1));
-    assertEquals("\\012", TableDefWriter.getHiveOctalCharCode((int) '\n'));
-    assertEquals("\\177", TableDefWriter.getHiveOctalCharCode(0177));
-
-    expectExceptionInCharCode(4096);
-    expectExceptionInCharCode(0200);
-    expectExceptionInCharCode(254);
-  }
-
-  public void testDifferentTableNames() throws Exception {
-    Configuration conf = new Configuration();
-    SqoopOptions options = new SqoopOptions();
-    TableDefWriter writer = new TableDefWriter(options, null,
-        "inputTable", "outputTable", conf, false);
-
-    Map<String, Integer> colTypes = new HashMap<String, Integer>();
-    writer.setColumnTypes(colTypes);
-
-    String createTable = writer.getCreateTableStmt();
-    String loadData = writer.getLoadDataStmt();
-
-    LOG.debug("Create table stmt: " + createTable);
-    LOG.debug("Load data stmt: " + loadData);
-
-    // Assert that the statements generated have the form we expect.
-    assertTrue(createTable.indexOf("CREATE TABLE IF NOT EXISTS outputTable") != -1);
-    assertTrue(loadData.indexOf("INTO TABLE outputTable") != -1);
-    assertTrue(loadData.indexOf("/inputTable'") != -1);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/io/TestSplittableBufferedWriter.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/io/TestSplittableBufferedWriter.java
deleted file mode 100644
index deb5dfd..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/io/TestSplittableBufferedWriter.java
+++ /dev/null
@@ -1,257 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.io;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.OutputStream;
-import java.util.zip.GZIPInputStream;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-import junit.framework.TestCase;
-
-/**
- * Test that the splittable buffered writer system works.
- */
-public class TestSplittableBufferedWriter extends TestCase {
-
-  public static final Log LOG = LogFactory.getLog(
-      TestSplittableBufferedWriter.class.getName());
-
-  private String getWriteDir() {
-    return new File(ImportJobTestCase.TEMP_BASE_DIR,
-        "bufferedWriterTest").toString();
-  }
-
-  private Path getWritePath() {
-    return new Path(ImportJobTestCase.TEMP_BASE_DIR, "bufferedWriterTest");
-  }
-
-  /** Create the directory where we'll write our test files to; and
-   * make sure it has no files in it.
-   */
-  private void ensureEmptyWriteDir() throws IOException {
-    FileSystem fs = FileSystem.getLocal(getConf());
-    Path writeDir = getWritePath();
-
-    fs.mkdirs(writeDir);
-
-    FileStatus [] stats = fs.listStatus(writeDir);
-
-    for (FileStatus stat : stats) {
-      if (stat.isDir()) {
-        fail("setUp(): Write directory " + writeDir
-            + " contains subdirectories");
-      }
-
-      LOG.debug("setUp(): Removing " + stat.getPath());
-      if (!fs.delete(stat.getPath(), false)) {
-        fail("setUp(): Could not delete residual file " + stat.getPath());
-      }
-    }
-
-    if (!fs.exists(writeDir)) {
-      fail("setUp: Could not create " + writeDir);
-    }
-  }
-
-  public void setUp() throws IOException {
-    ensureEmptyWriteDir();
-  }
-
-  private Configuration getConf() {
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    return conf;
-  }
-
-  /** Verifies contents of an InputStream. Closes the InputStream on
-    * its way out. Fails the test if the file doesn't match the expected set
-    * of lines.
-    */
-  private void verifyFileContents(InputStream is, String [] lines)
-      throws IOException {
-    BufferedReader r = new BufferedReader(new InputStreamReader(is));
-    try {
-      for (String expectedLine : lines) {
-        String actualLine = r.readLine();
-        assertNotNull(actualLine);
-        assertEquals("Input line mismatch", expectedLine, actualLine);
-      }
-
-      assertNull("Stream had additional contents after expected line",
-          r.readLine());
-    } finally {
-      r.close();
-    }
-  }
-
-  private void verifyFileExists(Path p) throws IOException {
-    FileSystem fs = FileSystem.getLocal(getConf());
-    assertTrue("File not found: " + p, fs.exists(p));
-  }
-
-  private void verifyFileDoesNotExist(Path p) throws IOException {
-    FileSystem fs = FileSystem.getLocal(getConf());
-    assertFalse("File found: " + p + " and we did not expect it", fs.exists(p));
-  }
-
-  public void testNonSplittingTextFile() throws IOException {
-    SplittingOutputStream os  = new SplittingOutputStream(getConf(),
-        getWritePath(), "nonsplit-", 0, false);
-    SplittableBufferedWriter w = new SplittableBufferedWriter(os, true);
-    try {
-      w.allowSplit();
-      w.write("This is a string!");
-      w.newLine();
-      w.write("This is another string!");
-      w.allowSplit();
-    } finally {
-      w.close();
-    }
-
-    // Ensure we made exactly one file.
-    Path writePath = new Path(getWritePath(), "nonsplit-00000");
-    Path badPath = new Path(getWritePath(), "nonsplit-00001");
-    verifyFileExists(writePath);
-    verifyFileDoesNotExist(badPath); // Ensure we didn't make a second file.
-
-    // Now ensure all the data got there.
-    String [] expectedLines = {
-      "This is a string!",
-      "This is another string!",
-    };
-    verifyFileContents(new FileInputStream(new File(getWriteDir(),
-        "nonsplit-00000")), expectedLines);
-  }
-
-  public void testNonSplittingGzipFile() throws IOException {
-    SplittingOutputStream os  = new SplittingOutputStream(getConf(),
-        getWritePath(), "nonsplit-", 0, true);
-    SplittableBufferedWriter w = new SplittableBufferedWriter(os, true);
-    try {
-      w.allowSplit();
-      w.write("This is a string!");
-      w.newLine();
-      w.write("This is another string!");
-      w.allowSplit();
-    } finally {
-      w.close();
-    }
-
-    // Ensure we made exactly one file.
-    Path writePath = new Path(getWritePath(), "nonsplit-00000.gz");
-    Path badPath = new Path(getWritePath(), "nonsplit-00001.gz");
-    verifyFileExists(writePath);
-    verifyFileDoesNotExist(badPath); // Ensure we didn't make a second file.
-
-    // Now ensure all the data got there.
-    String [] expectedLines = {
-      "This is a string!",
-      "This is another string!",
-    };
-    verifyFileContents(
-        new GZIPInputStream(new FileInputStream(new File(getWriteDir(),
-        "nonsplit-00000.gz"))), expectedLines);
-  }
-
-  public void testSplittingTextFile() throws IOException {
-    SplittingOutputStream os  = new SplittingOutputStream(getConf(),
-        getWritePath(), "split-", 10, false);
-    SplittableBufferedWriter w = new SplittableBufferedWriter(os, true);
-    try {
-      w.allowSplit();
-      w.write("This is a string!");
-      w.newLine();
-      w.write("This is another string!");
-    } finally {
-      w.close();
-    }
-
-    // Ensure we made exactly two files.
-    Path writePath = new Path(getWritePath(), "split-00000");
-    Path writePath2 = new Path(getWritePath(), "split-00001");
-    Path badPath = new Path(getWritePath(), "split-00002");
-    verifyFileExists(writePath);
-    verifyFileExists(writePath2);
-    verifyFileDoesNotExist(badPath); // Ensure we didn't make three files.
-
-    // Now ensure all the data got there.
-    String [] expectedLines0 = {
-      "This is a string!"
-    };
-    verifyFileContents(new FileInputStream(new File(getWriteDir(),
-        "split-00000")), expectedLines0);
-
-    String [] expectedLines1 = {
-      "This is another string!",
-    };
-    verifyFileContents(new FileInputStream(new File(getWriteDir(),
-        "split-00001")), expectedLines1);
-  }
-
-  public void testSplittingGzipFile() throws IOException {
-    SplittingOutputStream os  = new SplittingOutputStream(getConf(),
-        getWritePath(), "splitz-", 3, true);
-    SplittableBufferedWriter w = new SplittableBufferedWriter(os, true);
-    try {
-      w.write("This is a string!");
-      w.newLine();
-      w.write("This is another string!");
-    } finally {
-      w.close();
-    }
-
-    // Ensure we made exactly two files.
-    Path writePath = new Path(getWritePath(), "splitz-00000.gz");
-    Path writePath2 = new Path(getWritePath(), "splitz-00001.gz");
-    Path badPath = new Path(getWritePath(), "splitz-00002.gz");
-    verifyFileExists(writePath);
-    verifyFileExists(writePath2);
-    verifyFileDoesNotExist(badPath); // Ensure we didn't make three files.
-
-    // Now ensure all the data got there.
-    String [] expectedLines0 = {
-      "This is a string!"
-    };
-    verifyFileContents(
-        new GZIPInputStream(new FileInputStream(new File(getWriteDir(),
-        "splitz-00000.gz"))), expectedLines0);
-
-    String [] expectedLines1 = {
-      "This is another string!",
-    };
-    verifyFileContents(
-        new GZIPInputStream(new FileInputStream(new File(getWriteDir(),
-        "splitz-00001.gz"))), expectedLines1);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java
deleted file mode 100644
index 661a9ac..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import java.util.ArrayList;
-import java.util.List;
-import junit.framework.TestCase;
-
-
-/**
- * Test that the field formatter works in a variety of configurations
- */
-public class TestFieldFormatter extends TestCase {
-  
-  public void testAllEmpty() {
-    char [] chars = new char[0];
-    String result = FieldFormatter.escapeAndEnclose("", "", "", chars, false);
-    assertEquals("", result);
-  }
-
-  public void testNullArgs() {
-    String result = FieldFormatter.escapeAndEnclose("", null, null, null, false);
-    assertEquals("", result);
-
-    char [] encloseFor = { '\"' };
-    assertNull(FieldFormatter.escapeAndEnclose(null, "\\", "\"", encloseFor,
-        false));
-  }
-
-  public void testBasicStr() {
-    String result = FieldFormatter.escapeAndEnclose("foo", null, null, null, false);
-    assertEquals("foo", result);
-  }
-
-  public void testEscapeSlash() {
-    String result = FieldFormatter.escapeAndEnclose("foo\\bar", "\\", "\"", null, false);
-    assertEquals("foo\\\\bar", result);
-  }
-
-  public void testMustEnclose() {
-    String result = FieldFormatter.escapeAndEnclose("foo", null, "\"", null, true);
-    assertEquals("\"foo\"", result);
-  }
-
-  public void testEncloseComma1() {
-    char [] chars = { ',' };
-
-    String result = FieldFormatter.escapeAndEnclose("foo,bar", "\\", "\"", chars, false);
-    assertEquals("\"foo,bar\"", result);
-  }
-
-  public void testEncloseComma2() {
-    char [] chars = { '\n', ',' };
-
-    String result = FieldFormatter.escapeAndEnclose("foo,bar", "\\", "\"", chars, false);
-    assertEquals("\"foo,bar\"", result);
-  }
-
-  public void testEncloseComma3() {
-    char [] chars = { ',', '\n' };
-
-    String result = FieldFormatter.escapeAndEnclose("foo,bar", "\\", "\"", chars, false);
-    assertEquals("\"foo,bar\"", result);
-  }
-
-  public void testNoNeedToEnclose() {
-    char [] chars = { ',', '\n' };
-
-    String result = FieldFormatter.escapeAndEnclose(
-        "just another string", "\\", "\"", chars, false);
-    assertEquals("just another string", result);
-  }
-
-  public void testCannotEnclose1() {
-    char [] chars = { ',', '\n' };
-
-    // can't enclose because encloser is ""
-    String result = FieldFormatter.escapeAndEnclose("foo,bar", "\\", "", chars, false);
-    assertEquals("foo,bar", result);
-  }
-
-  public void testCannotEnclose2() {
-    char [] chars = { ',', '\n' };
-
-    // can't enclose because encloser is null
-    String result = FieldFormatter.escapeAndEnclose("foo,bar", "\\", null, chars, false);
-    assertEquals("foo,bar", result);
-  }
-
-  public void testEmptyCharToEscapeString() {
-    // test what happens when the escape char is null. It should encode the null char.
-
-    char nul = '\000';
-    String s = "" + nul;
-    assertEquals("\000", s);
-  }
-  
-  public void testEscapeCentralQuote() {
-    String result = FieldFormatter.escapeAndEnclose("foo\"bar", "\\", "\"", null, false);
-    assertEquals("foo\\\"bar", result);
-  }
-
-  public void testEscapeMultiCentralQuote() {
-    String result = FieldFormatter.escapeAndEnclose("foo\"\"bar", "\\", "\"", null, false);
-    assertEquals("foo\\\"\\\"bar", result);
-  }
-
-  public void testDoubleEscape() {
-    String result = FieldFormatter.escapeAndEnclose("foo\\\"bar", "\\", "\"", null, false);
-    assertEquals("foo\\\\\\\"bar", result);
-  }
-
-  public void testReverseEscape() {
-    String result = FieldFormatter.escapeAndEnclose("foo\"\\bar", "\\", "\"", null, false);
-    assertEquals("foo\\\"\\\\bar", result);
-  }
-
-  public void testQuotedEncloser() {
-    char [] chars = { ',', '\n' };
-    
-    String result = FieldFormatter.escapeAndEnclose("foo\",bar", "\\", "\"", chars, false);
-    assertEquals("\"foo\\\",bar\"", result);
-  }
-
-  public void testQuotedEscape() {
-    char [] chars = { ',', '\n' };
-    
-    String result = FieldFormatter.escapeAndEnclose("foo\\,bar", "\\", "\"", chars, false);
-    assertEquals("\"foo\\\\,bar\"", result);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestRecordParser.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestRecordParser.java
deleted file mode 100644
index 31d7959..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestRecordParser.java
+++ /dev/null
@@ -1,356 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.lib;
-
-import java.util.ArrayList;
-import java.util.List;
-import junit.framework.TestCase;
-
-
-/**
- * Test that the record parser works in a variety of configurations.
- */
-public class TestRecordParser extends TestCase {
-
-  private void assertListsEqual(String msg, List<String> expected, List<String> actual) {
-    if (expected == null && actual != null) {
-      if (null == msg) {
-        msg = "expected null list";
-      }
-
-      fail(msg);
-    } else if (expected != null && actual == null) {
-      if (null == msg) {
-        msg = "expected non-null list";
-      }
-
-      fail(msg);
-    }
-
-    if (expected == null && actual == null) {
-      return; // ok. Both null; nothing to do.
-    }
-
-    int expectedLen = expected.size();
-    int actualLen = actual.size();
-
-    if (expectedLen != actualLen) {
-      if (null == msg) {
-        msg = "Expected list of length " + expectedLen + "; got " + actualLen;
-      }
-
-      fail(msg);
-    }
-
-    // Check the list contents.
-    for (int i = 0; i < expectedLen; i++) {
-      String expectedElem = expected.get(i);
-      String actualElem = actual.get(i);
-
-      if (expectedElem == null && actualElem != null) {
-        if (null == msg) {
-          msg = "Expected null element at position " + i + "; got [" + actualElem + "]";
-        }
-
-        fail(msg);
-      }
-
-      if (!expectedElem.equals(actualElem)) {
-        if (null == msg) {
-          msg = "Expected [" + expectedElem + "] at position " + i + "; got [" + actualElem + "]";
-        }
-
-        fail(msg);
-      }
-    }
-  }
-
-  private List<String> list(String [] items) {
-
-    if (null == items) {
-      return null;
-    }
-
-    ArrayList<String> asList = new ArrayList<String>();
-    for (int i = 0; i < items.length; i++) {
-      asList.add(items[i]);
-    }
-
-    return asList;
-  }
-  
-  public void testEmptyLine() throws RecordParser.ParseError {
-    // an empty line should return no fields.
-
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { };
-    assertListsEqual(null, list(strings), parser.parseRecord(""));
-  }
-
-  public void testJustEOR() throws RecordParser.ParseError {
-    // a line with just a newline char should return a single zero-length field.
-
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\n"));
-  }
-
-  public void testOneField() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("the field"));
-  }
-
-  public void testOneField2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("the field\n"));
-  }
-
-  public void testQuotedField1() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the field\"\n"));
-  }
-
-  public void testQuotedField2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the field\""));
-  }
-
-  public void testQuotedField3() throws RecordParser.ParseError {
-    // quoted containing EOF
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the ,field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the ,field\""));
-  }
-
-  public void testQuotedField4() throws RecordParser.ParseError {
-    // quoted containing multiple EOFs
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the ,,field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the ,,field\""));
-  }
-
-  public void testQuotedField5() throws RecordParser.ParseError {
-    // quoted containing EOF and EOR
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the ,\nfield" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the ,\nfield\""));
-  }
-
-  public void testQuotedField6() throws RecordParser.ParseError {
-    // quoted containing EOR
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the \nfield" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the \nfield\""));
-  }
-
-  public void testQuotedField7() throws RecordParser.ParseError {
-    // quoted containing multiple EORs
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the \n\nfield" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the \n\nfield\""));
-  }
-
-  public void testQuotedField8() throws RecordParser.ParseError {
-    // quoted containing escaped quoted char
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the \"field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"the \\\"field\""));
-  }
-
-  public void testUnquotedEscape1() throws RecordParser.ParseError {
-    // field without quotes with an escaped EOF char.
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the ,field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("the \\,field"));
-  }
-
-  public void testUnquotedEscape2() throws RecordParser.ParseError {
-    // field without quotes with an escaped escape char.
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "the \\field" };
-    assertListsEqual(null, list(strings), parser.parseRecord("the \\\\field"));
-  }
-
-  public void testTwoFields1() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("field1,field2"));
-  }
-
-  public void testTwoFields2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("field1,field2\n"));
-  }
-
-  public void testTwoFields3() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"field1\",field2\n"));
-  }
-
-  public void testTwoFields4() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("field1,\"field2\"\n"));
-  }
-
-  public void testTwoFields5() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("field1,\"field2\""));
-  }
-  
-  public void testRequiredQuotes0() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"field1\",\"field2\"\n"));
-  }
-
-  public void testRequiredQuotes1() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String [] strings = { "field1", "field2" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\"field1\",\"field2\""));
-  }
-
-  public void testRequiredQuotes2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String [] strings = { "field1", "field2" };
-    try {
-      parser.parseRecord("\"field1\",field2");
-      fail("Expected parse error for required quotes");
-    } catch (RecordParser.ParseError pe) {
-      // ok. expected.
-    }
-  }
-
-  public void testRequiredQuotes3() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String [] strings = { "field1", "field2" };
-    try {
-      parser.parseRecord("field1,\"field2\"");
-      fail("Expected parse error for required quotes");
-    } catch (RecordParser.ParseError pe) {
-      // ok. expected.
-    }
-  }
-
-  public void testRequiredQuotes4() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String [] strings = { "field1", "field2" };
-    try {
-      parser.parseRecord("field1,\"field2\"\n");
-      fail("Expected parse error for required quotes");
-    } catch (RecordParser.ParseError pe) {
-      // ok. expected.
-    }
-  }
-
-  public void testNull() {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', true);
-    String input = null;
-    try {
-      parser.parseRecord(input);
-      fail("Expected parse error for null string");
-    } catch (RecordParser.ParseError pe) {
-      // ok. expected.
-    }
-  }
-
-
-  public void testEmptyFields1() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "", ""};
-    assertListsEqual(null, list(strings), parser.parseRecord(","));
-  }
-
-  public void testEmptyFields2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "", "" };
-    assertListsEqual(null, list(strings), parser.parseRecord(",\n"));
-  }
-
-  public void testEmptyFields3() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "", "", "" };
-    assertListsEqual(null, list(strings), parser.parseRecord(",,\n"));
-  }
-
-  public void testEmptyFields4() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "", "foo", "" };
-    assertListsEqual(null, list(strings), parser.parseRecord(",foo,\n"));
-  }
-
-  public void testEmptyFields5() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "", "foo", "" };
-    assertListsEqual(null, list(strings), parser.parseRecord(",foo,"));
-  }
-
-  public void testEmptyFields6() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "foo", "" };
-    assertListsEqual(null, list(strings), parser.parseRecord("foo,"));
-  }
-
-  public void testTrailingText() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "foo", "bar" };
-    assertListsEqual(null, list(strings), parser.parseRecord("foo,bar\nbaz"));
-  }
-
-  public void testTrailingText2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\nbaz"));
-  }
-
-  public void testLeadingEscape() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', '\n', '\"', '\\', false);
-    String [] strings = { "\nbaz" };
-    assertListsEqual(null, list(strings), parser.parseRecord("\\\nbaz"));
-  }
-
-  public void testEofIsEor() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', ',', '\"', '\\', false);
-    String [] strings = { "three", "different", "fields" };
-    assertListsEqual(null, list(strings), parser.parseRecord("three,different,fields"));
-  }
-
-  public void testEofIsEor2() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', ',', '\"', '\\', false);
-    String [] strings = { "three", "different", "fields" };
-    assertListsEqual(null, list(strings), parser.parseRecord("three,\"different\",fields"));
-  }
-
-  public void testRepeatedParse() throws RecordParser.ParseError {
-    RecordParser parser = new RecordParser(',', ',', '\"', '\\', false);
-    String [] strings = { "three", "different", "fields" };
-    assertListsEqual(null, list(strings), parser.parseRecord("three,\"different\",fields"));
-
-    String [] strings2 = { "foo", "bar" };
-    assertListsEqual(null, list(strings2), parser.parseRecord("foo,\"bar\""));
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/LocalMySQLTest.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/LocalMySQLTest.java
deleted file mode 100644
index 0bf694c..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/LocalMySQLTest.java
+++ /dev/null
@@ -1,405 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.FileInputStream;
-import java.io.File;
-import java.sql.Connection;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.util.FileListing;
-
-/**
- * Test the LocalMySQLManager implementation.
- * This differs from MySQLManager only in its importTable() method, which
- * uses mysqldump instead of mapreduce+DBInputFormat.
- *
- * Since this requires a MySQL installation on your local machine to use, this
- * class is named in such a way that Hadoop's default QA process does not run
- * it. You need to run this manually with -Dtestcase=LocalMySQLTest.
- *
- * You need to put MySQL's Connector/J JDBC driver library into a location
- * where Hadoop will be able to access it (since this library cannot be checked
- * into Apache's tree for licensing reasons).
- *
- * You should also create a database named 'sqooptestdb' and authorize yourself:
- *
- * CREATE DATABASE sqooptestdb;
- * use mysql;
- * GRANT ALL PRIVILEGES ON sqooptestdb.* TO 'yourusername'@'localhost';
- * flush privileges;
- *
- */
-public class LocalMySQLTest extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(LocalMySQLTest.class.getName());
-
-  static final String HOST_URL = "jdbc:mysql://localhost/";
-
-  static final String MYSQL_DATABASE_NAME = "sqooptestdb";
-  static final String TABLE_NAME = "EMPLOYEES_MYSQL";
-  static final String CONNECT_STRING = HOST_URL + MYSQL_DATABASE_NAME;
-
-  // instance variables populated during setUp, used during tests
-  private LocalMySQLManager manager;
-
-  @Before
-  public void setUp() {
-    SqoopOptions options = new SqoopOptions(CONNECT_STRING, TABLE_NAME);
-    options.setUsername(getCurrentUser());
-    manager = new LocalMySQLManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = manager.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data. 
-      st.executeUpdate("DROP TABLE IF EXISTS " + TABLE_NAME);
-      st.executeUpdate("CREATE TABLE " + TABLE_NAME + " ("
-          + "id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, "
-          + "name VARCHAR(24) NOT NULL, "
-          + "start_date DATE, "
-          + "salary FLOAT, "
-          + "dept VARCHAR(32))");
-
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "NULL,'Aaron','2009-05-14',1000000.00,'engineering')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "NULL,'Bob','2009-04-20',400.00,'sales')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "NULL,'Fred','2009-01-23',15.00,'marketing')");
-      connection.commit();
-    } catch (SQLException sqlE) {
-      LOG.error("Encountered SQL Exception: " + sqlE);
-      sqlE.printStackTrace();
-      fail("SQLException when running test setUp(): " + sqlE);
-    } finally {
-      try {
-        if (null != st) {
-          st.close();
-        }
-
-        if (null != connection) {
-          connection.close();
-        }
-      } catch (SQLException sqlE) {
-        LOG.warn("Got SQLException when closing connection: " + sqlE);
-      }
-    }
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  /** @return the current username. */
-  private String getCurrentUser() {
-    // First, check the $USER environment variable.
-    String envUser = System.getenv("USER");
-    if (null != envUser) {
-      return envUser;
-    }
-
-    // Try `whoami`
-    String [] whoamiArgs = new String[1];
-    whoamiArgs[0] = "whoami";
-    Process p = null;
-    BufferedReader r = null;
-    try {
-      p = Runtime.getRuntime().exec(whoamiArgs);
-      InputStream is = p.getInputStream();
-      r = new BufferedReader(new InputStreamReader(is));
-      return r.readLine();
-    } catch (IOException ioe) {
-      LOG.error("IOException reading from `whoami`: " + ioe.toString());
-      return null;
-    } finally {
-      // close our stream.
-      if (null != r) {
-        try {
-          r.close();
-        } catch (IOException ioe) {
-          LOG.warn("IOException closing input stream from `whoami`: " + ioe.toString());
-        }
-      }
-
-      // wait for whoami to exit.
-      while (p != null) {
-        try {
-          int ret = p.waitFor();
-          if (0 != ret) {
-            LOG.error("whoami exited with error status " + ret);
-            // suppress original return value from this method.
-            return null; 
-          }
-        } catch (InterruptedException ie) {
-          continue; // loop around.
-        }
-      }
-    }
-  }
-
-  private String [] getArgv(boolean mysqlOutputDelims, boolean isDirect,
-      String tableName, String... extraArgs) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    CommonArgs.addHadoopFlags(args);
-
-    args.add("--table");
-    args.add(tableName);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(CONNECT_STRING);
-    if (isDirect) {
-      args.add("--direct");
-    }
-    args.add("--username");
-    args.add(getCurrentUser());
-    args.add("--where");
-    args.add("id > 1");
-    args.add("--num-mappers");
-    args.add("1");
-
-    if (mysqlOutputDelims) {
-      args.add("--mysql-delimiters");
-    }
-
-    if (null != extraArgs) {
-      for (String arg : extraArgs) {
-        args.add(arg);
-      }
-    }
-
-    return args.toArray(new String[0]);
-  }
-
-  private void doImport(boolean mysqlOutputDelims, boolean isDirect,
-      String tableName, String [] expectedResults, String [] extraArgs)
-      throws IOException {
-
-    Path warehousePath = new Path(this.getWarehouseDir());
-    Path tablePath = new Path(warehousePath, tableName);
-
-    Path filePath;
-    if (isDirect) {
-      filePath = new Path(tablePath, "data-00000");
-    } else {
-      filePath = new Path(tablePath, "part-m-00000");
-    }
-
-    File tableFile = new File(tablePath.toString());
-    if (tableFile.exists() && tableFile.isDirectory()) {
-      // remove the directory before running the import.
-      FileListing.recursiveDeleteDir(tableFile);
-    }
-
-    String [] argv = getArgv(mysqlOutputDelims, isDirect, tableName, extraArgs);
-    try {
-      runImport(argv);
-    } catch (IOException ioe) {
-      LOG.error("Got IOException during import: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    }
-
-    File f = new File(filePath.toString());
-    assertTrue("Could not find imported data file: " + f, f.exists());
-    BufferedReader r = null;
-    try {
-      // Read through the file and make sure it's all there.
-      r = new BufferedReader(new InputStreamReader(new FileInputStream(f)));
-      for (String expectedLine : expectedResults) {
-        assertEquals(expectedLine, r.readLine());
-      }
-    } catch (IOException ioe) {
-      LOG.error("Got IOException verifying results: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(r);
-    }
-  }
-
-  @Test
-  public void testLocalBulkImportWithDefaultDelims() throws IOException {
-    // no quoting of strings allowed.
-    String [] expectedResults = {
-        "2,Bob,2009-04-20,400,sales",
-        "3,Fred,2009-01-23,15,marketing"
-    };
-
-    doImport(false, true, TABLE_NAME, expectedResults, null);
-  }
-
-  @Test
-  public void testWithExtraParams() throws IOException {
-    // no quoting of strings allowed.
-    String [] expectedResults = {
-        "2,Bob,2009-04-20,400,sales",
-        "3,Fred,2009-01-23,15,marketing"
-    };
-
-    String [] extraArgs = { "-", "--lock-tables" };
-
-    doImport(false, true, TABLE_NAME, expectedResults, extraArgs);
-  }
-
-  @Test
-  public void testLocalBulkImportWithMysqlQuotes() throws IOException {
-    // mysql quotes all string-based output.
-    String [] expectedResults = {
-        "2,'Bob','2009-04-20',400,'sales'",
-        "3,'Fred','2009-01-23',15,'marketing'"
-    };
-
-    doImport(true, true, TABLE_NAME, expectedResults, null);
-  }
-
-  @Test
-  public void testMysqlJdbcImport() throws IOException {
-    String [] expectedResults = {
-        "2,Bob,2009-04-20,400.0,sales",
-        "3,Fred,2009-01-23,15.0,marketing"
-    };
-
-    doImport(false, false, TABLE_NAME, expectedResults, null);
-  }
-
-  @Test
-  public void testJdbcEscapedTableName() throws Exception {
-    // Test a JDBC-based import of a table whose name is
-    // a reserved sql keyword (and is thus `quoted`)
-    final String reservedTableName = "TABLE";
-    SqoopOptions options = new SqoopOptions(CONNECT_STRING,
-        reservedTableName);
-    options.setUsername(getCurrentUser());
-    ConnManager mgr = new MySQLManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = mgr.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data.
-      st.executeUpdate("DROP TABLE IF EXISTS `" + reservedTableName + "`");
-      st.executeUpdate("CREATE TABLE `" + reservedTableName + "` ("
-          + "id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, "
-          + "name VARCHAR(24) NOT NULL, "
-          + "start_date DATE, "
-          + "salary FLOAT, "
-          + "dept VARCHAR(32))");
-
-      st.executeUpdate("INSERT INTO `" + reservedTableName + "` VALUES("
-          + "2,'Aaron','2009-05-14',1000000.00,'engineering')");
-      connection.commit();
-    } finally {
-      if (null != st) {
-        st.close();
-      }
-
-      if (null != connection) {
-        connection.close();
-      }
-    }
-
-    String [] expectedResults = {
-        "2,Aaron,2009-05-14,1000000.0,engineering"
-    };
-
-    doImport(false, false, reservedTableName, expectedResults, null);
-  }
-
-  @Test
-  public void testJdbcEscapedColumnName() throws Exception {
-    // Test a JDBC-based import of a table with a column whose name is
-    // a reserved sql keyword (and is thus `quoted`)
-    final String tableName = "mysql_escaped_col_table";
-    SqoopOptions options = new SqoopOptions(CONNECT_STRING,
-        tableName);
-    options.setUsername(getCurrentUser());
-    ConnManager mgr = new MySQLManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = mgr.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data.
-      st.executeUpdate("DROP TABLE IF EXISTS " + tableName);
-      st.executeUpdate("CREATE TABLE " + tableName + " ("
-          + "id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, "
-          + "`table` VARCHAR(24) NOT NULL, "
-          + "`CREATE` DATE, "
-          + "salary FLOAT, "
-          + "dept VARCHAR(32))");
-
-      st.executeUpdate("INSERT INTO " + tableName + " VALUES("
-          + "2,'Aaron','2009-05-14',1000000.00,'engineering')");
-      connection.commit();
-    } finally {
-      if (null != st) {
-        st.close();
-      }
-
-      if (null != connection) {
-        connection.close();
-      }
-    }
-
-    String [] expectedResults = {
-        "2,Aaron,2009-05-14,1000000.0,engineering"
-    };
-
-    doImport(false, false, tableName, expectedResults, null);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/MySQLAuthTest.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/MySQLAuthTest.java
deleted file mode 100644
index 9ce82a9..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/MySQLAuthTest.java
+++ /dev/null
@@ -1,323 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.FileInputStream;
-import java.io.File;
-import java.sql.Connection;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.ArrayList;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test authentication and remote access to direct mysqldump-based imports.
- *
- * Since this requires a MySQL installation on your local machine to use, this
- * class is named in such a way that Hadoop's default QA process does not run
- * it. You need to run this manually with -Dtestcase=MySQLAuthTest
- *
- * You need to put MySQL's Connector/J JDBC driver library into a location
- * where Hadoop will be able to access it (since this library cannot be checked
- * into Apache's tree for licensing reasons).
- *
- * You need to create a database used by Sqoop for password tests:
- *
- * CREATE DATABASE sqooppasstest;
- * use mysql;
- * GRANT ALL PRIVILEGES on sqooppasstest.* TO 'sqooptest'@'localhost' IDENTIFIED BY '12345';
- * flush privileges;
- *
- */
-public class MySQLAuthTest extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(MySQLAuthTest.class.getName());
-
-  static final String HOST_URL = "jdbc:mysql://localhost/";
-
-  static final String AUTH_TEST_DATABASE = "sqooppasstest";
-  static final String AUTH_TEST_USER = "sqooptest";
-  static final String AUTH_TEST_PASS = "12345";
-  static final String AUTH_TABLE_NAME = "authtest";
-  static final String AUTH_CONNECT_STRING = HOST_URL + AUTH_TEST_DATABASE;
-
-  // instance variables populated during setUp, used during tests
-  private LocalMySQLManager manager;
-
-  @Before
-  public void setUp() {
-    SqoopOptions options = new SqoopOptions(AUTH_CONNECT_STRING, AUTH_TABLE_NAME);
-    options.setUsername(AUTH_TEST_USER);
-    options.setPassword(AUTH_TEST_PASS);
-
-    manager = new LocalMySQLManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = manager.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data. 
-      st.executeUpdate("DROP TABLE IF EXISTS " + AUTH_TABLE_NAME);
-      st.executeUpdate("CREATE TABLE " + AUTH_TABLE_NAME + " ("
-          + "id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, "
-          + "name VARCHAR(24) NOT NULL)");
-
-      st.executeUpdate("INSERT INTO " + AUTH_TABLE_NAME + " VALUES("
-          + "NULL,'Aaron')");
-      connection.commit();
-    } catch (SQLException sqlE) {
-      LOG.error("Encountered SQL Exception: " + sqlE);
-      sqlE.printStackTrace();
-      fail("SQLException when running test setUp(): " + sqlE);
-    } finally {
-      try {
-        if (null != st) {
-          st.close();
-        }
-
-        if (null != connection) {
-          connection.close();
-        }
-      } catch (SQLException sqlE) {
-        LOG.warn("Got SQLException when closing connection: " + sqlE);
-      }
-    }
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  private String [] getArgv(boolean includeHadoopFlags,
-      boolean useDirect, String connectString, String tableName) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(tableName);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(connectString);
-    if (useDirect) {
-      args.add("--direct");
-    }
-    args.add("--username");
-    args.add(AUTH_TEST_USER);
-    args.add("--password");
-    args.add(AUTH_TEST_PASS);
-    args.add("--mysql-delimiters");
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  /**
-   * Connect to a db and ensure that password-based authentication
-   * succeeds.
-   */
-  @Test
-  public void testAuthAccess() {
-    String [] argv = getArgv(true, true, AUTH_CONNECT_STRING, AUTH_TABLE_NAME);
-    try {
-      runImport(argv);
-    } catch (IOException ioe) {
-      LOG.error("Got IOException during import: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    }
-
-    Path warehousePath = new Path(this.getWarehouseDir());
-    Path tablePath = new Path(warehousePath, AUTH_TABLE_NAME);
-    Path filePath = new Path(tablePath, "data-00000");
-
-    File f = new File(filePath.toString());
-    assertTrue("Could not find imported data file", f.exists());
-    BufferedReader r = null;
-    try {
-      // Read through the file and make sure it's all there.
-      r = new BufferedReader(new InputStreamReader(new FileInputStream(f)));
-      assertEquals("1,'Aaron'", r.readLine());
-    } catch (IOException ioe) {
-      LOG.error("Got IOException verifying results: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(r);
-    }
-  }
-
-  @Test
-  public void testZeroTimestamp() throws IOException, SQLException {
-    // MySQL timestamps can hold values whose range causes problems
-    // for java.sql.Timestamp. The MySQLManager adds settings to the
-    // connect string which configure the driver's handling of
-    // zero-valued timestamps. Check that all of these modifications
-    // to the connect string are successful.
-
-    try {
-      // A connect string with a null 'query' component.
-      doZeroTimestampTest(0, true, AUTH_CONNECT_STRING);
-
-      // A connect string with a zero-length query component.
-      doZeroTimestampTest(1, true, AUTH_CONNECT_STRING + "?");
-
-      // A connect string with another argument
-      doZeroTimestampTest(2, true, AUTH_CONNECT_STRING + "?connectTimeout=0");
-      doZeroTimestampTest(3, true, AUTH_CONNECT_STRING + "?connectTimeout=0&");
-
-      // A connect string with the zero-timestamp behavior already
-      // configured.
-      doZeroTimestampTest(4, true, AUTH_CONNECT_STRING
-          + "?zeroDateTimeBehavior=convertToNull");
-
-      // And finally, behavior already configured in such a way as to
-      // cause the timestamp import to fail.
-      doZeroTimestampTest(5, false, AUTH_CONNECT_STRING
-          + "?zeroDateTimeBehavior=exception");
-    } finally {
-      // Clean up our mess on the way out.
-      dropTimestampTables();
-    }
-  }
-
-  private void dropTimestampTables() throws SQLException {
-    SqoopOptions options = new SqoopOptions(AUTH_CONNECT_STRING, null);
-    options.setUsername(AUTH_TEST_USER);
-    options.setPassword(AUTH_TEST_PASS);
-
-    manager = new LocalMySQLManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    connection = manager.getConnection();
-    connection.setAutoCommit(false);
-    st = connection.createStatement();
-
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable0");
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable1");
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable2");
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable3");
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable4");
-    st.executeUpdate("DROP TABLE IF EXISTS mysqlTimestampTable5");
-    connection.commit();
-    st.close();
-    connection.close();
-  }
-
-  public void doZeroTimestampTest(int testNum, boolean expectSuccess,
-      String connectString) throws IOException, SQLException {
-
-    LOG.info("Beginning zero-timestamp test #" + testNum);
-
-    try {
-      final String tableName = "mysqlTimestampTable" + Integer.toString(testNum);
-
-      // Create a table containing a full-zeros timestamp.
-      SqoopOptions options = new SqoopOptions(connectString, tableName);
-      options.setUsername(AUTH_TEST_USER);
-      options.setPassword(AUTH_TEST_PASS);
-
-      manager = new LocalMySQLManager(options);
-
-      Connection connection = null;
-      Statement st = null;
-
-      connection = manager.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data. 
-      st.executeUpdate("DROP TABLE IF EXISTS " + tableName);
-      st.executeUpdate("CREATE TABLE " + tableName + " ("
-          + "id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, "
-          + "ts TIMESTAMP NOT NULL)");
-
-      st.executeUpdate("INSERT INTO " + tableName + " VALUES("
-          + "NULL,'0000-00-00 00:00:00.0')");
-      connection.commit();
-      st.close();
-      connection.close();
-
-      // Run the import.
-      String [] argv = getArgv(true, false, connectString, tableName);
-      try {
-        runImport(argv);
-      } catch (Exception e) {
-        if (expectSuccess) {
-          // This is unexpected. rethrow.
-          throw new RuntimeException(e);
-        } else {
-          // We expected an error.
-          LOG.info("Got exception running import (expected). msg: " + e);
-        }
-      }
-
-      // Make sure the result file is there.
-      Path warehousePath = new Path(this.getWarehouseDir());
-      Path tablePath = new Path(warehousePath, tableName);
-      Path filePath = new Path(tablePath, "part-m-00000");
-
-      File f = new File(filePath.toString());
-      if (expectSuccess) {
-        assertTrue("Could not find imported data file", f.exists());
-        BufferedReader r = new BufferedReader(new InputStreamReader(
-            new FileInputStream(f)));
-        assertEquals("1,null", r.readLine());
-        IOUtils.closeStream(r);
-      } else {
-        assertFalse("Imported data when expected failure", f.exists());
-      }
-    } finally {
-      LOG.info("Finished zero timestamp test #" + testNum);
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/OracleManagerTest.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/OracleManagerTest.java
deleted file mode 100644
index de5488e..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/OracleManagerTest.java
+++ /dev/null
@@ -1,295 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.FileInputStream;
-import java.io.File;
-import java.sql.Connection;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.Date;
-import java.util.Calendar;
-import java.util.TimeZone;
-import java.util.ArrayList;
-import java.text.ParseException;
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.util.FileListing;
-
-/**
- * Test the OracleManager implementation.
- *
- * This uses JDBC to import data from an Oracle database into HDFS.
- *
- * Since this requires an Oracle installation on your local machine to use,
- * this class is named in such a way that Hadoop's default QA process does
- * not run it. You need to run this manually with -Dtestcase=OracleManagerTest.
- *
- * You need to put Oracle's JDBC driver library (ojdbc6_g.jar) in a location
- * where Hadoop will be able to access it (since this library cannot be checked
- * into Apache's tree for licensing reasons).
- *
- * To set up your test environment:
- *   Install Oracle Express Edition 10.2.0.
- *   Create a database user named SQOOPTEST
- *   Set the user's password to '12345'
- *   Grant the user the CREATE TABLE privilege
- */
-public class OracleManagerTest extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(OracleManagerTest.class.getName());
-
-  static final String ORACLE_DATABASE_NAME = "xe"; // Express edition hardcoded name.
-  static final String TABLE_NAME = "EMPLOYEES";
-  static final String CONNECT_STRING = "jdbc:oracle:thin:@//localhost/" + ORACLE_DATABASE_NAME;
-  static final String ORACLE_USER_NAME = "SQOOPTEST";
-  static final String ORACLE_USER_PASS = "12345";
-
-  // instance variables populated during setUp, used during tests
-  private OracleManager manager;
-
-  @Before
-  public void setUp() {
-    SqoopOptions options = new SqoopOptions(CONNECT_STRING, TABLE_NAME);
-    options.setUsername(ORACLE_USER_NAME);
-    options.setPassword(ORACLE_USER_PASS);
-
-    manager = new OracleManager(options);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = manager.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data. 
-      st.executeUpdate("BEGIN EXECUTE IMMEDIATE 'DROP TABLE " + TABLE_NAME + "'; "
-          + "exception when others then null; end;");
-      st.executeUpdate("CREATE TABLE " + TABLE_NAME + " ("
-          + "id INT NOT NULL, "
-          + "name VARCHAR2(24) NOT NULL, "
-          + "start_date DATE, "
-          + "salary FLOAT, "
-          + "dept VARCHAR2(32), "
-          + "timestamp_tz TIMESTAMP WITH TIME ZONE, "
-          + "timestamp_ltz TIMESTAMP WITH LOCAL TIME ZONE, "
-          + "PRIMARY KEY (id))");
-
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "1,'Aaron',to_date('2009-05-14','yyyy-mm-dd'),1000000.00,'engineering','29-DEC-09 12.00.00.000000000 PM','29-DEC-09 12.00.00.000000000 PM')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "2,'Bob',to_date('2009-04-20','yyyy-mm-dd'),400.00,'sales','30-DEC-09 12.00.00.000000000 PM','30-DEC-09 12.00.00.000000000 PM')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "3,'Fred',to_date('2009-01-23','yyyy-mm-dd'),15.00,'marketing','31-DEC-09 12.00.00.000000000 PM','31-DEC-09 12.00.00.000000000 PM')");
-      connection.commit();
-    } catch (SQLException sqlE) {
-      LOG.error("Encountered SQL Exception: " + sqlE);
-      sqlE.printStackTrace();
-      fail("SQLException when running test setUp(): " + sqlE);
-    } finally {
-      try {
-        if (null != st) {
-          st.close();
-        }
-
-        if (null != connection) {
-          connection.close();
-        }
-      } catch (SQLException sqlE) {
-        LOG.warn("Got SQLException when closing connection: " + sqlE);
-      }
-    }
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  private String [] getArgv() {
-    ArrayList<String> args = new ArrayList<String>();
-
-    CommonArgs.addHadoopFlags(args);
-
-    args.add("--table");
-    args.add(TABLE_NAME);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(CONNECT_STRING);
-    args.add("--username");
-    args.add(ORACLE_USER_NAME);
-    args.add("--password");
-    args.add(ORACLE_USER_PASS);
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  private void runOracleTest(String [] expectedResults) throws IOException {
-
-    Path warehousePath = new Path(this.getWarehouseDir());
-    Path tablePath = new Path(warehousePath, TABLE_NAME);
-    Path filePath = new Path(tablePath, "part-00000");
-
-    File tableFile = new File(tablePath.toString());
-    if (tableFile.exists() && tableFile.isDirectory()) {
-      // remove the directory before running the import.
-      FileListing.recursiveDeleteDir(tableFile);
-    }
-
-    String [] argv = getArgv();
-    try {
-      runImport(argv);
-    } catch (IOException ioe) {
-      LOG.error("Got IOException during import: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    }
-      
-    File f = new File(filePath.toString());
-    assertTrue("Could not find imported data file", f.exists());
-    BufferedReader r = null;
-    try {
-      // Read through the file and make sure it's all there.
-      r = new BufferedReader(new InputStreamReader(new FileInputStream(f)));
-      for (String expectedLine : expectedResults) {
-        compareRecords(expectedLine, r.readLine());
-      }
-    } catch (IOException ioe) {
-      LOG.error("Got IOException verifying results: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(r);
-    }
-  }
-
-  @Test
-  public void testOracleImport() throws IOException {
-    // no quoting of strings allowed.
-    // NOTE(aaron): Oracle JDBC 11.1 drivers auto-cast SQL DATE to java.sql.Timestamp.
-    // Even if you define your columns as DATE in Oracle, they may still contain
-    // time information, so the JDBC drivers lie to us and will never tell us we have
-    // a strict DATE type. Thus we include HH:MM:SS.mmmmm below.
-    // See http://www.oracle.com/technology/tech/java/sqlj_jdbc/htdocs/jdbc_faq.html#08_01
-    String [] expectedResults = {
-        "1,Aaron,2009-05-14 00:00:00.0,1000000,engineering,2009-12-29 12:00:00.0,2009-12-29 12:00:00.0",
-        "2,Bob,2009-04-20 00:00:00.0,400,sales,2009-12-30 12:00:00.0,2009-12-30 12:00:00.0",
-        "3,Fred,2009-01-23 00:00:00.0,15,marketing,2009-12-31 12:00:00.0,2009-12-31 12:00:00.0"
-    };
-
-    runOracleTest(expectedResults);
-  }
-
-  /**
-   * Compare two lines
-   * @param expectedLine    expected line
-   * @param receivedLine    received line
-   * @throws IOException    exception during lines comparison
-   */
-  private void compareRecords(String expectedLine, String receivedLine) throws IOException {
-    // handle null case
-    if (expectedLine == null || receivedLine == null) {
-      return;
-    }
-
-    // check if lines are equal
-    if (expectedLine.equals(receivedLine)) {
-      return;
-    }
-
-    // check if size is the same
-    String [] expectedValues = expectedLine.split(",");
-    String [] receivedValues = receivedLine.split(",");
-    if (expectedValues.length != 7 || receivedValues.length != 7) {
-      LOG.error("Number of expected fields did not match number of received fields");
-      throw new IOException("Number of expected fields did not match number of received fields");
-    }
-
-    // check first 5 values
-    boolean mismatch = false;
-    for (int i = 0; !mismatch && i < 5; i++) {
-      mismatch = !expectedValues[i].equals(receivedValues[i]);
-    }
-    if (mismatch) {
-      throw new IOException("Expected:<" + expectedLine + "> but was:<" + receivedLine + ">");
-    }
-
-    Date expectedDate = null;
-    Date receivedDate = null;
-    DateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.S");
-    int offset = TimeZone.getDefault().getOffset(System.currentTimeMillis()) / 3600000;
-    for (int i = 5; i < 7; i++) {
-      // parse expected timestamp
-      try {
-        expectedDate = df.parse(expectedValues[i]);
-      } catch (ParseException ex) {
-        LOG.error("Could not parse expected timestamp: " + expectedValues[i]);
-        throw new IOException("Could not parse expected timestamp: " + expectedValues[i]);
-      }
-
-      // parse received timestamp
-      try {
-        receivedDate = df.parse(receivedValues[i]);
-      } catch (ParseException ex) {
-        LOG.error("Could not parse received timestamp: " + receivedValues[i]);
-        throw new IOException("Could not parse received timestamp: " + receivedValues[i]);
-      }
-
-      // compare two timestamps considering timezone offset
-      Calendar expectedCal = Calendar.getInstance();
-      expectedCal.setTime(expectedDate);
-      expectedCal.add(Calendar.HOUR, offset);
-
-      Calendar receivedCal = Calendar.getInstance();
-      receivedCal.setTime(receivedDate);
-
-      if (!expectedCal.equals(receivedCal)) {
-        throw new IOException("Expected:<" + expectedLine + "> but was:<" + receivedLine + ">, while timezone offset is: " + offset);
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/PostgresqlTest.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/PostgresqlTest.java
deleted file mode 100644
index d4a066f..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/PostgresqlTest.java
+++ /dev/null
@@ -1,239 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.FileInputStream;
-import java.io.File;
-import java.sql.Connection;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.util.FileListing;
-
-/**
- * Test the PostgresqlManager and DirectPostgresqlManager implementations.
- * The former uses the postgres JDBC driver to perform an import;
- * the latter uses pg_dump to facilitate it.
- *
- * Since this requires a Postgresql installation on your local machine to use, this
- * class is named in such a way that Hadoop's default QA process does not run
- * it. You need to run this manually with -Dtestcase=PostgresqlTest.
- *
- * You need to put Postgresql's JDBC driver library into a location where Hadoop
- * can access it (e.g., $HADOOP_HOME/lib).
- *
- * To configure a postgresql database to allow local connections, put the following
- * in /etc/postgresql/8.3/main/pg_hba.conf:
- *     local  all all trust
- *     host all all 127.0.0.1/32 trust
- * 
- * ... and comment out any other lines referencing 127.0.0.1.
- *
- * For postgresql 8.1, this may be in /var/lib/pgsql/data, instead.
- * You may need to restart the postgresql service after modifying this file.
- *
- * You should also create a sqooptest user and database:
- *
- * $ sudo -u postgres psql -U postgres template1
- * template1=> CREATE USER sqooptest;
- * template1=> CREATE DATABASE sqooptest;
- * template1=> \q
- *
- */
-public class PostgresqlTest extends ImportJobTestCase {
-
-  public static final Log LOG = LogFactory.getLog(PostgresqlTest.class.getName());
-
-  static final String HOST_URL = "jdbc:postgresql://localhost/";
-
-  static final String DATABASE_USER = "sqooptest";
-  static final String DATABASE_NAME = "sqooptest";
-  static final String TABLE_NAME = "EMPLOYEES_PG";
-  static final String CONNECT_STRING = HOST_URL + DATABASE_NAME;
-
-  @Before
-  public void setUp() {
-    LOG.debug("Setting up another postgresql test...");
-
-    SqoopOptions options = new SqoopOptions(CONNECT_STRING, TABLE_NAME);
-    options.setUsername(DATABASE_USER);
-
-    ConnManager manager = null;
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      manager = new PostgresqlManager(options);
-      connection = manager.getConnection();
-      connection.setAutoCommit(false);
-      st = connection.createStatement();
-
-      // create the database table and populate it with data. 
-
-      try {
-        // Try to remove the table first. DROP TABLE IF EXISTS didn't
-        // get added until pg 8.3, so we just use "DROP TABLE" and ignore
-        // any exception here if one occurs.
-        st.executeUpdate("DROP TABLE " + TABLE_NAME);
-      } catch (SQLException e) {
-        LOG.info("Couldn't drop table " + TABLE_NAME + " (ok)");
-        LOG.info(e.toString());
-        // Now we need to reset the transaction.
-        connection.rollback();
-      }
-
-      st.executeUpdate("CREATE TABLE " + TABLE_NAME + " ("
-          + "id INT NOT NULL PRIMARY KEY, "
-          + "name VARCHAR(24) NOT NULL, "
-          + "start_date DATE, "
-          + "salary FLOAT, "
-          + "dept VARCHAR(32))");
-
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "1,'Aaron','2009-05-14',1000000.00,'engineering')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "2,'Bob','2009-04-20',400.00,'sales')");
-      st.executeUpdate("INSERT INTO " + TABLE_NAME + " VALUES("
-          + "3,'Fred','2009-01-23',15.00,'marketing')");
-      connection.commit();
-    } catch (SQLException sqlE) {
-      LOG.error("Encountered SQL Exception: " + sqlE);
-      sqlE.printStackTrace();
-      fail("SQLException when running test setUp(): " + sqlE);
-    } finally {
-      try {
-        if (null != st) {
-          st.close();
-        }
-
-        if (null != manager) {
-          manager.close();
-        }
-      } catch (SQLException sqlE) {
-        LOG.warn("Got SQLException when closing connection: " + sqlE);
-      }
-    }
-
-    LOG.debug("setUp complete.");
-  }
-
-
-  private String [] getArgv(boolean isDirect) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    CommonArgs.addHadoopFlags(args);
-
-    args.add("--table");
-    args.add(TABLE_NAME);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(CONNECT_STRING);
-    args.add("--username");
-    args.add(DATABASE_USER);
-    args.add("--where");
-    args.add("id > 1");
-
-    if (isDirect) {
-      args.add("--direct");
-    }
-
-    return args.toArray(new String[0]);
-  }
-
-  private void doImportAndVerify(boolean isDirect, String [] expectedResults)
-      throws IOException {
-
-    Path warehousePath = new Path(this.getWarehouseDir());
-    Path tablePath = new Path(warehousePath, TABLE_NAME);
-
-    Path filePath;
-    if (isDirect) {
-      filePath = new Path(tablePath, "data-00000");
-    } else {
-      filePath = new Path(tablePath, "part-m-00000");
-    }
-
-    File tableFile = new File(tablePath.toString());
-    if (tableFile.exists() && tableFile.isDirectory()) {
-      // remove the directory before running the import.
-      FileListing.recursiveDeleteDir(tableFile);
-    }
-
-    String [] argv = getArgv(isDirect);
-    try {
-      runImport(argv);
-    } catch (IOException ioe) {
-      LOG.error("Got IOException during import: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    }
-
-    File f = new File(filePath.toString());
-    assertTrue("Could not find imported data file, " + f, f.exists());
-    BufferedReader r = null;
-    try {
-      // Read through the file and make sure it's all there.
-      r = new BufferedReader(new InputStreamReader(new FileInputStream(f)));
-      for (String expectedLine : expectedResults) {
-        assertEquals(expectedLine, r.readLine());
-      }
-    } catch (IOException ioe) {
-      LOG.error("Got IOException verifying results: " + ioe.toString());
-      ioe.printStackTrace();
-      fail(ioe.toString());
-    } finally {
-      IOUtils.closeStream(r);
-    }
-  }
-
-  @Test
-  public void testJdbcBasedImport() throws IOException {
-    String [] expectedResults = {
-        "2,Bob,2009-04-20,400.0,sales",
-        "3,Fred,2009-01-23,15.0,marketing"
-    };
-
-    doImportAndVerify(false, expectedResults);
-  }
-
-  @Test
-  public void testDirectImport() throws IOException {
-    String [] expectedResults = {
-        "2,Bob,2009-04-20,400,sales",
-        "3,Fred,2009-01-23,15,marketing"
-    };
-
-    doImportAndVerify(true, expectedResults);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestHsqldbManager.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestHsqldbManager.java
deleted file mode 100644
index a0b6ee7..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestHsqldbManager.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.sql.SQLException;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-
-/**
- * Test HsqldbManager-specific functionality that overrides SqlManager behavior
- *
- * 
- */
-public class TestHsqldbManager extends TestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestHsqldbManager.class.getName());
-
-  // instance variables populated during setUp, used during tests
-  private HsqldbTestServer testServer;
-  private ConnManager manager;
-
-  @Before
-  public void setUp() {
-    testServer = new HsqldbTestServer();
-    try {
-      testServer.resetServer();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    } catch (ClassNotFoundException cnfe) {
-      LOG.error("Could not find class for db driver: " + cnfe.toString());
-      fail("Could not find class for db driver: " + cnfe.toString());
-    }
-
-    manager = testServer.getManager();
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  // note: hsql returns only the "PUBLIC" schema name; not individual user db names.
-  @Test
-  public void testListDatabases() {
-    String [] databases = manager.listDatabases();
-
-    assertNotNull("manager returned no database list", databases);
-    assertEquals("Database list should be length 1", 1, databases.length);
-    assertEquals(HsqldbTestServer.getSchemaName(), databases[0]);
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestSqlManager.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestSqlManager.java
deleted file mode 100644
index 2c9d6c8..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/manager/TestSqlManager.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.manager;
-
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.ResultSetMetaData;
-import java.sql.SQLException;
-import java.sql.Types;
-import java.util.Map;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-
-/**
- * Test methods of the generic SqlManager implementation.
- *
- * 
- *
- */
-public class TestSqlManager extends TestCase {
-
-  public static final Log LOG = LogFactory.getLog(TestSqlManager.class.getName());
-
-  /** the name of a table that doesn't exist. */
-  static final String MISSING_TABLE = "MISSING_TABLE";
-
-  // instance variables populated during setUp, used during tests
-  private HsqldbTestServer testServer;
-  private ConnManager manager;
-
-  @Before
-  public void setUp() {
-    testServer = new HsqldbTestServer();
-    try {
-      testServer.resetServer();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    } catch (ClassNotFoundException cnfe) {
-      LOG.error("Could not find class for db driver: " + cnfe.toString());
-      fail("Could not find class for db driver: " + cnfe.toString());
-    }
-
-    manager = testServer.getManager();
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  @Test
-  public void testListColNames() {
-    String [] colNames = manager.getColumnNames(HsqldbTestServer.getTableName());
-    assertNotNull("manager returned no colname list", colNames);
-    assertEquals("Table list should be length 2", 2, colNames.length);
-    String [] knownFields = HsqldbTestServer.getFieldNames();
-    for (int i = 0; i < colNames.length; i++) {
-      assertEquals(knownFields[i], colNames[i]);
-    }
-  }
-
-  @Test
-  public void testListColTypes() {
-    Map<String, Integer> types = manager.getColumnTypes(HsqldbTestServer.getTableName());
-
-    assertNotNull("manager returned no types map", types);
-    assertEquals("Map should be size=2", 2, types.size());
-    assertEquals(types.get("INTFIELD1").intValue(), Types.INTEGER);
-    assertEquals(types.get("INTFIELD2").intValue(), Types.INTEGER);
-  }
-
-  @Test
-  public void testMissingTableColNames() {
-    String [] colNames = manager.getColumnNames(MISSING_TABLE);
-    assertNull("No column names should be returned for missing table", colNames);
-  }
-
-  @Test
-  public void testMissingTableColTypes() {
-    Map<String, Integer> colTypes = manager.getColumnTypes(MISSING_TABLE);
-    assertNull("No column types should be returned for missing table", colTypes);
-  }
-
-  @Test
-  public void testListTables() {
-    String [] tables = manager.listTables();
-    for (String table : tables) {
-      System.err.println("Got table: " + table);
-    }
-    assertNotNull("manager returned no table list", tables);
-    assertEquals("Table list should be length 1", 1, tables.length);
-    assertEquals(HsqldbTestServer.getTableName(), tables[0]);
-  }
-
-  // constants related to testReadTable()
-  final static int EXPECTED_NUM_ROWS = 4;
-  final static int EXPECTED_COL1_SUM = 16;
-  final static int EXPECTED_COL2_SUM = 20;
-
-  @Test
-  public void testReadTable() {
-    ResultSet results = null;
-    try {
-      results = manager.readTable(HsqldbTestServer.getTableName(),
-          HsqldbTestServer.getFieldNames());
-
-      assertNotNull("ResultSet from readTable() is null!", results);
-
-      ResultSetMetaData metaData = results.getMetaData();
-      assertNotNull("ResultSetMetadata is null in readTable()", metaData);
-
-      // ensure that we get the correct number of columns back
-      assertEquals("Number of returned columns was unexpected!", metaData.getColumnCount(),
-          HsqldbTestServer.getFieldNames().length);
-
-      // should get back 4 rows. They are:
-      // 1 2
-      // 3 4
-      // 5 6
-      // 7 8
-      // .. so while order isn't guaranteed, we should get back 16 on the left and 20 on the right.
-      int sumCol1 = 0, sumCol2 = 0, rowCount = 0;
-      while (results.next()) {
-        rowCount++;
-        sumCol1 += results.getInt(1);
-        sumCol2 += results.getInt(2);
-      }
-
-      assertEquals("Expected 4 rows back", EXPECTED_NUM_ROWS, rowCount);
-      assertEquals("Expected left sum of 16", EXPECTED_COL1_SUM, sumCol1);
-      assertEquals("Expected right sum of 20", EXPECTED_COL2_SUM, sumCol2);
-    } catch (SQLException sqlException) {
-      fail("SQL Exception: " + sqlException.toString());
-    } finally {
-      if (null != results) {
-        try {
-          results.close();
-        } catch (SQLException sqlE) {
-          fail("SQL Exception in ResultSet.close(): " + sqlE.toString());
-        }
-      }
-    }
-  }
-
-  @Test
-  public void testReadMissingTable() {
-    ResultSet results = null;
-    try {
-      String [] colNames = { "*" };
-      results = manager.readTable(MISSING_TABLE, colNames);
-      assertNull("Expected null resultset from readTable(MISSING_TABLE)", results);
-    } catch (SQLException sqlException) {
-      // we actually expect this. pass.
-    } finally {
-      if (null != results) {
-        try {
-          results.close();
-        } catch (SQLException sqlE) {
-          fail("SQL Exception in ResultSet.close(): " + sqlE.toString());
-        }
-      }
-    }
-  }
-
-  @Test
-  public void getPrimaryKeyFromMissingTable() {
-    String primaryKey = manager.getPrimaryKey(MISSING_TABLE);
-    assertNull("Expected null pkey for missing table", primaryKey);
-  }
-
-  @Test
-  public void getPrimaryKeyFromTableWithoutKey() {
-    String primaryKey = manager.getPrimaryKey(HsqldbTestServer.getTableName());
-    assertNull("Expected null pkey for table without key", primaryKey);
-  }
-
-  // constants for getPrimaryKeyFromTable()
-  static final String TABLE_WITH_KEY = "TABLE_WITH_KEY";
-  static final String KEY_FIELD_NAME = "KEYFIELD";
-
-  @Test
-  public void getPrimaryKeyFromTable() {
-    // first, create a table with a primary key
-    Connection conn = null;
-    try {
-      conn = testServer.getConnection();
-      PreparedStatement statement = conn.prepareStatement(
-          "CREATE TABLE " + TABLE_WITH_KEY + "(" + KEY_FIELD_NAME
-          + " INT NOT NULL PRIMARY KEY, foo INT)",
-          ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      statement.executeUpdate();
-    } catch (SQLException sqlException) {
-      fail("Could not create table with primary key: " + sqlException.toString());
-    } finally {
-      if (null != conn) {
-        try {
-          conn.close();
-        } catch (SQLException sqlE) {
-          LOG.warn("Got SQLException during close: " + sqlE.toString());
-        }
-      }
-    }
-
-    String primaryKey = manager.getPrimaryKey(TABLE_WITH_KEY);
-    assertEquals("Expected null pkey for table without key", primaryKey, KEY_FIELD_NAME);
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/MapredTests.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/MapredTests.java
deleted file mode 100644
index 948dddc..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/MapredTests.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import junit.framework.Test;
-import junit.framework.TestSuite;
-
-/**
- * All tests for Sqoop old mapred-api (org.apache.hadoop.sqoop.mapred)
- */
-public final class MapredTests {
-
-  private MapredTests() { }
-
-  public static Test suite() {
-    TestSuite suite = new TestSuite("Tests for org.apache.hadoop.sqoop.mapred");
-    suite.addTestSuite(TestAutoProgressMapRunner.class);
-    return suite;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/TestAutoProgressMapRunner.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/TestAutoProgressMapRunner.java
deleted file mode 100644
index 7b236e6..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapred/TestAutoProgressMapRunner.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapred;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.MapReduceBase;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobStatus;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.ManagerFactory;
-
-import junit.framework.TestCase;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-/**
- * Test the AutoProgressMapRunner implementation and prove that it makes
- * progress updates when the mapper itself isn't.
- */
-public class TestAutoProgressMapRunner extends TestCase {
-
-  /** Parameter: how long should each map() call sleep for? */
-  public static final String MAPPER_SLEEP_INTERVAL_KEY = "sqoop.test.mapper.sleep.ival";
-
-
-  /** Mapper that just sleeps for a configurable amount of time. */
-  public static class SleepingMapper<K1, V1, K2, V2> extends MapReduceBase
-      implements Mapper<K1, V1, K2, V2> {
-
-    private int sleepInterval;
-
-    public void configure(JobConf job) {
-      this.sleepInterval = job.getInt(MAPPER_SLEEP_INTERVAL_KEY, 100);
-    }
-
-    public void map(K1 k, V1 v, OutputCollector<K2, V2> out, Reporter r) throws IOException {
-      while (true) {
-        try {
-          Thread.sleep(this.sleepInterval);
-          break;
-        } catch (InterruptedException ie) {
-        }
-      }
-    }
-  }
-
-  /** Mapper that sleeps for 1 second then fails. */
-  public static class FailingMapper<K1, V1, K2, V2> extends MapReduceBase
-      implements Mapper<K1, V1, K2, V2> {
-
-    public void map(K1 k, V1 v, OutputCollector<K2, V2> out, Reporter r) throws IOException {
-      throw new IOException("Causing job failure.");
-    }
-  }
-
-  private final Path inPath  = new Path("./input");
-  private final Path outPath = new Path("./output");
-
-  private MiniMRCluster mr = null;
-  private FileSystem fs = null;
-
-  private final static int NUM_NODES = 1;
-
-  public void setUp() throws IOException {
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    fs = FileSystem.get(conf);
-    mr = new MiniMRCluster(NUM_NODES, fs.getUri().toString(), 1, null, null, new JobConf(conf));
-
-    // Create a file to use as a dummy input
-    DataOutputStream os = fs.create(new Path(inPath, "part-0"));
-    os.writeBytes("This is a line of text.");
-    os.close();
-  }
-
-  public void tearDown() throws IOException {
-    if (null != fs) {
-      fs.delete(inPath, true);
-      fs.delete(outPath, true);
-    }
-
-    if (null != mr) {
-      mr.shutdown();
-      this.mr = null;
-    }
-  }
-
-
-  /**
-   * Test that even if the mapper just sleeps, the auto-progress thread keeps it all alive
-   */
-  public void testBackgroundProgress() throws IOException {
-    // Report progress every 2.5 seconds.
-    final int REPORT_INTERVAL = 2500;
-
-    // Tasks need to report progress once every ten seconds.
-    final int TASK_KILL_TIMEOUT = 4 * REPORT_INTERVAL;
-
-    // Create and run the job.
-    JobConf job = mr.createJobConf();
-
-    // Set the task timeout to be pretty strict.
-    job.setInt("mapred.task.timeout", TASK_KILL_TIMEOUT);
-
-    // Set the mapper itself to block for long enough that it should be killed on its own.
-    job.setInt(MAPPER_SLEEP_INTERVAL_KEY, 2 * TASK_KILL_TIMEOUT);
-
-    // Report progress frequently..
-    job.setInt(AutoProgressMapRunner.SLEEP_INTERVAL_KEY, REPORT_INTERVAL);
-    job.setInt(AutoProgressMapRunner.REPORT_INTERVAL_KEY, REPORT_INTERVAL);
-
-    job.setMapRunnerClass(AutoProgressMapRunner.class);
-    job.setMapperClass(SleepingMapper.class);
-
-    job.setNumReduceTasks(0);
-    job.setNumMapTasks(1);
-
-    FileInputFormat.addInputPath(job, inPath);
-    FileOutputFormat.setOutputPath(job, outPath);
-
-    RunningJob runningJob = JobClient.runJob(job);
-    runningJob.waitForCompletion();
-
-    assertEquals("Sleep job failed!", JobStatus.SUCCEEDED, runningJob.getJobState());
-  }
-
-  /** Test that if the mapper bails early, we shut down the progress thread
-      in a timely fashion.
-    */
-  public void testEarlyExit() throws IOException {
-    JobConf job = mr.createJobConf();
-
-    final int REPORT_INTERVAL = 30000;
-
-    job.setInt(AutoProgressMapRunner.SLEEP_INTERVAL_KEY, REPORT_INTERVAL);
-    job.setInt(AutoProgressMapRunner.REPORT_INTERVAL_KEY, REPORT_INTERVAL);
-
-    job.setNumReduceTasks(0);
-    job.setNumMapTasks(1);
-
-    job.setInt("mapred.map.max.attempts", 1);
-
-    job.setMapRunnerClass(AutoProgressMapRunner.class);
-    job.setMapperClass(FailingMapper.class);
-
-    FileInputFormat.addInputPath(job, inPath);
-    FileOutputFormat.setOutputPath(job, outPath);
-
-    RunningJob runningJob = null;
-    long startTime = System.currentTimeMillis();
-    try {
-      runningJob = JobClient.runJob(job);
-      runningJob.waitForCompletion();
-      assertEquals("Failing job succeded!", JobStatus.FAILED, runningJob.getJobState());
-    } catch(IOException ioe) {
-      // Expected
-    }
-
-    long endTime = System.currentTimeMillis();
-    long duration = endTime - startTime;
-
-    assertTrue("Job took too long to clean up (" + duration + ")",
-        duration < (REPORT_INTERVAL * 2));
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/MapreduceTests.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/MapreduceTests.java
deleted file mode 100644
index 441fac9..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/MapreduceTests.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import junit.framework.Test;
-import junit.framework.TestSuite;
-
-/**
- * All tests for Sqoop new mapreduce-api (org.apache.hadoop.sqoop.mapreduce)
- */
-public final class MapreduceTests {
-
-  private MapreduceTests() { }
-
-  public static Test suite() {
-    TestSuite suite = new TestSuite("Tests for org.apache.hadoop.sqoop.mapreduce");
-    suite.addTestSuite(TestTextImportMapper.class);
-    suite.addTestSuite(TestImportJob.class);
-    return suite;
-  }
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestImportJob.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestImportJob.java
deleted file mode 100644
index 643e508..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestImportJob.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.sql.SQLException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.junit.Before;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.db.DBWritable;
-import org.apache.hadoop.sqoop.Sqoop;
-import org.apache.hadoop.sqoop.mapreduce.AutoProgressMapper;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.util.ToolRunner;
-
-/**
- * Test aspects of the DataDrivenImportJob class
- */
-public class TestImportJob extends ImportJobTestCase {
-
-  public void testFailedImportDueToIOException() throws IOException {
-    // Make sure that if a MapReduce job to do the import fails due
-    // to an IOException, we tell the user about it.
-
-    // Create a table to attempt to import.
-    createTableForColType("VARCHAR(32)", "'meep'");
-
-    // Make the output dir exist so we know the job will fail via IOException.
-    Path outputPath = new Path(new Path(getWarehouseDir()), getTableName());
-    FileSystem fs = FileSystem.getLocal(new Configuration());
-    fs.mkdirs(outputPath);
-
-    assertTrue(fs.exists(outputPath));
-
-    String [] argv = getArgv(true, new String [] { "DATA_COL0" });
-
-    Sqoop importer = new Sqoop();
-    try {
-      ToolRunner.run(importer, argv);
-      fail("Expected IOException running this job.");
-    } catch (Exception e) {
-      // In debug mode, IOException is wrapped in RuntimeException.
-      LOG.info("Got exceptional return (expected: ok). msg is: " + e);
-    }
-  }
-
-  // A mapper that is guaranteed to cause the task to fail.
-  public static class NullDereferenceMapper
-      extends AutoProgressMapper<LongWritable, DBWritable, Text, NullWritable> {
-
-    public void map(LongWritable key, DBWritable val, Context c)
-        throws IOException, InterruptedException {
-      String s = null;
-      s.length(); // This will throw a NullPointerException.
-    }
-  }
-
-  public void testFailedImportDueToJobFail() throws IOException {
-    // Test that if the job returns 'false' it still fails and informs
-    // the user.
-
-    // Create a table to attempt to import.
-    createTableForColType("VARCHAR(32)", "'meep'");
-
-    String [] argv = getArgv(true, new String [] { "DATA_COL0" });
-
-    // Use dependency injection to specify a mapper that we know
-    // will fail.
-    Configuration conf = new Configuration();
-    conf.setClass(DataDrivenImportJob.DATA_DRIVEN_MAPPER_KEY,
-        NullDereferenceMapper.class,
-        Mapper.class);
-
-    Sqoop importer = new Sqoop(conf);
-    try {
-      ToolRunner.run(importer, argv);
-      fail("Expected ImportException running this job.");
-    } catch (Exception e) {
-      // In debug mode, ImportException is wrapped in RuntimeException.
-      LOG.info("Got exceptional return (expected: ok). msg is: " + e);
-    }
-  }
-
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestTextImportMapper.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestTextImportMapper.java
deleted file mode 100644
index ad33018..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/mapreduce/TestTextImportMapper.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.mapreduce;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.lib.db.DBWritable;
-import org.apache.hadoop.mrunit.mapreduce.MapDriver;
-
-import junit.framework.TestCase;
-
-/**
- * Test the TextImportMapper
- */
-public class TestTextImportMapper extends TestCase {
-
-
-  static class DummyDBWritable implements DBWritable {
-    long field;
-
-    public DummyDBWritable(final long val) {
-      this.field = val;
-    }
-
-    public void readFields(DataInput in) throws IOException {
-      field = in.readLong();
-    }
-
-    public void write(DataOutput out) throws IOException {
-      out.writeLong(field);
-    }
-
-    public void readFields(ResultSet rs) throws SQLException {
-      field = rs.getLong(1);
-    }
-
-    public void write(PreparedStatement s) throws SQLException {
-      s.setLong(1, field);
-    }
-
-    public String toString() {
-      return "" + field;
-    }
-  }
-
-  public void testTextImport() {
-    TextImportMapper m = new TextImportMapper();
-    MapDriver<LongWritable, DBWritable, Text, NullWritable> driver =
-      new MapDriver<LongWritable, DBWritable, Text, NullWritable>(m);
-
-    driver.withInput(new LongWritable(0), new DummyDBWritable(42))
-          .withOutput(new Text("42"), NullWritable.get())
-          .runTest();
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestClassWriter.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestClassWriter.java
deleted file mode 100644
index 863de48..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestClassWriter.java
+++ /dev/null
@@ -1,318 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.orm;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.Statement;
-import java.sql.SQLException;
-import java.util.Enumeration;
-import java.util.jar.JarEntry;
-import java.util.jar.JarInputStream;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.testutil.DirUtil;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-
-/**
- * Test that the ClassWriter generates Java classes based on the given table,
- * which compile.
- */
-public class TestClassWriter extends TestCase {
-
-  public static final Log LOG =
-      LogFactory.getLog(TestClassWriter.class.getName());
-
-  // instance variables populated during setUp, used during tests
-  private HsqldbTestServer testServer;
-  private ConnManager manager;
-  private SqoopOptions options;
-
-  @Before
-  public void setUp() {
-    testServer = new HsqldbTestServer();
-    org.apache.log4j.Logger root = org.apache.log4j.Logger.getRootLogger();
-    root.setLevel(org.apache.log4j.Level.DEBUG);
-    try {
-      testServer.resetServer();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    } catch (ClassNotFoundException cnfe) {
-      LOG.error("Could not find class for db driver: " + cnfe.toString());
-      fail("Could not find class for db driver: " + cnfe.toString());
-    }
-
-    manager = testServer.getManager();
-    options = testServer.getSqoopOptions();
-
-    // sanity check: make sure we're in a tmp dir before we blow anything away.
-    assertTrue("Test generates code in non-tmp dir!",
-        CODE_GEN_DIR.startsWith(ImportJobTestCase.TEMP_BASE_DIR));
-    assertTrue("Test generates jars in non-tmp dir!",
-        JAR_GEN_DIR.startsWith(ImportJobTestCase.TEMP_BASE_DIR));
-
-    // start out by removing these directories ahead of time
-    // to ensure that this is truly generating the code.
-    File codeGenDirFile = new File(CODE_GEN_DIR);
-    File classGenDirFile = new File(JAR_GEN_DIR);
-
-    if (codeGenDirFile.exists()) {
-      LOG.debug("Removing code gen dir: " + codeGenDirFile);
-      if (!DirUtil.deleteDir(codeGenDirFile)) {
-        LOG.warn("Could not delete " + codeGenDirFile + " prior to test");
-      }
-    }
-
-    if (classGenDirFile.exists()) {
-      LOG.debug("Removing class gen dir: " + classGenDirFile);
-      if (!DirUtil.deleteDir(classGenDirFile)) {
-        LOG.warn("Could not delete " + classGenDirFile + " prior to test");
-      }
-    }
-  }
-
-  @After
-  public void tearDown() {
-    try {
-      manager.close();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-  }
-
-  static final String CODE_GEN_DIR = ImportJobTestCase.TEMP_BASE_DIR + "sqoop/test/codegen";
-  static final String JAR_GEN_DIR = ImportJobTestCase.TEMP_BASE_DIR + "sqoop/test/jargen";
-
-  /**
-   * Run a test to verify that we can generate code and it emits the output files
-   * where we expect them.
-   */
-  private void runGenerationTest(String [] argv, String classNameToCheck) {
-    File codeGenDirFile = new File(CODE_GEN_DIR);
-    File classGenDirFile = new File(JAR_GEN_DIR);
-
-    try {
-      options.parse(argv);
-    } catch (InvalidOptionsException ioe) {
-      LOG.error("Could not parse options: " + ioe.toString());
-    }
-
-    CompilationManager compileMgr = new CompilationManager(options);
-    ClassWriter writer = new ClassWriter(options, manager, HsqldbTestServer.getTableName(),
-        compileMgr);
-
-    try {
-      writer.generate();
-      compileMgr.compile();
-      compileMgr.jar();
-    } catch (IOException ioe) {
-      LOG.error("Got IOException: " + ioe.toString());
-      fail("Got IOException: " + ioe.toString());
-    }
-
-    String classFileNameToCheck = classNameToCheck.replace('.', File.separatorChar);
-    LOG.debug("Class file to check for: " + classFileNameToCheck);
-
-    // check that all the files we expected to generate (.java, .class, .jar) exist.
-    File tableFile = new File(codeGenDirFile, classFileNameToCheck + ".java");
-    assertTrue("Cannot find generated source file for table!", tableFile.exists());
-    LOG.debug("Found generated source: " + tableFile);
-
-    File tableClassFile = new File(classGenDirFile, classFileNameToCheck + ".class");
-    assertTrue("Cannot find generated class file for table!", tableClassFile.exists());
-    LOG.debug("Found generated class: " + tableClassFile);
-
-    File jarFile = new File(compileMgr.getJarFilename());
-    assertTrue("Cannot find compiled jar", jarFile.exists());
-    LOG.debug("Found generated jar: " + jarFile);
-
-    // check that the .class file made it into the .jar by enumerating 
-    // available entries in the jar file.
-    boolean foundCompiledClass = false;
-    try {
-      JarInputStream jis = new JarInputStream(new FileInputStream(jarFile));
-
-      LOG.debug("Jar file has entries:");
-      while (true) {
-        JarEntry entry = jis.getNextJarEntry();
-        if (null == entry) {
-          // no more entries.
-          break;
-        }
-
-        if (entry.getName().equals(classFileNameToCheck + ".class")) {
-          foundCompiledClass = true;
-          LOG.debug(" * " + entry.getName());
-        } else {
-          LOG.debug("   " + entry.getName());
-        }
-      }
-
-      jis.close();
-    } catch (IOException ioe) {
-      fail("Got IOException iterating over Jar file: " + ioe.toString());
-    }
-
-    assertTrue("Cannot find .class file " + classFileNameToCheck + ".class in jar file",
-        foundCompiledClass);
-
-    LOG.debug("Found class in jar - test success!");
-  }
-
-  /**
-   * Test that we can generate code. Test that we can redirect the --outdir and --bindir too.
-   */
-  @Test
-  public void testCodeGen() {
-
-    // Set the option strings in an "argv" to redirect our srcdir and bindir
-    String [] argv = {
-        "--bindir",
-        JAR_GEN_DIR,
-        "--outdir",
-        CODE_GEN_DIR
-    };
-
-    runGenerationTest(argv, HsqldbTestServer.getTableName());
-  }
-
-  private static final String OVERRIDE_CLASS_NAME = "override";
-
-  /**
-   * Test that we can generate code with a custom class name
-   */
-  @Test
-  public void testSetClassName() {
-
-    // Set the option strings in an "argv" to redirect our srcdir and bindir
-    String [] argv = {
-        "--bindir",
-        JAR_GEN_DIR,
-        "--outdir",
-        CODE_GEN_DIR,
-        "--class-name",
-        OVERRIDE_CLASS_NAME
-    };
-
-    runGenerationTest(argv, OVERRIDE_CLASS_NAME);
-  }
-
-  private static final String OVERRIDE_CLASS_AND_PACKAGE_NAME = "override.pkg.prefix.classname";
-
-  /**
-   * Test that we can generate code with a custom class name that includes a package
-   */
-  @Test
-  public void testSetClassAndPackageName() {
-
-    // Set the option strings in an "argv" to redirect our srcdir and bindir
-    String [] argv = {
-        "--bindir",
-        JAR_GEN_DIR,
-        "--outdir",
-        CODE_GEN_DIR,
-        "--class-name",
-        OVERRIDE_CLASS_AND_PACKAGE_NAME
-    };
-
-    runGenerationTest(argv, OVERRIDE_CLASS_AND_PACKAGE_NAME);
-  }
- 
-  private static final String OVERRIDE_PACKAGE_NAME = "special.userpackage.name";
-
-  /**
-   * Test that we can generate code with a custom class name that includes a package
-   */
-  @Test
-  public void testSetPackageName() {
-
-    // Set the option strings in an "argv" to redirect our srcdir and bindir
-    String [] argv = {
-        "--bindir",
-        JAR_GEN_DIR,
-        "--outdir",
-        CODE_GEN_DIR,
-        "--package-name",
-        OVERRIDE_PACKAGE_NAME
-    };
-
-    runGenerationTest(argv, OVERRIDE_PACKAGE_NAME + "." + HsqldbTestServer.getTableName());
-  }
-
-
-  // Test the SQL identifier -> Java identifier conversion.
-  @Test
-  public void testIdentifierConversion() {
-    assertNull(ClassWriter.getIdentifierStrForChar(' '));
-    assertNull(ClassWriter.getIdentifierStrForChar('\t'));
-    assertNull(ClassWriter.getIdentifierStrForChar('\r'));
-    assertNull(ClassWriter.getIdentifierStrForChar('\n'));
-    assertEquals("x", ClassWriter.getIdentifierStrForChar('x'));
-    assertEquals("_", ClassWriter.getIdentifierStrForChar('-'));
-    assertEquals("_", ClassWriter.getIdentifierStrForChar('_'));
-
-    assertEquals("foo", ClassWriter.toIdentifier("foo"));
-    assertEquals("_class", ClassWriter.toIdentifier("class"));
-    assertEquals("_class", ClassWriter.toIdentifier("cla ss"));
-    assertEquals("_int", ClassWriter.toIdentifier("int"));
-    assertEquals("thisismanywords", ClassWriter.toIdentifier("this is many words"));
-    assertEquals("_9isLegalInSql", ClassWriter.toIdentifier("9isLegalInSql"));
-    assertEquals("___", ClassWriter.toIdentifier("___"));
-  }
-
-  @Test
-  public void testWeirdColumnNames() throws SQLException {
-    // Recreate the table with column names that aren't legal Java identifiers.
-    String tableName = HsqldbTestServer.getTableName();
-    Connection connection = testServer.getConnection();
-    Statement st = connection.createStatement();
-    st.executeUpdate("DROP TABLE " + tableName + " IF EXISTS");
-    st.executeUpdate("CREATE TABLE " + tableName + " (class INT, \"9field\" INT)");
-    st.executeUpdate("INSERT INTO " + tableName + " VALUES(42, 41)");
-    connection.commit();
-    connection.close();
-
-    String [] argv = {
-        "--bindir",
-        JAR_GEN_DIR,
-        "--outdir",
-        CODE_GEN_DIR,
-        "--package-name",
-        OVERRIDE_PACKAGE_NAME
-    };
-
-    runGenerationTest(argv, OVERRIDE_PACKAGE_NAME + "." + HsqldbTestServer.getTableName());
-  }
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestParseMethods.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestParseMethods.java
deleted file mode 100644
index ff5e5bd..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/orm/TestParseMethods.java
+++ /dev/null
@@ -1,183 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.orm;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.mapred.RawKeyTextOutputFormat;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.testutil.CommonArgs;
-import org.apache.hadoop.sqoop.testutil.HsqldbTestServer;
-import org.apache.hadoop.sqoop.testutil.ImportJobTestCase;
-import org.apache.hadoop.sqoop.testutil.ReparseMapper;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-/**
- * Test that the parse() methods generated in user SqoopRecord implementations
- * work.
- */
-public class TestParseMethods extends ImportJobTestCase {
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @return the argv as an array of strings.
-   */
-  private String [] getArgv(boolean includeHadoopFlags, String fieldTerminator, 
-      String lineTerminator, String encloser, String escape, boolean encloserRequired) {
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(getTableName());
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--as-textfile");
-    args.add("--split-by");
-    args.add("DATA_COL0"); // always split by first column.
-    args.add("--fields-terminated-by");
-    args.add(fieldTerminator);
-    args.add("--lines-terminated-by");
-    args.add(lineTerminator);
-    args.add("--escaped-by");
-    args.add(escape);
-    if (encloserRequired) {
-      args.add("--enclosed-by");
-    } else {
-      args.add("--optionally-enclosed-by");
-    }
-    args.add(encloser);
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  public void runParseTest(String fieldTerminator, String lineTerminator, String encloser,
-      String escape, boolean encloseRequired) throws IOException {
-
-    ClassLoader prevClassLoader = null;
-
-    String [] argv = getArgv(true, fieldTerminator, lineTerminator, encloser, escape,
-        encloseRequired);
-    runImport(argv);
-    try {
-      SqoopOptions opts = new SqoopOptions();
-
-      String tableClassName = getTableName();
-
-      opts.parse(getArgv(false, fieldTerminator, lineTerminator, encloser, escape,
-          encloseRequired));
-
-      CompilationManager compileMgr = new CompilationManager(opts);
-      String jarFileName = compileMgr.getJarFilename();
-
-      // make sure the user's class is loaded into our address space.
-      prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, tableClassName);
-
-      JobConf job = new JobConf();
-      job.setJar(jarFileName);
-
-      // Tell the job what class we're testing.
-      job.set(ReparseMapper.USER_TYPE_NAME_KEY, tableClassName);
-
-      // use local mode in the same JVM.
-      job.set("mapred.job.tracker", "local");
-      job.set("fs.default.name", "file:///");
-
-      String warehouseDir = getWarehouseDir();
-      Path warehousePath = new Path(warehouseDir);
-      Path inputPath = new Path(warehousePath, getTableName());
-      Path outputPath = new Path(warehousePath, getTableName() + "-out");
-
-      job.setMapperClass(ReparseMapper.class);
-      job.setNumReduceTasks(0);
-      FileInputFormat.addInputPath(job, inputPath);
-      FileOutputFormat.setOutputPath(job, outputPath);
-
-      job.setOutputKeyClass(Text.class);
-      job.setOutputValueClass(NullWritable.class);
-      job.setOutputFormat(RawKeyTextOutputFormat.class);
-
-      JobClient.runJob(job);
-    } catch (InvalidOptionsException ioe) {
-      fail(ioe.toString());
-    } finally {
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  public void testDefaults() throws IOException {
-    String [] types = { "INTEGER", "VARCHAR(32)", "INTEGER" };
-    String [] vals = { "64", "'foo'", "128" };
-
-    createTableWithColTypes(types, vals);
-    runParseTest(",", "\\n", "\\\"", "\\", false);
-  }
-
-  public void testRequiredEnclose() throws IOException {
-    String [] types = { "INTEGER", "VARCHAR(32)", "INTEGER" };
-    String [] vals = { "64", "'foo'", "128" };
-
-    createTableWithColTypes(types, vals);
-    runParseTest(",", "\\n", "\\\"", "\\", true);
-  }
-
-  public void testStringEscapes() throws IOException {
-    String [] types = { "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)", "VARCHAR(32)" };
-    String [] vals = { "'foo'", "'foo,bar'", "'foo''bar'", "'foo\\bar'", "'foo,bar''baz'" };
-
-    createTableWithColTypes(types, vals);
-    runParseTest(",", "\\n", "\\\'", "\\", false);
-  }
-
-  public void testNumericTypes() throws IOException {
-    String [] types = { "INTEGER", "REAL", "FLOAT", "DATE", "TIME",
-        "TIMESTAMP", "NUMERIC", "BOOLEAN" };
-    String [] vals = { "42", "36.0", "127.1", "'2009-07-02'", "'11:24:00'",
-        "'2009-08-13 20:32:00.1234567'", "92104916282869291837672829102857271948687.287475322",
-        "true" };
-    
-    createTableWithColTypes(types, vals);
-    runParseTest(",", "\\n", "\\\'", "\\", false);
-  }
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/BaseSqoopTestCase.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/BaseSqoopTestCase.java
deleted file mode 100644
index f170df9..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/BaseSqoopTestCase.java
+++ /dev/null
@@ -1,286 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.File;
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.log4j.BasicConfigurator;
-import org.junit.After;
-import org.junit.Before;
-
-import org.apache.hadoop.sqoop.manager.ConnManager;
-
-import junit.framework.TestCase;
-
-/**
- * Class that implements common methods required for tests
- */
-public class BaseSqoopTestCase extends TestCase {
-
-  public static final Log LOG = LogFactory.getLog(BaseSqoopTestCase.class.getName());
-
-  /** Base directory for all temporary data */
-  public static final String TEMP_BASE_DIR;
-
-  /** Where to import table data to in the local filesystem for testing */
-  public static final String LOCAL_WAREHOUSE_DIR;
-
-  // Initializer for the above
-  static {
-    String tmpDir = System.getProperty("test.build.data", "/tmp/");
-    if (!tmpDir.endsWith(File.separator)) {
-      tmpDir = tmpDir + File.separator;
-    }
-
-    TEMP_BASE_DIR = tmpDir;
-    LOCAL_WAREHOUSE_DIR = TEMP_BASE_DIR + "sqoop/warehouse";
-  }
-
-  // Used if a test manually sets the table name to be used.
-  private String curTableName;
-
-  protected void setCurTableName(String curName) {
-    this.curTableName = curName;
-  }
-
-  /**
-   * Because of how classloading works, we don't actually want to name
-   * all the tables the same thing -- they'll actually just use the same
-   * implementation of the Java class that was classloaded before. So we
-   * use this counter to uniquify table names.
-   */
-  private static int tableNum = 0;
-
-  /** When creating sequentially-identified tables, what prefix should
-   *  be applied to these tables?
-   */
-  protected String getTablePrefix() {
-    return "SQOOP_TABLE_";
-  }
-
-  protected String getTableName() {
-    if (null != curTableName) {
-      return curTableName;
-    } else {
-      return getTablePrefix() + Integer.toString(tableNum);
-    }
-  }
-
-  protected String getWarehouseDir() {
-    return LOCAL_WAREHOUSE_DIR;
-  }
-
-  private String [] colNames;
-  protected String [] getColNames() {
-    return colNames;
-  }
-
-  protected HsqldbTestServer getTestServer() {
-    return testServer;
-  }
-
-  protected ConnManager getManager() {
-    return manager;
-  }
-
-  // instance variables populated during setUp, used during tests
-  private HsqldbTestServer testServer;
-  private ConnManager manager;
-
-  private static boolean isLog4jConfigured = false;
-
-  protected void incrementTableNum() {
-    tableNum++;
-  }
-
-  @Before
-  public void setUp() {
-
-    incrementTableNum();
-
-    if (!isLog4jConfigured) {
-      BasicConfigurator.configure();
-      isLog4jConfigured = true;
-      LOG.info("Configured log4j with console appender.");
-    }
-
-    testServer = new HsqldbTestServer();
-    try {
-      testServer.resetServer();
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    } catch (ClassNotFoundException cnfe) {
-      LOG.error("Could not find class for db driver: " + cnfe.toString());
-      fail("Could not find class for db driver: " + cnfe.toString());
-    }
-
-    manager = testServer.getManager();
-  }
-
-  @After
-  public void tearDown() {
-    setCurTableName(null); // clear user-override table name.
-
-    try {
-      if (null != manager) {
-        manager.close();
-      }
-    } catch (SQLException sqlE) {
-      LOG.error("Got SQLException: " + sqlE.toString());
-      fail("Got SQLException: " + sqlE.toString());
-    }
-
-  }
-
-  static final String BASE_COL_NAME = "DATA_COL";
-
-  /**
-   * Create a table with a set of columns and add a row of values.
-   * @param colTypes the types of the columns to make
-   * @param vals the SQL text for each value to insert
-   */
-  protected void createTableWithColTypes(String [] colTypes, String [] vals) {
-    Connection conn = null;
-    try {
-      conn = getTestServer().getConnection();
-      PreparedStatement statement = conn.prepareStatement(
-          "DROP TABLE " + getTableName() + " IF EXISTS",
-          ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      statement.executeUpdate();
-      statement.close();
-
-      String columnDefStr = "";
-      String columnListStr = "";
-      String valueListStr = "";
-
-      String [] myColNames = new String[colTypes.length];
-
-      for (int i = 0; i < colTypes.length; i++) {
-        String colName = BASE_COL_NAME + Integer.toString(i);
-        columnDefStr += colName + " " + colTypes[i];
-        columnListStr += colName;
-        valueListStr += vals[i];
-        myColNames[i] = colName;
-        if (i < colTypes.length - 1) {
-          columnDefStr += ", ";
-          columnListStr += ", ";
-          valueListStr += ", ";
-        }
-      }
-
-      statement = conn.prepareStatement(
-          "CREATE TABLE " + getTableName() + "(" + columnDefStr + ")",
-          ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      statement.executeUpdate();
-      statement.close();
-
-      statement = conn.prepareStatement(
-          "INSERT INTO " + getTableName() + "(" + columnListStr + ")"
-          + " VALUES(" + valueListStr + ")",
-          ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-      statement.executeUpdate();
-      statement.close();
-      conn.commit();
-      this.colNames = myColNames;
-    } catch (SQLException sqlException) {
-      fail("Could not create table: " + sqlException.toString());
-    } finally {
-      if (null != conn) {
-        try {
-          conn.close();
-        } catch (SQLException sqlE) {
-          LOG.warn("Got SQLException during close: " + sqlE.toString());
-        }
-      }
-    }
-  }
-
-  /**
-   * Create a table with a single column and put a data element in it.
-   * @param colType the type of the column to create
-   * @param val the value to insert (reformatted as a string)
-   */
-  protected void createTableForColType(String colType, String val) {
-    String [] types = { colType };
-    String [] vals = { val };
-
-    createTableWithColTypes(types, vals);
-  }
-
-  protected Path getTablePath() {
-    Path warehousePath = new Path(getWarehouseDir());
-    Path tablePath = new Path(warehousePath, getTableName());
-    return tablePath;
-  }
-
-  protected Path getDataFilePath() {
-    return new Path(getTablePath(), "part-m-00000");
-  }
-
-  protected void removeTableDir() {
-    File tableDirFile = new File(getTablePath().toString());
-    if (tableDirFile.exists()) {
-      // Remove the director where the table will be imported to,
-      // prior to running the MapReduce job.
-      if (!DirUtil.deleteDir(tableDirFile)) {
-        LOG.warn("Could not delete table directory: " + tableDirFile.getAbsolutePath());
-      }
-    }
-  }
-
-  /**
-   * verify that the single-column single-row result can be read back from the db.
-   */
-  protected void verifyReadback(int colNum, String expectedVal) {
-    ResultSet results = null;
-    try {
-      results = getManager().readTable(getTableName(), getColNames());
-      assertNotNull("Null results from readTable()!", results);
-      assertTrue("Expected at least one row returned", results.next());
-      String resultVal = results.getString(colNum);
-      if (null != expectedVal) {
-        assertNotNull("Expected non-null result value", resultVal);
-      }
-
-      assertEquals("Error reading inserted value back from db", expectedVal, resultVal);
-      assertFalse("Expected at most one row returned", results.next());
-    } catch (SQLException sqlE) {
-      fail("Got SQLException: " + sqlE.toString());
-    } finally {
-      if (null != results) {
-        try {
-          results.close();
-        } catch (SQLException sqlE) {
-          fail("Got SQLException in resultset.close(): " + sqlE.toString());
-        }
-      }
-    }
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/CommonArgs.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/CommonArgs.java
deleted file mode 100644
index 7763bac..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/CommonArgs.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.util.List;
-
-/**
- * Sets common arguments to Sqoop sub-instances for testing.
- */
-public final class CommonArgs {
-
-  private CommonArgs() {
-  }
-
-  /** Craft a list of arguments that are common to (virtually) all Sqoop programs
-   */
-  public static void addHadoopFlags(List<String> args) {
-    args.add("-D");
-    args.add("mapred.job.tracker=local");
-    args.add("-D");
-    args.add("mapred.map.tasks=1");
-    args.add("-D");
-    args.add("fs.default.name=file:///");
-    args.add("-D");
-    args.add("jobclient.completion.poll.interval=50");
-    args.add("-D");
-    args.add("jobclient.progress.monitor.poll.interval=50");
-  }
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/DirUtil.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/DirUtil.java
deleted file mode 100644
index 6d39f1a..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/DirUtil.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.File;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Misc directory operations
- * 
- *
- */
-public final class DirUtil {
-
-  public static final Log LOG = LogFactory.getLog(DirUtil.class.getName());
-
-  /**
-   * recursively delete a dir and its children.
-   * @param dir
-   * @return true on succesful removal of a dir
-   */
-  public static boolean deleteDir(File dir) {
-    if (dir.isDirectory()) {
-      String [] children = dir.list();
-      for (int i = 0; i < children.length; i++) {
-        File f = new File(dir, children[i]);
-        boolean success = deleteDir(f);
-        if (!success) {
-          LOG.warn("Could not delete " + f.getAbsolutePath());
-          return false;
-        }
-      }
-    }
-
-    // The directory is now empty so delete it too.
-    LOG.debug("Removing: " + dir);
-    return dir.delete();
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ExportJobTestCase.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ExportJobTestCase.java
deleted file mode 100644
index 0a21f80..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ExportJobTestCase.java
+++ /dev/null
@@ -1,196 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.IOException;
-import java.sql.Connection;
-import java.sql.PreparedStatement;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.util.ToolRunner;
-
-import org.apache.hadoop.sqoop.Sqoop;
-
-/**
- * Class that implements common methods required for tests which export data
- * from HDFS to databases, to verify correct export
- */
-public class ExportJobTestCase extends BaseSqoopTestCase {
-
-  public static final Log LOG = LogFactory.getLog(ExportJobTestCase.class.getName());
-
-  protected String getTablePrefix() {
-    return "EXPORT_TABLE_";
-  }
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @param includeHadoopFlags if true, then include -D various.settings=values
-   * @return the argv as an array of strings.
-   */
-  protected String [] getArgv(boolean includeHadoopFlags, String... additionalArgv) {
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(getTableName());
-    args.add("--export-dir");
-    args.add(getTablePath().toString());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--fields-terminated-by");
-    args.add("\\t");
-    args.add("--lines-terminated-by");
-    args.add("\\n");
-
-
-    if (null != additionalArgv) {
-      for (String arg : additionalArgv) {
-        args.add(arg);
-      }
-    }
-
-    return args.toArray(new String[0]);
-  }
-
-  /** When exporting text columns, what should the text contain? */
-  protected String getMsgPrefix() {
-    return "textfield";
-  }
-
-
-  /** @return the minimum 'id' value in the table */
-  protected int getMinRowId() throws SQLException {
-    Connection conn = getTestServer().getConnection();
-    PreparedStatement statement = conn.prepareStatement(
-        "SELECT MIN(id) FROM " + getTableName(),
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    ResultSet rs = statement.executeQuery();
-    rs.next();
-    int minVal = rs.getInt(1);
-    rs.close();
-    statement.close();
-
-    return minVal;
-  }
-
-  /** @return the maximum 'id' value in the table */
-  protected int getMaxRowId() throws SQLException {
-    Connection conn = getTestServer().getConnection();
-    PreparedStatement statement = conn.prepareStatement(
-        "SELECT MAX(id) FROM " + getTableName(),
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    ResultSet rs = statement.executeQuery();
-    rs.next();
-    int maxVal = rs.getInt(1);
-    rs.close();
-    statement.close();
-
-    return maxVal;
-  }
-
-  /**
-   * Check that we got back the expected row set
-   * @param expectedNumRecords The number of records we expected to load
-   * into the database.
-   */
-  protected void verifyExport(int expectedNumRecords) throws IOException, SQLException {
-    Connection conn = getTestServer().getConnection();
-
-    LOG.info("Verifying export: " + getTableName());
-    // Check that we got back the correct number of records.
-    PreparedStatement statement = conn.prepareStatement(
-        "SELECT COUNT(*) FROM " + getTableName(),
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    ResultSet rs = statement.executeQuery();
-    rs.next();
-    int actualNumRecords = rs.getInt(1);
-    rs.close();
-    statement.close();
-
-    assertEquals("Got back unexpected row count", expectedNumRecords,
-        actualNumRecords);
-
-    // Check that we start with row 0.
-    int minVal = getMinRowId();
-    assertEquals("Minimum row was not zero", 0, minVal);
-
-    // Check that the last row we loaded is numRows - 1
-    int maxVal = getMaxRowId();
-    assertEquals("Maximum row had invalid id", expectedNumRecords - 1, maxVal);
-
-    // Check that the string values associated with these points match up.
-    statement = conn.prepareStatement("SELECT msg FROM " + getTableName()
-        + " WHERE id = " + minVal,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    rs = statement.executeQuery();
-    rs.next();
-    String minMsg = rs.getString(1);
-    rs.close();
-    statement.close();
-
-    assertEquals("Invalid msg field for min value", getMsgPrefix() + minVal, minMsg);
-
-    statement = conn.prepareStatement("SELECT msg FROM " + getTableName()
-        + " WHERE id = " + maxVal,
-        ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
-    rs = statement.executeQuery();
-    rs.next();
-    String maxMsg = rs.getString(1);
-    rs.close();
-    statement.close();
-
-    assertEquals("Invalid msg field for min value", getMsgPrefix() + maxVal, maxMsg);
-  }
-
-  /**
-   * Run a MapReduce-based export (using the argv provided to control execution).
-   * @return the generated jar filename
-   */
-  protected List<String> runExport(String [] argv) throws IOException {
-    // run the tool through the normal entry-point.
-    int ret;
-    List<String> generatedJars = null;
-    try {
-      Sqoop exporter = new Sqoop();
-      ret = ToolRunner.run(exporter, argv);
-      generatedJars = exporter.getGeneratedJarFiles();
-    } catch (Exception e) {
-      LOG.error("Got exception running Sqoop: " + e.toString());
-      e.printStackTrace();
-      ret = 1;
-    }
-
-    // expect a successful return.
-    if (0 != ret) {
-      throw new IOException("Failure during job; return status " + ret);
-    }
-
-    return generatedJars;
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/HsqldbTestServer.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/HsqldbTestServer.java
deleted file mode 100644
index 71d0882..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/HsqldbTestServer.java
+++ /dev/null
@@ -1,239 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.util.Arrays;
-
-import java.sql.Connection;
-import java.sql.DriverManager;
-import java.sql.SQLException;
-import java.sql.Statement;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.hsqldb.Server;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.manager.ConnManager;
-import org.apache.hadoop.sqoop.manager.HsqldbManager;
-
-/**
- * Create a simple hsqldb server and schema to use for testing.
- * 
- *
- */
-public class HsqldbTestServer {
-  public static final Log LOG =
-    LogFactory.getLog(HsqldbTestServer.class.getName());
-
-  // singleton server instance.
-  private static Server server;
-
-  private static final String DATABASE_NAME = "db1";
-
-  // hsqldb always capitalizes table and column names
-  private static final String DUMMY_TABLE_NAME = "TWOINTTABLE";
-  private static final String [] TWO_INT_TABLE_FIELDS = {"INTFIELD1", "INTFIELD2"};
-
-  private static final String EMPLOYEE_TABLE_NAME = "EMPLOYEES";
-
-  private static final String DB_URL = "jdbc:hsqldb:mem:" + DATABASE_NAME;
-  private static final String DRIVER_CLASS = "org.hsqldb.jdbcDriver";
-
-  // all user-created HSQLDB tables are in the "PUBLIC" schema when connected to a database.
-  private static final String HSQLDB_SCHEMA_NAME = "PUBLIC";
-
-  public static String getSchemaName() {
-    return HSQLDB_SCHEMA_NAME;
-  }
-
-  public static String [] getFieldNames() {
-    return Arrays.copyOf(TWO_INT_TABLE_FIELDS, TWO_INT_TABLE_FIELDS.length);
-  }
-
-  public static String getUrl() {
-    return DB_URL;
-  }
-
-  public static String getTableName() {
-    return DUMMY_TABLE_NAME;
-  }
-
-  public static String getDatabaseName() {
-    return DATABASE_NAME;
-  }
-
-  /**
-   * start the server
-   */
-  public void start() {
-    if (null == server) {
-      LOG.info("Starting new hsqldb server; database=" + DATABASE_NAME);
-      server = new Server();
-      server.putPropertiesFromString("database.0=mem:" + DATABASE_NAME
-          + ";no_system_exit=true");
-      server.start();
-    }
-  }
-
-  public Connection getConnection() throws SQLException {
-    try {
-      Class.forName(DRIVER_CLASS);
-    } catch (ClassNotFoundException cnfe) {
-      LOG.error("Could not get connection; driver class not found: " + DRIVER_CLASS);
-      return null;
-    }
-
-    Connection connection = DriverManager.getConnection(DB_URL);
-    connection.setAutoCommit(false);
-    return connection;
-  }
-
-  /**
-   * Create a table
-   */
-  public void createSchema() throws SQLException {
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = getConnection();
-
-      st = connection.createStatement();
-      st.executeUpdate("DROP TABLE " + DUMMY_TABLE_NAME + " IF EXISTS");
-      st.executeUpdate("CREATE TABLE " + DUMMY_TABLE_NAME + "(intField1 INT, intField2 INT)");
-
-      connection.commit();
-    } finally {
-      if (null != st) {
-        st.close();
-      }
-
-      if (null != connection) {
-        connection.close();
-      }
-    }
-  }
-
-  /**
-   * @return the sum of the integers in the first column of TWOINTTABLE.
-   */
-  public static int getFirstColSum() {
-    return 1 + 3 + 5 + 7;
-  }
-
-  /**
-   * Fill the table with some data
-   */
-  public void populateData() throws SQLException {
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = getConnection();
-
-      st = connection.createStatement();
-      st.executeUpdate("INSERT INTO " + DUMMY_TABLE_NAME + " VALUES(1, 8)");
-      st.executeUpdate("INSERT INTO " + DUMMY_TABLE_NAME + " VALUES(3, 6)");
-      st.executeUpdate("INSERT INTO " + DUMMY_TABLE_NAME + " VALUES(5, 4)");
-      st.executeUpdate("INSERT INTO " + DUMMY_TABLE_NAME + " VALUES(7, 2)");
-
-      connection.commit();
-    } finally {
-      if (null != st) {
-        st.close();
-      }
-
-      if (null != connection) {
-        connection.close();
-      }
-    }
-  }
-
-  public void createEmployeeDemo() throws SQLException, ClassNotFoundException {
-    Class.forName(DRIVER_CLASS);
-
-    Connection connection = null;
-    Statement st = null;
-
-    try {
-      connection = getConnection();
-
-      st = connection.createStatement();
-      st.executeUpdate("DROP TABLE " + EMPLOYEE_TABLE_NAME + " IF EXISTS");
-      st.executeUpdate("CREATE TABLE " + EMPLOYEE_TABLE_NAME
-          + "(emp_id INT NOT NULL PRIMARY KEY, name VARCHAR(64))");
-
-      st.executeUpdate("INSERT INTO " + EMPLOYEE_TABLE_NAME + " VALUES(1, 'Aaron')");
-      st.executeUpdate("INSERT INTO " + EMPLOYEE_TABLE_NAME + " VALUES(2, 'Joe')");
-      st.executeUpdate("INSERT INTO " + EMPLOYEE_TABLE_NAME + " VALUES(3, 'Jim')");
-      st.executeUpdate("INSERT INTO " + EMPLOYEE_TABLE_NAME + " VALUES(4, 'Lisa')");
-
-      connection.commit();
-    } finally {
-      if (null != st) {
-        st.close();
-      }
-
-      if (null != connection) {
-        connection.close();
-      }
-    }
-  }
-
-  /**
-   * Delete any existing tables.
-   */
-  public void dropExistingSchema() throws SQLException {
-    ConnManager mgr = getManager();
-    String [] tables = mgr.listTables();
-    if (null != tables) {
-      Connection conn = mgr.getConnection();
-      for (String table : tables) {
-        Statement s = conn.createStatement();
-        s.executeUpdate("DROP TABLE " + table);
-        conn.commit();
-        s.close();
-      }
-    }
-  }
-
-  /**
-   * Creates an hsqldb server, fills it with tables and data.
-   */
-  public void resetServer() throws ClassNotFoundException, SQLException {
-    start();
-    dropExistingSchema();
-    createSchema();
-    populateData();
-  }
-
-  public SqoopOptions getSqoopOptions() {
-    return new SqoopOptions(HsqldbTestServer.getUrl(),
-        HsqldbTestServer.getTableName());
-  }
-
-  public ConnManager getManager() {
-    return new HsqldbManager(getSqoopOptions());
-  }
-
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ImportJobTestCase.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ImportJobTestCase.java
deleted file mode 100644
index 91e8d82..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ImportJobTestCase.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.util.ToolRunner;
-
-import org.apache.hadoop.sqoop.SqoopOptions;
-import org.apache.hadoop.sqoop.Sqoop;
-import org.apache.hadoop.sqoop.SqoopOptions.InvalidOptionsException;
-import org.apache.hadoop.sqoop.orm.CompilationManager;
-import org.apache.hadoop.sqoop.util.ClassLoaderStack;
-
-/**
- * Class that implements common methods required for tests which import data
- * from SQL into HDFS and verify correct import.
- */
-public class ImportJobTestCase extends BaseSqoopTestCase {
-
-  public static final Log LOG = LogFactory.getLog(ImportJobTestCase.class.getName());
-
-  protected String getTablePrefix() {
-    return "IMPORT_TABLE_";
-  }
-
-  /**
-   * Create the argv to pass to Sqoop
-   * @param includeHadoopFlags if true, then include -D various.settings=values
-   * @param colNames the columns to import. If null, all columns are used.
-   * @return the argv as an array of strings.
-   */
-  protected String [] getArgv(boolean includeHadoopFlags, String [] colNames) {
-    if (null == colNames) {
-      colNames = getColNames();
-    }
-
-    String splitByCol = colNames[0];
-    String columnsString = "";
-    for (String col : colNames) {
-      columnsString += col + ",";
-    }
-
-    ArrayList<String> args = new ArrayList<String>();
-
-    if (includeHadoopFlags) {
-      CommonArgs.addHadoopFlags(args);
-    }
-
-    args.add("--table");
-    args.add(getTableName());
-    args.add("--columns");
-    args.add(columnsString);
-    args.add("--split-by");
-    args.add(splitByCol);
-    args.add("--warehouse-dir");
-    args.add(getWarehouseDir());
-    args.add("--connect");
-    args.add(HsqldbTestServer.getUrl());
-    args.add("--as-sequencefile");
-    args.add("--num-mappers");
-    args.add("1");
-
-    return args.toArray(new String[0]);
-  }
-
-  /**
-   * Do a MapReduce-based import of the table and verify that the results
-   * were imported as expected. (tests readFields(ResultSet) and toString())
-   * @param expectedVal the value we injected into the table.
-   * @param importCols the columns to import. If null, all columns are used.
-   */
-  protected void verifyImport(String expectedVal, String [] importCols) {
-
-    // paths to where our output file will wind up.
-    Path dataFilePath = getDataFilePath();
-
-    removeTableDir();
-
-    // run the tool through the normal entry-point.
-    int ret;
-    try {
-      Sqoop importer = new Sqoop();
-      ret = ToolRunner.run(importer, getArgv(true, importCols));
-    } catch (Exception e) {
-      LOG.error("Got exception running Sqoop: " + e.toString());
-      throw new RuntimeException(e);
-    }
-
-    // expect a successful return.
-    assertEquals("Failure during job", 0, ret);
-
-    SqoopOptions opts = new SqoopOptions();
-    try {
-      opts.parse(getArgv(false, importCols));
-    } catch (InvalidOptionsException ioe) {
-      fail(ioe.toString());
-    }
-    CompilationManager compileMgr = new CompilationManager(opts);
-    String jarFileName = compileMgr.getJarFilename();
-    ClassLoader prevClassLoader = null;
-    try {
-      prevClassLoader = ClassLoaderStack.addJarFile(jarFileName, getTableName());
-
-      // now actually open the file and check it
-      File f = new File(dataFilePath.toString());
-      assertTrue("Error: " + dataFilePath.toString() + " does not exist", f.exists());
-
-      Object readValue = SeqFileReader.getFirstValue(dataFilePath.toString());
-      // add trailing '\n' to expected value since SqoopRecord.toString() encodes the record delim
-      if (null == expectedVal) {
-        assertEquals("Error validating result from SeqFile", "null\n", readValue.toString());
-      } else {
-        assertEquals("Error validating result from SeqFile", expectedVal + "\n",
-            readValue.toString());
-      }
-    } catch (IOException ioe) {
-      fail("IOException: " + ioe.toString());
-    } finally {
-      if (null != prevClassLoader) {
-        ClassLoaderStack.setCurrentClassLoader(prevClassLoader);
-      }
-    }
-  }
-
-  /**
-   * Run a MapReduce-based import (using the argv provided to control execution).
-   */
-  protected void runImport(String [] argv) throws IOException {
-    removeTableDir();
-
-    // run the tool through the normal entry-point.
-    int ret;
-    try {
-      Sqoop importer = new Sqoop();
-      ret = ToolRunner.run(importer, argv);
-    } catch (Exception e) {
-      LOG.error("Got exception running Sqoop: " + e.toString());
-      e.printStackTrace();
-      ret = 1;
-    }
-
-    // expect a successful return.
-    if (0 != ret) {
-      throw new IOException("Failure during job; return status " + ret);
-    }
-  }
-
-}
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ReparseMapper.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ReparseMapper.java
deleted file mode 100644
index b54bb2a..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/ReparseMapper.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.MapReduceBase;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.sqoop.lib.RecordParser;
-import org.apache.hadoop.sqoop.lib.SqoopRecord;
-import org.apache.hadoop.util.ReflectionUtils;
-
-
-/**
- * Test harness mapper. Instantiate the user's specific type, parse() the input 
- * line of text, and throw an IOException if the output toString() line of text
- * differs.
- */
-public class ReparseMapper extends MapReduceBase
-    implements Mapper<LongWritable, Text, Text, NullWritable> {
-
-  public static final Log LOG = LogFactory.getLog(ReparseMapper.class.getName());
-
-  public static final String USER_TYPE_NAME_KEY = "sqoop.user.class";
-
-  private SqoopRecord userRecord;
-
-  public void configure(JobConf job) {
-    String userTypeName = job.get(USER_TYPE_NAME_KEY);
-    if (null == userTypeName) {
-      throw new RuntimeException("Unconfigured parameter: " + USER_TYPE_NAME_KEY);
-    }
-
-    LOG.info("User type name set to " + userTypeName);
-
-    this.userRecord = null;
-
-    try {
-      Configuration conf = new Configuration();
-      Class userClass = Class.forName(userTypeName, true,
-          Thread.currentThread().getContextClassLoader());
-      this.userRecord =
-          (SqoopRecord) ReflectionUtils.newInstance(userClass, conf);
-    } catch (ClassNotFoundException cnfe) {
-      // handled by the next block.
-      LOG.error("ClassNotFound exception: " + cnfe.toString());
-    } catch (Exception e) {
-      LOG.error("Got an exception reflecting user class: " + e.toString());
-    }
-
-    if (null == this.userRecord) {
-      LOG.error("Could not instantiate user record of type " + userTypeName);
-      throw new RuntimeException("Could not instantiate user record of type " + userTypeName);
-    }
-  }
-
-  public void map(LongWritable key, Text val, OutputCollector<Text, NullWritable> out, Reporter r)
-      throws IOException {
-
-    LOG.info("Mapper input line: " + val.toString());
-
-    try {
-      // Use the user's record class to parse the line back in.
-      userRecord.parse(val);
-    } catch (RecordParser.ParseError pe) {
-      LOG.error("Got parse error: " + pe.toString());
-      throw new IOException(pe);
-    }
-
-    LOG.info("Mapper output line: " + userRecord.toString());
-
-    out.collect(new Text(userRecord.toString()), NullWritable.get());
-
-    if (!userRecord.toString().equals(val.toString() + "\n")) {
-      // misparsed.
-      throw new IOException("Returned string has value [" + userRecord.toString() + "] when ["
-          + val.toString() + "\n] was expected.");
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/SeqFileReader.java b/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/SeqFileReader.java
deleted file mode 100644
index b4b7932..0000000
--- a/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/testutil/SeqFileReader.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.sqoop.testutil;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.SequenceFile.Reader;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * Utility class to help with test cases. Just reads the first (k, v) pair
- * from a SequenceFile and returns the value part.
- * 
- *
- */
-public final class SeqFileReader {
-
-  public static final Log LOG = LogFactory.getLog(SeqFileReader.class.getName());
-
-  public static Reader getSeqFileReader(String filename) throws IOException {
-    // read from local filesystem
-    Configuration conf = new Configuration();
-    conf.set("fs.default.name", "file:///");
-    FileSystem fs = FileSystem.get(conf);
-    LOG.info("Opening SequenceFile " + filename);
-    return new SequenceFile.Reader(fs, new Path(filename), conf);
-  }
-
-  public static Object getFirstValue(String filename) throws IOException {
-    Reader r = null;
-    try {
-      // read from local filesystem
-      Configuration conf = new Configuration();
-      conf.set("fs.default.name", "file:///");
-      FileSystem fs = FileSystem.get(conf);
-      r = new SequenceFile.Reader(fs, new Path(filename), conf);
-      Object key = ReflectionUtils.newInstance(r.getKeyClass(), conf);
-      Object val = ReflectionUtils.newInstance(r.getValueClass(), conf);
-      LOG.info("Reading value of type " + r.getValueClassName()
-          + " from SequenceFile " + filename);
-      r.next(key);
-      r.getCurrentValue(val);
-      LOG.info("Value as string: " + val.toString());
-      return val;
-    } finally {
-      if (null != r) {
-        try {
-          r.close();
-        } catch (IOException ioe) {
-          LOG.warn("IOException during close: " + ioe.toString());
-        }
-      }
-    }
-  }
-}
-
diff --git a/src/contrib/sqoop/testdata/hive/bin/hive b/src/contrib/sqoop/testdata/hive/bin/hive
deleted file mode 100755
index a4cb852..0000000
--- a/src/contrib/sqoop/testdata/hive/bin/hive
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# This is a mock "Hive" shell that validates whether various test imports
-# succeeded. It accepts commands of the form 'hive -f scriptname'
-# and validates that the script contents match those of an expected script.
-# The filename to that expected script is set via the environment variable
-# EXPECTED_SCRIPT.
-
-# The script will contain a pathname as part of the LOAD DATA INPATH statement;
-# depending on where you run the tests from, this can change. So the expected
-# script file actually contains the marker string "BASEPATH" which is replaced
-# by this script with the contents of $TMPDIR, which is set to 'test.build.data'.
-
-if [ -z "$EXPECTED_SCRIPT" ]; then
-  echo "No expected script set"
-  exit 1
-elif [ -z "$TMPDIR" ]; then
-  TMPDIR=/tmp
-elif [ "$1" != "-f" ]; then
-  echo "Misunderstood argument: $1"
-  echo "Expected '-f'."
-  exit 1
-elif [ -z "$2" ]; then
-  echo "Expected: hive -f filename"
-  exit 1
-else
-  GENERATED_SCRIPT=$2
-fi
-
-# Normalize this to an absolute path
-TMPDIR=`cd $TMPDIR && pwd`
-
-# Copy the expected script into the tmpdir and replace the marker.
-cp "$EXPECTED_SCRIPT" "$TMPDIR"
-SCRIPT_BASE=`basename $EXPECTED_SCRIPT`
-COPIED_SCRIPT="$TMPDIR/$SCRIPT_BASE"
-sed -i -e "s|BASEPATH|$TMPDIR|" $COPIED_SCRIPT
-
-# Actually check to see that the input we got matches up.
-diff --ignore-all-space --ignore-blank-lines "$COPIED_SCRIPT" "$GENERATED_SCRIPT"
-ret=$?
-
-exit $ret
-
diff --git a/src/contrib/sqoop/testdata/hive/scripts/createOnlyImport.q b/src/contrib/sqoop/testdata/hive/scripts/createOnlyImport.q
deleted file mode 100644
index 6863304..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/createOnlyImport.q
+++ /dev/null
@@ -1 +0,0 @@
-CREATE TABLE IF NOT EXISTS CREATE_ONLY_HIVE_IMPORT ( DATA_COL0 STRING, DATA_COL1 INT, DATA_COL2 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/createOverwriteImport.q b/src/contrib/sqoop/testdata/hive/scripts/createOverwriteImport.q
deleted file mode 100644
index d0b9beb..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/createOverwriteImport.q
+++ /dev/null
@@ -1 +0,0 @@
-CREATE TABLE CREATE_OVERWRITE_HIVE_IMPORT ( DATA_COL0 STRING, DATA_COL1 INT, DATA_COL2 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/customDelimImport.q b/src/contrib/sqoop/testdata/hive/scripts/customDelimImport.q
deleted file mode 100644
index 38a5f65..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/customDelimImport.q
+++ /dev/null
@@ -1,2 +0,0 @@
-CREATE TABLE IF NOT EXISTS CUSTOM_DELIM_IMPORT ( DATA_COL0 STRING, DATA_COL1 INT, DATA_COL2 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054' LINES TERMINATED BY '\174' STORED AS TEXTFILE;
-LOAD DATA INPATH 'file:BASEPATH/sqoop/warehouse/CUSTOM_DELIM_IMPORT' INTO TABLE CUSTOM_DELIM_IMPORT;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/dateImport.q b/src/contrib/sqoop/testdata/hive/scripts/dateImport.q
deleted file mode 100644
index 21446d4..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/dateImport.q
+++ /dev/null
@@ -1,2 +0,0 @@
-CREATE TABLE IF NOT EXISTS DATE_HIVE_IMPORT ( DATA_COL0 STRING, DATA_COL1 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
-LOAD DATA INPATH 'file:BASEPATH/sqoop/warehouse/DATE_HIVE_IMPORT' INTO TABLE DATE_HIVE_IMPORT;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/failingImport.q b/src/contrib/sqoop/testdata/hive/scripts/failingImport.q
deleted file mode 100644
index 21446d4..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/failingImport.q
+++ /dev/null
@@ -1,2 +0,0 @@
-CREATE TABLE IF NOT EXISTS DATE_HIVE_IMPORT ( DATA_COL0 STRING, DATA_COL1 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
-LOAD DATA INPATH 'file:BASEPATH/sqoop/warehouse/DATE_HIVE_IMPORT' INTO TABLE DATE_HIVE_IMPORT;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/normalImport.q b/src/contrib/sqoop/testdata/hive/scripts/normalImport.q
deleted file mode 100644
index 7cb31bc..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/normalImport.q
+++ /dev/null
@@ -1,2 +0,0 @@
-CREATE TABLE IF NOT EXISTS NORMAL_HIVE_IMPORT ( DATA_COL0 STRING, DATA_COL1 INT, DATA_COL2 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
-LOAD DATA INPATH 'file:BASEPATH/sqoop/warehouse/NORMAL_HIVE_IMPORT' INTO TABLE NORMAL_HIVE_IMPORT;
diff --git a/src/contrib/sqoop/testdata/hive/scripts/numericImport.q b/src/contrib/sqoop/testdata/hive/scripts/numericImport.q
deleted file mode 100644
index f0690d1..0000000
--- a/src/contrib/sqoop/testdata/hive/scripts/numericImport.q
+++ /dev/null
@@ -1,2 +0,0 @@
-CREATE TABLE IF NOT EXISTS NUMERIC_HIVE_IMPORT ( DATA_COL0 DOUBLE, DATA_COL1 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
-LOAD DATA INPATH 'file:BASEPATH/sqoop/warehouse/NUMERIC_HIVE_IMPORT' INTO TABLE NUMERIC_HIVE_IMPORT;
-- 
1.7.0.4

