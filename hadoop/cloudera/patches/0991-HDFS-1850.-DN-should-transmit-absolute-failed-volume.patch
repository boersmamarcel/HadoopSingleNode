From 3c9402dcc658e6415c59e4866ec3ee0227e819f1 Mon Sep 17 00:00:00 2001
From: Eli Collins <eli@cloudera.com>
Date: Wed, 22 Jun 2011 19:10:58 -0700
Subject: [PATCH 0991/1020] HDFS-1850. DN should transmit absolute failed volume count rather than
 increments to the NN.

Reason: Improvement
Author: Eli Collins
Ref: CDH-3065
---
 src/hdfs/org/apache/hadoop/hdfs/DFSConfigKeys.java |    2 -
 .../hadoop/hdfs/server/datanode/DataNode.java      |   15 +-
 .../hadoop/hdfs/server/datanode/FSDataset.java     |   17 +-
 .../server/datanode/metrics/DataNodeMetrics.java   |    4 +-
 .../server/datanode/metrics/FSDatasetMBean.java    |    5 +
 .../hdfs/server/namenode/DatanodeDescriptor.java   |   28 +--
 .../hadoop/hdfs/server/namenode/FSNamesystem.java  |   22 +--
 .../hadoop/hdfs/server/namenode/NameNode.java      |   19 +-
 .../hdfs/server/protocol/DatanodeProtocol.java     |    3 +-
 src/test/org/apache/hadoop/hdfs/DFSTestUtil.java   |   88 ++++++
 .../hdfs/server/datanode/SimulatedFSDataset.java   |    9 +
 .../TestDataNodeVolumeFailureReporting.java        |  286 +++++---------------
 .../TestDataNodeVolumeFailureToleration.java       |  159 +++++++++++
 .../server/namenode/NNThroughputBenchmark.java     |    4 +-
 .../hdfs/server/namenode/TestDeadDatanode.java     |    2 +-
 .../server/namenode/TestHeartbeatHandling.java     |   10 +-
 .../server/namenode/TestOverReplicatedBlocks.java  |    2 +-
 .../server/namenode/TestReplicationPolicy.java     |   14 +-
 18 files changed, 407 insertions(+), 282 deletions(-)
 create mode 100644 src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java

diff --git a/src/hdfs/org/apache/hadoop/hdfs/DFSConfigKeys.java b/src/hdfs/org/apache/hadoop/hdfs/DFSConfigKeys.java
index 135750a..c1cc3b4 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/DFSConfigKeys.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/DFSConfigKeys.java
@@ -62,8 +62,6 @@ public class DFSConfigKeys extends CommonConfigurationKeys {
   public static final long    DFS_NAMENODE_CHECKPOINT_SIZE_DEFAULT = 4194304;
   public static final String  DFS_NAMENODE_UPGRADE_PERMISSION_KEY = "dfs.namenode.upgrade.permission";
   public static final int     DFS_NAMENODE_UPGRADE_PERMISSION_DEFAULT = 00777;
-  public static final String  DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY = "dfs.namenode.heartbeat.recheck-interval";
-  public static final int     DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_DEFAULT = 5*60*1000;
   public static final String  DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_KEY = "dfs.client.https.keystore.resource";
   public static final String  DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_DEFAULT = "ssl-client.xml";
   public static final String  DFS_CLIENT_HTTPS_NEED_AUTH_KEY = "dfs.client.https.need-auth";
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index 3e5ec50..da56618 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -815,7 +815,7 @@ public class DataNode extends Configured
   protected void checkDiskError( ) {
     try {
       data.checkDataDir();
-    } catch(DiskErrorException de) {
+    } catch (DiskErrorException de) {
       handleDiskError(de.getMessage());
     }
   }
@@ -828,7 +828,7 @@ public class DataNode extends Configured
     // shutdown the DN completely.
     int dpError = hasEnoughResources ? DatanodeProtocol.DISK_ERROR  
                                      : DatanodeProtocol.FATAL_DISK_ERROR; 
-    myMetrics.volumesFailed.inc(1);
+    myMetrics.volumeFailures.inc(1);
     try {
       namenode.errorReport(dnRegistration, dpError, errMsgr);
     } catch(IOException ignored) {
@@ -883,7 +883,8 @@ public class DataNode extends Configured
                                                        data.getDfsUsed(),
                                                        data.getRemaining(),
                                                        xmitsInProgress.get(),
-                                                       getXceiverCount());
+                                                       getXceiverCount(),
+                                                       data.getNumFailedVolumes());
           myMetrics.heartbeats.inc(now() - startTime);
           //LOG.info("Just sent heartbeat, with name " + localName);
           if (!processCommand(cmds))
@@ -1439,7 +1440,11 @@ public class DataNode extends Configured
   }
   
   static boolean isDatanodeUp(DataNode dn) {
-    return dn.dataNodeThread != null && dn.dataNodeThread.isAlive();
+    return dn.isDatanodeUp();
+  }
+
+  public boolean isDatanodeUp() {
+    return dataNodeThread != null && dataNodeThread.isAlive();
   }
 
   /** Instantiate a single datanode object. This must be run by invoking
@@ -1525,7 +1530,7 @@ public class DataNode extends Configured
       try {
         DiskChecker.checkDir(localFS, new Path(dir), dataDirPermission);
         dirs.add(new File(dir));
-      } catch(DiskErrorException e) {
+      } catch (DiskErrorException e) {
         LOG.warn("Invalid directory in " + DATA_DIR_KEY +  ": " + 
                  e.getMessage());
       }
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
index c804798..5114c3b 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
@@ -595,7 +595,8 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
   static class FSVolumeSet {
     FSVolume[] volumes = null;
     int curVolume = 0;
-      
+    int numFailedVolumes = 0;
+
     FSVolumeSet(FSVolume[] volumes) {
       this.volumes = volumes;
     }
@@ -603,7 +604,11 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
     private int numberOfVolumes() {
       return volumes.length;
     }
-      
+
+    private int numberOfFailedVolumes() {
+      return numFailedVolumes;
+    }
+
     synchronized FSVolume getNextVolume(long blockSize) throws IOException {
       
       if(volumes.length < 1) {
@@ -685,6 +690,7 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
           }
           removed_vols.add(volumes[idx]);
           volumes[idx] = null; //remove the volume
+          numFailedVolumes++;
         }
       }
       
@@ -938,6 +944,13 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
   }
 
   /**
+   * Return the number of failed volumes in the FSDataset.
+   */
+  public int getNumFailedVolumes() {
+    return volumes.numberOfFailedVolumes();
+  }
+
+  /**
    * Find the block's on-disk length
    */
   public long getLength(Block b) throws IOException {
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
index 8d4d531..4d6cdca 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java
@@ -74,8 +74,8 @@ public class DataNodeMetrics implements Updater {
   public MetricsTimeVaryingInt writesFromRemoteClient = 
               new MetricsTimeVaryingInt("writes_from_remote_client", registry);
 
-  public MetricsTimeVaryingInt volumesFailed =
-    new MetricsTimeVaryingInt("volumes_failed", registry);
+  public MetricsTimeVaryingInt volumeFailures =
+    new MetricsTimeVaryingInt("volumeFailures", registry);
   
   public MetricsTimeVaryingRate readBlockOp = 
                 new MetricsTimeVaryingRate("readBlockOp", registry);
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java
index ad5cca7..db7b2b0 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/metrics/FSDatasetMBean.java
@@ -62,4 +62,9 @@ public interface FSDatasetMBean {
    */
   public String getStorageInfo();
 
+  /**
+   * Returns the number of failed volumes in the datanode.
+   * @return The number of failed volumes in the datanode.
+   */
+  public int getNumFailedVolumes();
 }
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java
index 1b5586c..fa09344 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/DatanodeDescriptor.java
@@ -118,7 +118,7 @@ public class DatanodeDescriptor extends DatanodeInfo {
    * @param nodeID id of the data node
    */
   public DatanodeDescriptor(DatanodeID nodeID) {
-    this(nodeID, 0L, 0L, 0L, 0);
+    this(nodeID, 0L, 0L, 0L, 0, 0);
   }
 
   /** DatanodeDescriptor constructor
@@ -140,7 +140,7 @@ public class DatanodeDescriptor extends DatanodeInfo {
   public DatanodeDescriptor(DatanodeID nodeID, 
                             String networkLocation,
                             String hostName) {
-    this(nodeID, networkLocation, hostName, 0L, 0L, 0L, 0);
+    this(nodeID, networkLocation, hostName, 0L, 0L, 0L, 0, 0);
   }
   
   /** DatanodeDescriptor constructor
@@ -155,9 +155,10 @@ public class DatanodeDescriptor extends DatanodeInfo {
                             long capacity,
                             long dfsUsed,
                             long remaining,
-                            int xceiverCount) {
+                            int xceiverCount,
+                            int failedVolumes) {
     super(nodeID);
-    updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
+    updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount, failedVolumes);
   }
 
   /** DatanodeDescriptor constructor
@@ -175,9 +176,10 @@ public class DatanodeDescriptor extends DatanodeInfo {
                             long capacity,
                             long dfsUsed,
                             long remaining,
-                            int xceiverCount) {
+                            int xceiverCount,
+                            int failedVolumes) {
     super(nodeID, networkLocation, hostName);
-    updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
+    updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount, failedVolumes);
   }
 
   /**
@@ -216,6 +218,7 @@ public class DatanodeDescriptor extends DatanodeInfo {
     this.xceiverCount = 0;
     this.blockList = null;
     this.invalidateBlocks.clear();
+    this.volumeFailures = 0;
   }
 
   public int numBlocks() {
@@ -225,12 +228,13 @@ public class DatanodeDescriptor extends DatanodeInfo {
   /**
    */
   void updateHeartbeat(long capacity, long dfsUsed, long remaining,
-      int xceiverCount) {
+      int xceiverCount, int volFailures) {
     this.capacity = capacity;
     this.dfsUsed = dfsUsed;
     this.remaining = remaining;
     this.lastUpdate = System.currentTimeMillis();
     this.xceiverCount = xceiverCount;
+    this.volumeFailures = volFailures;
     rollBlocksScheduled(lastUpdate);
   }
 
@@ -541,13 +545,7 @@ public class DatanodeDescriptor extends DatanodeInfo {
     }
   } // End of class DecommissioningStatus
 
-  /**
-   * Increment the volume failure count.
-   */
-  public void incVolumeFailure() {
-    volumeFailures++;
-  }
-   
+
   /**
    * @return number of failed volumes in the datanode.
    */
@@ -556,11 +554,9 @@ public class DatanodeDescriptor extends DatanodeInfo {
   }
 
   /**
-   * Reset the volume failure count when a DN re-registers.
    * @param nodeReg DatanodeID to update registration for.
    */
   public void updateRegInfo(DatanodeID nodeReg) {
     super.updateRegInfo(nodeReg);
-    volumeFailures = 0;
   }
 }
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 415d18b..43c1e2b 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -2387,7 +2387,7 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean,
         if( !heartbeats.contains(nodeS)) {
           heartbeats.add(nodeS);
           //update its timestamp
-          nodeS.updateHeartbeat(0L, 0L, 0L, 0);
+          nodeS.updateHeartbeat(0L, 0L, 0L, 0, 0);
           nodeS.isAlive = true;
         }
       }
@@ -2502,7 +2502,7 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean,
    */
   DatanodeCommand[] handleHeartbeat(DatanodeRegistration nodeReg,
       long capacity, long dfsUsed, long remaining,
-      int xceiverCount, int xmitsInProgress) throws IOException {
+      int xceiverCount, int xmitsInProgress, int failedVolumes) throws IOException {
     DatanodeCommand cmd = null;
     synchronized (heartbeats) {
       synchronized (datanodeMap) {
@@ -2524,7 +2524,7 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean,
         }
 
         updateStats(nodeinfo, false);
-        nodeinfo.updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount);
+        nodeinfo.updateHeartbeat(capacity, dfsUsed, remaining, xceiverCount, failedVolumes);
         updateStats(nodeinfo, true);
         
         //check lease recovery
@@ -3119,22 +3119,6 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean,
   }
 
   /**
-   * Update the descriptor for the datanode to reflect a volume failure.
-   * @param nodeID DatanodeID to update count for.
-   * @throws IOException
-   */
-  synchronized public void incVolumeFailure(DatanodeID nodeID)
-    throws IOException {
-    DatanodeDescriptor nodeInfo = getDatanode(nodeID);
-    if (nodeInfo != null) {
-      nodeInfo.incVolumeFailure();
-    } else {
-      NameNode.stateChangeLog.warn("BLOCK* NameSystem.incVolumeFailure: "
-                                   + nodeID.getName() + " does not exist");
-    }
-  }
-
-  /**
    * Remove a datanode descriptor.
    * @param nodeID datanode ID.
    * @throws IOException
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index 12b5e26..873b927 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -932,10 +932,11 @@ public class NameNode implements ClientProtocol, DatanodeProtocol,
                                        long dfsUsed,
                                        long remaining,
                                        int xmitsInProgress,
-                                       int xceiverCount) throws IOException {
+                                       int xceiverCount,
+                                       int failedVolumes) throws IOException {
     verifyRequest(nodeReg);
     return namesystem.handleHeartbeat(nodeReg, capacity, dfsUsed, remaining,
-        xceiverCount, xmitsInProgress);
+        xceiverCount, xmitsInProgress, failedVolumes);
   }
 
   public DatanodeCommand blockReport(DatanodeRegistration nodeReg,
@@ -968,22 +969,24 @@ public class NameNode implements ClientProtocol, DatanodeProtocol,
   }
 
   /**
+   * Handle an error report from a datanode.
    */
   public void errorReport(DatanodeRegistration nodeReg,
-                          int errorCode, 
-                          String msg) throws IOException {
-    // Log error message from datanode
+                          int errorCode, String msg) throws IOException { 
     String dnName = (nodeReg == null ? "unknown DataNode" : nodeReg.getName());
-    LOG.info("Error report from " + dnName + ": " + msg);
     if (errorCode == DatanodeProtocol.NOTIFY) {
+      LOG.info("Error report from " + dnName + ": " + msg);
       return;
     }
     verifyRequest(nodeReg);
-    namesystem.incVolumeFailure(nodeReg);
+
     if (errorCode == DatanodeProtocol.DISK_ERROR) {
-      LOG.warn("Volume failed on " + dnName); 
+      LOG.warn("Disk error on " + dnName + ": " + msg);
     } else if (errorCode == DatanodeProtocol.FATAL_DISK_ERROR) {
+      LOG.warn("Fatal disk error on " + dnName + ": " + msg);
       namesystem.removeDatanode(nodeReg);            
+    } else {
+      LOG.info("Error report from " + dnName + ": " + msg);
     }
   }
     
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java b/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
index ddb9e25..d11a897 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/protocol/DatanodeProtocol.java
@@ -89,7 +89,8 @@ public interface DatanodeProtocol extends VersionedProtocol {
                                        long capacity,
                                        long dfsUsed, long remaining,
                                        int xmitsInProgress,
-                                       int xceiverCount) throws IOException;
+                                       int xceiverCount,
+                                       int failedVolumes) throws IOException;
 
   /**
    * blockReport() tells the NameNode about all the locally-stored blocks.
diff --git a/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java b/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
index 886022a..ff0c761 100644
--- a/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
+++ b/src/test/org/apache/hadoop/hdfs/DFSTestUtil.java
@@ -31,10 +31,15 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
+import java.util.concurrent.TimeoutException;
+
 import org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.LocatedBlock;
 import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.namenode.DatanodeDescriptor;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -47,6 +52,7 @@ import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;
 import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.security.UserGroupInformation;
 
+
 /** Utilities for HDFS tests */
 public class DFSTestUtil {
   
@@ -206,6 +212,88 @@ public class DFSTestUtil {
     }
   }
 
+  /*
+   * Return the total capacity of all live DNs.
+   */
+  public static long getLiveDatanodeCapacity(FSNamesystem ns) {
+    ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
+    ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
+    ns.DFSNodesStatus(live, dead);
+    long capacity = 0;
+    for (final DatanodeDescriptor dn : live) {
+      capacity += dn.getCapacity();
+    }
+    return capacity;
+  }
+
+  /*
+   * Return the capacity of the given live DN.
+   */
+  public static long getDatanodeCapacity(FSNamesystem ns, int index) {
+    ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
+    ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
+    ns.DFSNodesStatus(live, dead);
+    return live.get(index).getCapacity();
+  }
+
+  /*
+   * Wait for the given # live/dead DNs, total capacity, and # vol failures. 
+   */
+  public static void waitForDatanodeStatus(FSNamesystem ns, int expectedLive, 
+      int expectedDead, long expectedVolFails, long expectedTotalCapacity, 
+      long timeout) throws InterruptedException, TimeoutException {
+    ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
+    ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
+    final int ATTEMPTS = 20;
+    int count = 0;
+    long currTotalCapacity = 0;
+    int volFails = 0;
+
+    do {
+      Thread.sleep(timeout);
+      live.clear();
+      dead.clear();
+      ns.DFSNodesStatus(live, dead);
+      currTotalCapacity = 0;
+      volFails = 0;
+      for (final DatanodeDescriptor dd : live) {
+        currTotalCapacity += dd.getCapacity();
+        volFails += dd.getVolumeFailures();
+      }
+      count++;
+    } while ((expectedLive != live.size() ||
+              expectedDead != dead.size() ||
+              expectedTotalCapacity != currTotalCapacity ||
+              expectedVolFails != volFails)
+             && count < ATTEMPTS);
+
+    if (count == ATTEMPTS) {
+      throw new TimeoutException("Timed out waiting for capacity."
+          + " Live = "+live.size()+" Expected = "+expectedLive
+          + " Dead = "+dead.size()+" Expected = "+expectedDead
+          + " Total capacity = "+currTotalCapacity
+          + " Expected = "+expectedTotalCapacity
+          + " Vol Fails = "+volFails+" Expected = "+expectedVolFails);
+    }
+  }
+
+  /*
+   * Wait for the given DN to consider itself dead.
+   */
+  public static void waitForDatanodeDeath(DataNode dn) 
+      throws InterruptedException, TimeoutException {
+    final int ATTEMPTS = 10;
+    int count = 0;
+    do {
+      Thread.sleep(1000);
+      count++;
+    } while (dn.isDatanodeUp() && count < ATTEMPTS);
+
+    if (count == ATTEMPTS) {
+      throw new TimeoutException("Timed out waiting for DN to die");
+    }
+  }
+  
   /** return list of filenames created as part of createFiles */
   public String[] getFileNames(String topDir) {
     if (nFiles == 0)
diff --git a/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java b/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
index 5502d71..97d0546 100644
--- a/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
+++ b/src/test/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
@@ -199,6 +199,10 @@ public class SimulatedFSDataset  implements FSConstants, FSDatasetInterface, Con
       return used;
     }
     
+    int getNumFailedVolumes() {
+      return 0;
+    }
+
     synchronized boolean alloc(long amount) {
       if (getFree() >= amount) {
         used += amount;
@@ -320,6 +324,11 @@ public class SimulatedFSDataset  implements FSConstants, FSDatasetInterface, Con
     return storage.getFree();
   }
 
+  @Override // FSDatasetMBean
+  public int getNumFailedVolumes() {
+    return storage.getNumFailedVolumes();
+  }
+
   public synchronized long getLength(Block b) throws IOException {
     BInfo binfo = blockMap.get(b);
     if (binfo == null) {
diff --git a/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java b/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java
index 917f517..2152b4d 100644
--- a/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java
+++ b/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java
@@ -39,9 +39,10 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 import static org.junit.Assert.*;
+import static org.junit.Assume.assumeTrue;
 
 /**
- * Test successive volume failures, failure metrics and capacity reporting.
+ * Test reporting of DN volume failure counts and metrics.
  */
 public class TestDataNodeVolumeFailureReporting {
 
@@ -55,6 +56,14 @@ public class TestDataNodeVolumeFailureReporting {
   private Configuration conf;
   private String dataDir;
 
+  // Sleep at least 3 seconds (a 1s heartbeat plus padding) to allow
+  // for heartbeats to propagate from the datanodes to the namenode.
+  final int WAIT_FOR_HEARTBEATS = 3000;
+
+  // Wait at least (2 * re-check + 10 * heartbeat) seconds for
+  // a datanode to be considered dead by the namenode.  
+  final int WAIT_FOR_DEATH = 15000;
+
   @Before
   public void setUp() throws Exception {
     conf = new Configuration();
@@ -76,6 +85,10 @@ public class TestDataNodeVolumeFailureReporting {
 
   @After
   public void tearDown() throws Exception {
+    for (int i = 0; i < 3; i++) {
+      new File(dataDir, "data"+(2*i+1)).setExecutable(true);
+      new File(dataDir, "data"+(2*i+2)).setExecutable(true);
+    }
     cluster.shutdown();
   }
 
@@ -86,42 +99,22 @@ public class TestDataNodeVolumeFailureReporting {
    */
   @Test
   public void testSuccessiveVolumeFailures() throws Exception {
-    if (System.getProperty("os.name").startsWith("Windows")) {
-      // See above
-      return;
-    }
+    assumeTrue(!System.getProperty("os.name").startsWith("Windows"));
+
     // Bring up two more datanodes
     cluster.startDataNodes(conf, 2, true, null, null);
     cluster.waitActive();
 
     /*
-     * Sleep at least 3 seconds (a 1s heartbeat plus padding) to allow
-     * for heartbeats to propagate from the datanodes to the namenode.
-     * Sleep  at least (2 * re-check + 10 * heartbeat) 12 seconds for
-     * a datanode  to be called dead by the namenode.
-     */
-    final int WAIT_FOR_HEARTBEATS = 3000;
-    final int WAIT_FOR_DEATH = 15000;
-    final int ATTEMPTS = 5;
-
-    /*
      * Calculate the total capacity of all the datanodes. Sleep for
      * three seconds to be sure the datanodes have had a chance to
      * heartbeat their capacities.
      */
     Thread.sleep(WAIT_FOR_HEARTBEATS);
-    FSNamesystem namesystem = cluster.getNameNode().getNamesystem();
-    ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
-    ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
-    namesystem.DFSNodesStatus(live, dead);
-    assertEquals("All DNs should be live", 3, live.size());
-    assertEquals("All DNs should be live", 0, dead.size());
-    long origCapacity = 0;
-    for (final DatanodeDescriptor dn : live) {
-      origCapacity += dn.getCapacity();
-      assertEquals("DN "+dn+" vols should be healthy",
-          0, dn.getVolumeFailures());
-    }
+    FSNamesystem ns = cluster.getNameNode().getNamesystem();
+
+    long origCapacity = DFSTestUtil.getLiveDatanodeCapacity(ns);
+    long dnCapacity = DFSTestUtil.getDatanodeCapacity(ns, 0);
 
     File dn1Vol1 = new File(dataDir, "data"+(2*0+1));
     File dn2Vol1 = new File(dataDir, "data"+(2*1+1));
@@ -147,9 +140,9 @@ public class TestDataNodeVolumeFailureReporting {
     DFSTestUtil.createFile(fs, file1, 1024, (short)3, 1L);
     DFSTestUtil.waitReplication(fs, file1, (short)3);
     ArrayList<DataNode> dns = cluster.getDataNodes();
-    assertTrue("DN1 should be up", DataNode.isDatanodeUp(dns.get(0)));
-    assertTrue("DN2 should be up", DataNode.isDatanodeUp(dns.get(1)));
-    assertTrue("DN3 should be up", DataNode.isDatanodeUp(dns.get(2)));
+    assertTrue("DN1 should be up", dns.get(0).isDatanodeUp());
+    assertTrue("DN2 should be up", dns.get(1).isDatanodeUp());
+    assertTrue("DN3 should be up", dns.get(2).isDatanodeUp());
 
     /*
      * The metrics should confirm the volume failures.
@@ -158,31 +151,18 @@ public class TestDataNodeVolumeFailureReporting {
     DataNodeMetrics metrics2 = dns.get(1).getMetrics();
     DataNodeMetrics metrics3 = dns.get(2).getMetrics();
     assertEquals("Vol1 should report 1 failure",
-        1, metrics1.volumesFailed.getCurrentIntervalValue());
+        1, metrics1.volumeFailures.getCurrentIntervalValue());
     assertEquals("Vol2 should report 1 failure",
-        1, metrics2.volumesFailed.getCurrentIntervalValue());
+        1, metrics2.volumeFailures.getCurrentIntervalValue());
     assertEquals("Vol3 should have no failures",
-        0, metrics3.volumesFailed.getCurrentIntervalValue());
-
-    // Eventually the NN should report two volume failures as well
-    int count = 0;
-    int volumeFailures = 0;
-    while (count < ATTEMPTS) {
-      Thread.sleep(WAIT_FOR_HEARTBEATS);
-      live.clear();
-      dead.clear();
-      namesystem.DFSNodesStatus(live, dead);
-      volumeFailures = 0;
-      for (final DatanodeDescriptor dn : live) {
-        volumeFailures += dn.getVolumeFailures();
-      }
-      if (2 == volumeFailures) {
-        break;
-      }
-      count++;
-      LOG.warn("Waiting for vol failures. Attempt "+count);
-    }
-    assertEquals("Incorrect failure count", 2, volumeFailures);
+        0, metrics3.volumeFailures.getCurrentIntervalValue());
+
+    // Ensure we wait a sufficient amount of time
+    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
+
+    // Eventually the NN should report two volume failures
+    DFSTestUtil.waitForDatanodeStatus(ns, 3, 0, 2, 
+        origCapacity - (1*dnCapacity), WAIT_FOR_HEARTBEATS);
 
     /*
      * Now fail a volume on the third datanode. We should be able to get
@@ -192,12 +172,16 @@ public class TestDataNodeVolumeFailureReporting {
     Path file2 = new Path("/test2");
     DFSTestUtil.createFile(fs, file2, 1024, (short)3, 1L);
     DFSTestUtil.waitReplication(fs, file2, (short)3);
-    assertTrue("DN3 should still be up", DataNode.isDatanodeUp(dns.get(2)));
+    assertTrue("DN3 should still be up", dns.get(2).isDatanodeUp());
     assertEquals("Vol3 should report 1 failure",
-        1, metrics3.volumesFailed.getCurrentIntervalValue());
+        1, metrics3.volumeFailures.getCurrentIntervalValue());
+
+    ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
+    ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
+    ns.DFSNodesStatus(live, dead);
     live.clear();
     dead.clear();
-    namesystem.DFSNodesStatus(live, dead);
+    ns.DFSNodesStatus(live, dead);
     assertEquals("DN3 should have 1 failed volume",
         1, live.get(2).getVolumeFailures());
 
@@ -206,36 +190,9 @@ public class TestDataNodeVolumeFailureReporting {
      * total capacity should be down by three volumes (assuming the host
      * did not grow or shrink the data volume while the test was running).
      */
-    count = 0;
-    int liveSize = 0;
-    int deadSize = 0;
-    long currCapacity = 0;
-    long singleVolCapacity = 0;
-    while (count < ATTEMPTS) {
-      Thread.sleep(WAIT_FOR_HEARTBEATS);
-      live.clear();
-      dead.clear();
-      namesystem.DFSNodesStatus(live, dead);
-      currCapacity = 0;
-      singleVolCapacity = live.get(0).getCapacity();
-      for (final DatanodeDescriptor dn : live) {
-        currCapacity += dn.getCapacity();
-      }
-      liveSize = live.size();
-      deadSize = dead.size();
-      LOG.info("Original capacity: "+origCapacity);
-      LOG.info("Current capacity: "+currCapacity);
-      LOG.info("Volume capacity: "+singleVolCapacity);
-      count++;
-      if (3 == live.size() && 0 == dead.size() &&
-          origCapacity == (currCapacity + (3 * singleVolCapacity))) {
-        break;
-      }
-    }
-    assertEquals("Invalid live node count", 3, liveSize);
-    assertEquals("Invalid dead node count", 0, deadSize);
-    assertEquals("Invalid capacity", origCapacity, 
-                 (currCapacity + (3 * singleVolCapacity)));
+    dnCapacity = DFSTestUtil.getDatanodeCapacity(ns, 0);
+    DFSTestUtil.waitForDatanodeStatus(ns, 3, 0, 3, 
+        origCapacity - (3*dnCapacity), WAIT_FOR_HEARTBEATS);
 
     /*
      * Now fail the 2nd volume on the 3rd datanode. All its volumes
@@ -247,33 +204,18 @@ public class TestDataNodeVolumeFailureReporting {
     Path file3 = new Path("/test3");
     DFSTestUtil.createFile(fs, file3, 1024, (short)3, 1L);
     DFSTestUtil.waitReplication(fs, file3, (short)2);
-    // Eventually the DN should go down
-    while (DataNode.isDatanodeUp(dns.get(2))) {
-      Thread.sleep(1000);
-    }
-    // and report two failed volumes
+
+    // The DN should consider itself dead
+    DFSTestUtil.waitForDatanodeDeath(dns.get(2));
+
+    // And report two failed volumes
     metrics3 = dns.get(2).getMetrics();
     assertEquals("DN3 should report 2 vol failures",
-        2, metrics3.volumesFailed.getCurrentIntervalValue());
-    // and eventually be seen as dead by the NN.
-    count = 0;
-    deadSize = 0;
-    liveSize = 0;
-    while (count < ATTEMPTS) {
-      Thread.sleep(WAIT_FOR_DEATH);
-      live.clear();
-      dead.clear();
-      namesystem.DFSNodesStatus(live, dead);
-      deadSize = dead.size();
-      liveSize = live.size();
-      if (1 == deadSize && 2 == liveSize) {
-        break;
-      }
-      count++;
-      LOG.warn("Waiting for DN to die. Attempt "+count);
-    }
-    assertEquals("Invalid dead node count", 1, deadSize);
-    assertEquals("Invalid live node count", 2, liveSize);
+        2, metrics3.volumeFailures.getCurrentIntervalValue());
+
+    // The NN considers the DN dead
+    DFSTestUtil.waitForDatanodeStatus(ns, 2, 1, 2, 
+        origCapacity - (4*dnCapacity), WAIT_FOR_HEARTBEATS);
 
     /*
      * The datanode never tries to restore the failed volume, even if
@@ -296,122 +238,44 @@ public class TestDataNodeVolumeFailureReporting {
      * and that the volume failure count should be reported as zero by
      * both the metrics and the NN.
      */
-    count = 0;
-    deadSize = 0;
-    liveSize = 0;
-    int volFailures = 0;
-    while (count < ATTEMPTS) {
-      Thread.sleep(WAIT_FOR_DEATH);
-      live.clear();
-      dead.clear();
-      namesystem.DFSNodesStatus(live, dead);
-      assertEquals("All DNs should be live", 3, live.size());
-      assertEquals("All DNs should be live", 0, dead.size());
-      currCapacity = 0;
-      volFailures = 0;
-      for (final DatanodeDescriptor dn : live) {
-        currCapacity += dn.getCapacity();
-        volFailures += dn.getVolumeFailures();
-      }
-      deadSize = dead.size();
-      liveSize = live.size();
-      if (3 == liveSize && 0 == deadSize && 0 == volFailures &&
-          origCapacity == currCapacity) {
-        break;
-      }
-      count++;
-      LOG.warn("Waiting for capacity: original="+origCapacity+" current="+
-          currCapacity+" live="+liveSize+" dead="+deadSize+
-          " vols="+volFailures+". Attempt "+count);
-    }
-    assertEquals("Invalid dead node count", 0, deadSize);
-    assertEquals("Invalid live node count", 3, liveSize);
-    assertEquals("Invalid vol failures", 0, volFailures);
-    assertEquals("Invalid capacity", origCapacity, currCapacity);
+    DFSTestUtil.waitForDatanodeStatus(ns, 3, 0, 0, origCapacity, 
+        WAIT_FOR_HEARTBEATS);
   }
 
   /**
-   * Test the DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY configuration
-   * option, ie the DN shuts itself down when the number of failures
-   * experienced drops below the tolerated amount.
+   * Test that the NN re-learns of volume failures after restart.
    */
   @Test
-  public void testConfigureMinValidVolumes() throws Exception {
-    if (System.getProperty("os.name").startsWith("Windows")) {
-      // See above
-      return;
-    }
+  public void testVolFailureStatsPreservedOnNNRestart() throws Exception {
+    assumeTrue(!System.getProperty("os.name").startsWith("Windows"));
 
-    // Bring up two additional datanodes that need both of their volumes
-    // functioning in order to stay up.
-    conf.setInt("dfs.datanode.failed.volumes.tolerated", 0);
+    // Bring up two more datanodes that can tolerate 1 failure
     cluster.startDataNodes(conf, 2, true, null, null);
     cluster.waitActive();
 
-    // Fail a volume on the 2nd DN
+    FSNamesystem ns = cluster.getNameNode().getNamesystem();
+    long origCapacity = DFSTestUtil.getLiveDatanodeCapacity(ns);
+    long dnCapacity = DFSTestUtil.getDatanodeCapacity(ns, 0);
+
+    // Fail the first volume on both datanodes (we have to keep the 
+    // third healthy so one node in the pipeline will not fail). 
+    File dn1Vol1 = new File(dataDir, "data"+(2*0+1));
     File dn2Vol1 = new File(dataDir, "data"+(2*1+1));
+    assertTrue("Couldn't chmod local vol", dn1Vol1.setExecutable(false));
     assertTrue("Couldn't chmod local vol", dn2Vol1.setExecutable(false));
 
-    // Should only get two replicas (the first DN and the 3rd)
     Path file1 = new Path("/test1");
-    DFSTestUtil.createFile(fs, file1, 1024, (short)3, 1L);
+    DFSTestUtil.createFile(fs, file1, 1024, (short)2, 1L);
     DFSTestUtil.waitReplication(fs, file1, (short)2);
 
-    // Check that this single failure caused a DN to die.
-    int count = 0;
-    int deadSize = 0;
-    final int ATTEMPTS = 5;
-    while (count < ATTEMPTS) {
-      final int WAIT_FOR_DEATH = 15000;
-      Thread.sleep(WAIT_FOR_DEATH);
-      FSNamesystem namesystem = cluster.getNameNode().getNamesystem();
-      ArrayList<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();
-      ArrayList<DatanodeDescriptor> dead = new ArrayList<DatanodeDescriptor>();
-      namesystem.DFSNodesStatus(live, dead);
-      deadSize = dead.size();
-      if (1 == deadSize) {
-        break;
-      }
-      count++;
-      LOG.warn("Waiting for DN to die. Attempt "+count);
-    }
-    assertEquals("Invalid dead node count", 1, deadSize);
+    // The NN reports two volumes failures
+    DFSTestUtil.waitForDatanodeStatus(ns, 3, 0, 2, 
+        origCapacity - (1*dnCapacity), WAIT_FOR_HEARTBEATS);
 
-    // If we restore the volume we should still only be able to get
-    // two replicas since the DN is still considered dead.
-    assertTrue("Couldn't chmod local vol", dn2Vol1.setExecutable(true));
-    Path file2 = new Path("/test2");
-    DFSTestUtil.createFile(fs, file2, 1024, (short)3, 1L);
-    DFSTestUtil.waitReplication(fs, file2, (short)2);
-  }
-
-  /**
-   * Test invalid DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY values.
-   */
-  @Test
-  public void testInvalidFailedVolumesConfig() throws Exception {
-    if (System.getProperty("os.name").startsWith("Windows")) {
-      // See above
-      return;
-    }
-    /*
-     * Bring up another datanode that has an invalid value set.
-     * We should still be able to create a file with two replicas
-     * since the minimum valid volume parameter is only checked
-     * when we experience a disk error.
-     */
-    conf.setInt("dfs.datanode.failed.volumes.tolerated", -1);
-    cluster.startDataNodes(conf, 1, true, null, null);
-    cluster.waitActive();
-    Path file1 = new Path("/test1");
-    DFSTestUtil.createFile(fs, file1, 1024, (short)2, 1L);
-    DFSTestUtil.waitReplication(fs, file1, (short)2);
-    // Ditto if the value is too big.
-    conf.setInt("dfs.datanode.failed.volumes.tolerated", 100);
-    cluster.startDataNodes(conf, 1, true, null, null);
+    // After restarting the NN it still see the two failures
+    cluster.restartNameNode();
     cluster.waitActive();
-    Path file2 = new Path("/test1");
-    DFSTestUtil.createFile(fs, file2, 1024, (short)2, 1L);
-    DFSTestUtil.waitReplication(fs, file2, (short)2);
+    DFSTestUtil.waitForDatanodeStatus(ns, 3, 0, 2,
+        origCapacity - (1*dnCapacity), WAIT_FOR_HEARTBEATS);
   }
 }
diff --git a/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java b/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java
new file mode 100644
index 0000000..99b80c1
--- /dev/null
+++ b/src/test/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java
@@ -0,0 +1,159 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.datanode;
+
+import java.io.File;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.DFSTestUtil;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.impl.Log4JLogger;
+import org.apache.log4j.Level;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import static org.junit.Assert.*;
+import static org.junit.Assume.assumeTrue;
+
+/**
+ * Test the ability of a DN to tolerate volume failures.
+ */
+public class TestDataNodeVolumeFailureToleration {
+
+  private static final Log LOG = LogFactory.getLog(TestDataNodeVolumeFailureToleration.class);
+  {
+    ((Log4JLogger)TestDataNodeVolumeFailureToleration.LOG).getLogger().setLevel(Level.ALL);
+  }
+
+  private FileSystem fs;
+  private MiniDFSCluster cluster;
+  private Configuration conf;
+  private String dataDir;
+
+  // Sleep at least 3 seconds (a 1s heartbeat plus padding) to allow
+  // for heartbeats to propagate from the datanodes to the namenode.
+  final int WAIT_FOR_HEARTBEATS = 3000;
+
+  // Wait at least (2 * re-check + 10 * heartbeat) seconds for
+  // a datanode to be considered dead by the namenode.  
+  final int WAIT_FOR_DEATH = 15000;
+
+  @Before
+  public void setUp() throws Exception {
+    conf = new Configuration();
+    conf.setLong("dfs.block.size", 512L);
+    /*
+     * Lower the DN heartbeat, DF rate, and recheck interval to one second
+     * so state about failures and datanode death propagates faster.
+     */
+    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);
+    conf.setInt(DFSConfigKeys.DFS_DF_INTERVAL_KEY, 1000);
+    conf.setInt("heartbeat.recheck.interval", 1000);
+    // Allow a single volume failure (there are two volumes)
+    conf.setInt("dfs.datanode.failed.volumes.tolerated", 1);
+    cluster = new MiniDFSCluster(conf, 1, true, null);
+    cluster.waitActive();
+    fs = cluster.getFileSystem();
+    dataDir = cluster.getDataDirectory();
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    for (int i = 0; i < 3; i++) {
+      new File(dataDir, "data"+(2*i+1)).setExecutable(true);
+      new File(dataDir, "data"+(2*i+2)).setExecutable(true);
+    }
+    cluster.shutdown();
+  }
+
+  /**
+   * Test the DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY configuration
+   * option, ie the DN shuts itself down when the number of failures
+   * experienced drops below the tolerated amount.
+   */
+  @Test
+  public void testConfigureMinValidVolumes() throws Exception {
+    assumeTrue(!System.getProperty("os.name").startsWith("Windows"));
+
+    // Bring up two additional datanodes that need both of their volumes
+    // functioning in order to stay up.
+    conf.setInt("dfs.datanode.failed.volumes.tolerated", 0);
+    cluster.startDataNodes(conf, 2, true, null, null);
+    cluster.waitActive();
+    FSNamesystem ns = cluster.getNameNode().getNamesystem();
+    long origCapacity = DFSTestUtil.getLiveDatanodeCapacity(ns);
+    long dnCapacity = DFSTestUtil.getDatanodeCapacity(ns, 0);
+
+    // Fail a volume on the 2nd DN
+    File dn2Vol1 = new File(dataDir, "data"+(2*1+1));
+    assertTrue("Couldn't chmod local vol", dn2Vol1.setExecutable(false));
+
+    // Should only get two replicas (the first DN and the 3rd)
+    Path file1 = new Path("/test1");
+    DFSTestUtil.createFile(fs, file1, 1024, (short)3, 1L);
+    DFSTestUtil.waitReplication(fs, file1, (short)2);
+
+    // Check that this single failure caused a DN to die.
+    DFSTestUtil.waitForDatanodeStatus(ns, 2, 1, 0, 
+        origCapacity - (1*dnCapacity), WAIT_FOR_HEARTBEATS);
+
+    // If we restore the volume we should still only be able to get
+    // two replicas since the DN is still considered dead.
+    assertTrue("Couldn't chmod local vol", dn2Vol1.setExecutable(true));
+    Path file2 = new Path("/test2");
+    DFSTestUtil.createFile(fs, file2, 1024, (short)3, 1L);
+    DFSTestUtil.waitReplication(fs, file2, (short)2);
+  }
+
+  /**
+   * Test invalid DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY values.
+   */
+  @Test
+  public void testInvalidFailedVolumesConfig() throws Exception {
+    assumeTrue(!System.getProperty("os.name").startsWith("Windows"));
+
+    /*
+     * Bring up another datanode that has an invalid value set.
+     * We should still be able to create a file with two replicas
+     * since the minimum valid volume parameter is only checked
+     * when we experience a disk error.
+     */
+    conf.setInt("dfs.datanode.failed.volumes.tolerated", -1);
+    cluster.startDataNodes(conf, 1, true, null, null);
+    cluster.waitActive();
+    Path file1 = new Path("/test1");
+    DFSTestUtil.createFile(fs, file1, 1024, (short)2, 1L);
+    DFSTestUtil.waitReplication(fs, file1, (short)2);
+
+    // Ditto if the value is too big.
+    conf.setInt("dfs.datanode.failed.volumes.tolerated", 100);
+    cluster.startDataNodes(conf, 1, true, null, null);
+    cluster.waitActive();
+    Path file2 = new Path("/test1");
+    DFSTestUtil.createFile(fs, file2, 1024, (short)2, 1L);
+    DFSTestUtil.waitReplication(fs, file2, (short)2);
+  }
+}
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java b/src/test/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
index f1207de..0453374 100644
--- a/src/test/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
@@ -759,7 +759,7 @@ public class NNThroughputBenchmark {
     void sendHeartbeat() throws IOException {
       // register datanode
       DatanodeCommand[] cmds = nameNode.sendHeartbeat(
-          dnRegistration, DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, 0, 0);
+          dnRegistration, DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, 0, 0, 0);
       if(cmds != null) {
         for (DatanodeCommand cmd : cmds ) {
           LOG.debug("sendHeartbeat Name-node reply: " + cmd.getAction());
@@ -793,7 +793,7 @@ public class NNThroughputBenchmark {
     int replicateBlocks() throws IOException {
       // register datanode
       DatanodeCommand[] cmds = nameNode.sendHeartbeat(
-          dnRegistration, DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, 0, 0);
+          dnRegistration, DF_CAPACITY, DF_USED, DF_CAPACITY - DF_USED, 0, 0, 0);
       if (cmds != null) {
         for (DatanodeCommand cmd : cmds) {
           if (cmd.getAction() == DatanodeProtocol.DNA_TRANSFER) {
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java b/src/test/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java
index 2351f9a..5fa3eda 100644
--- a/src/test/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java
@@ -120,7 +120,7 @@ public class TestDeadDatanode {
 
     // Ensure heartbeat from dead datanode is rejected with a command
     // that asks datanode to register again
-    DatanodeCommand[] cmd = dnp.sendHeartbeat(reg, 0, 0, 0, 0, 0);
+    DatanodeCommand[] cmd = dnp.sendHeartbeat(reg, 0, 0, 0, 0, 0, 0);
     Assert.assertEquals(1, cmd.length);
     Assert.assertEquals(cmd[0].getAction(), DatanodeCommand.REGISTER
         .getAction());
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/TestHeartbeatHandling.java b/src/test/org/apache/hadoop/hdfs/server/namenode/TestHeartbeatHandling.java
index 17fb9bf..fd5be1e 100644
--- a/src/test/org/apache/hadoop/hdfs/server/namenode/TestHeartbeatHandling.java
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/TestHeartbeatHandling.java
@@ -45,7 +45,7 @@ public class TestHeartbeatHandling extends TestCase {
             new Block(i, 0, GenerationStamp.FIRST_VALID_STAMP), ONE_TARGET);
       }
       DatanodeCommand[] cmds = namesystem.handleHeartbeat(
-          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0);
+          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0, 0);
       assertEquals(1, cmds.length);
       assertEquals(DatanodeProtocol.DNA_TRANSFER, cmds[0].getAction());
       assertEquals(MAX_REPLICATE_LIMIT, ((BlockCommand)cmds[0]).getBlocks().length);
@@ -57,7 +57,7 @@ public class TestHeartbeatHandling extends TestCase {
       dd.addBlocksToBeInvalidated(blockList);
            
       cmds = namesystem.handleHeartbeat(
-          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0);
+          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0, 0);
       assertEquals(2, cmds.length);
       assertEquals(DatanodeProtocol.DNA_TRANSFER, cmds[0].getAction());
       assertEquals(MAX_REPLICATE_LIMIT, ((BlockCommand)cmds[0]).getBlocks().length);
@@ -65,7 +65,7 @@ public class TestHeartbeatHandling extends TestCase {
       assertEquals(MAX_INVALIDATE_LIMIT, ((BlockCommand)cmds[1]).getBlocks().length);
       
       cmds = namesystem.handleHeartbeat(
-          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0);
+          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0, 0);
       assertEquals(2, cmds.length);
       assertEquals(DatanodeProtocol.DNA_TRANSFER, cmds[0].getAction());
       assertEquals(REMAINING_BLOCKS, ((BlockCommand)cmds[0]).getBlocks().length);
@@ -73,13 +73,13 @@ public class TestHeartbeatHandling extends TestCase {
       assertEquals(MAX_INVALIDATE_LIMIT, ((BlockCommand)cmds[1]).getBlocks().length);
       
       cmds = namesystem.handleHeartbeat(
-          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0);
+          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0, 0);
       assertEquals(1, cmds.length);
       assertEquals(DatanodeProtocol.DNA_INVALIDATE, cmds[0].getAction());
       assertEquals(REMAINING_BLOCKS, ((BlockCommand)cmds[0]).getBlocks().length);
 
       cmds = namesystem.handleHeartbeat(
-          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0);
+          nodeReg, dd.getCapacity(), dd.getDfsUsed(), dd.getRemaining(), 0, 0, 0);
       assertEquals(null, cmds);
       }
     } finally {
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/TestOverReplicatedBlocks.java b/src/test/org/apache/hadoop/hdfs/server/namenode/TestOverReplicatedBlocks.java
index 7555a78..b3adcdf 100644
--- a/src/test/org/apache/hadoop/hdfs/server/namenode/TestOverReplicatedBlocks.java
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/TestOverReplicatedBlocks.java
@@ -77,7 +77,7 @@ public class TestOverReplicatedBlocks extends TestCase {
         // so they will be chosen to be deleted when over-replication occurs
         for (DatanodeDescriptor datanode : namesystem.heartbeats) {
           if (!corruptDataNode.equals(datanode)) {
-            datanode.updateHeartbeat(100L, 100L, 0L, 0);
+            datanode.updateHeartbeat(100L, 100L, 0L, 0, 0);
           }
         }
         
diff --git a/src/test/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java b/src/test/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
index 9324ba6..20c4f4b 100644
--- a/src/test/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
+++ b/src/test/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java
@@ -71,7 +71,7 @@ public class TestReplicationPolicy extends TestCase {
     for(int i=0; i<NUM_OF_DATANODES; i++) {
       dataNodes[i].updateHeartbeat(
           2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-          2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0);
+          2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0, 0);
     }
   }
   
@@ -87,7 +87,7 @@ public class TestReplicationPolicy extends TestCase {
   public void testChooseTarget1() throws Exception {
     dataNodes[0].updateHeartbeat(
         2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L, 
-        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 4); // overloaded
+        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 4, 0); // overloaded
 
     DatanodeDescriptor[] targets;
     targets = replicator.chooseTarget(
@@ -122,7 +122,7 @@ public class TestReplicationPolicy extends TestCase {
     
     dataNodes[0].updateHeartbeat(
         2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0); 
+        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0, 0); 
   }
 
   /**
@@ -193,7 +193,7 @@ public class TestReplicationPolicy extends TestCase {
     // make data node 0 to be not qualified to choose
     dataNodes[0].updateHeartbeat(
         2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-        (FSConstants.MIN_BLOCKS_FOR_WRITE-1)*BLOCK_SIZE, 0); // no space
+        (FSConstants.MIN_BLOCKS_FOR_WRITE-1)*BLOCK_SIZE, 0, 0); // no space
         
     DatanodeDescriptor[] targets;
     targets = replicator.chooseTarget(
@@ -231,7 +231,7 @@ public class TestReplicationPolicy extends TestCase {
 
     dataNodes[0].updateHeartbeat(
         2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0); 
+        FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0, 0); 
   }
   
   /**
@@ -247,7 +247,7 @@ public class TestReplicationPolicy extends TestCase {
     for(int i=0; i<2; i++) {
       dataNodes[i].updateHeartbeat(
           2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-          (FSConstants.MIN_BLOCKS_FOR_WRITE-1)*BLOCK_SIZE, 0);
+          (FSConstants.MIN_BLOCKS_FOR_WRITE-1)*BLOCK_SIZE, 0, 0);
     }
       
     DatanodeDescriptor[] targets;
@@ -279,7 +279,7 @@ public class TestReplicationPolicy extends TestCase {
     for(int i=0; i<2; i++) {
       dataNodes[i].updateHeartbeat(
           2*FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,
-          FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0);
+          FSConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0, 0);
     }
   }
   /**
-- 
1.7.0.4

