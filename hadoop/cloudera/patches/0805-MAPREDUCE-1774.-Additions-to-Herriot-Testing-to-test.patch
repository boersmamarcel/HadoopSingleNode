From 83a6619c2656f543e046521f515d97fd70d647bb Mon Sep 17 00:00:00 2001
From: Todd Lipcon <todd@cloudera.com>
Date: Wed, 2 Feb 2011 17:37:15 -0800
Subject: [PATCH 0805/1020] MAPREDUCE-1774. Additions to Herriot Testing to test Gridmix, Streaming, Task Controllers

Includes:
  MAPREDUCE-1758 Building blocks for the herriot test cases
  MAPREDUCE-1827 [Herriot] Task Killing/Failing tests for a streaming job.
  MAPREDUCE-2053 [Herriot] Test Gridmix file pool for different input file sizes based on pool minimum size
  MAPREDUCE-2033 [Herriot] Gridmix generate data tests with various submission policies and different user resolvers.
  ... and others from YDH

Reason: QA / YDH merge
Ref: CDH-2622
---
 src/contrib/build-contrib.xml                      |  112 ++++-
 src/contrib/build.xml                              |   23 +
 src/contrib/gridmix/build.xml                      |   10 +
 .../hadoop/mapred/gridmix/GridMixConfig.java       |   75 +++
 .../hadoop/mapred/gridmix/GridMixRunMode.java      |   38 ++
 .../mapred/gridmix/TestGridMixDataGeneration.java  |  203 +++++++
 .../hadoop/mapred/gridmix/TestGridMixFilePool.java |  115 ++++
 .../hadoop/mapred/gridmix/UtilsForGridmix.java     |  214 +++++++
 src/contrib/streaming/build.xml                    |    6 +
 src/contrib/streaming/ivy.xml                      |    6 +-
 .../org/apache/hadoop/streaming/StreamJob.java     |    2 +-
 ...stLinuxTaskControllerOtherUserStreamingJob.java |  157 +++++
 .../hadoop/mapred/TestStreamingJobProcessTree.java |  351 +++++++++++
 .../mapred/TestTaskKillingOfStreamingJob.java      |  310 ++++++++++
 .../src/test/system/scripts/ProcessTree.sh         |   40 ++
 .../src/test/system/scripts/StreamMapper.sh        |   41 ++
 src/test/aop/build/aop.xml                         |   13 +-
 .../org/apache/hadoop/mapred/JTProtocolAspect.aj   |   47 ++-
 .../apache/hadoop/mapred/JobInProgressAspect.aj    |   50 ++-
 .../org/apache/hadoop/mapred/JobTrackerAspect.aj   |  326 ++++++++++
 .../hadoop/mapred/StatisticsCollectorAspect.aj     |   57 ++
 .../hadoop/test/system/DaemonProtocolAspect.aj     |  134 ++++-
 src/test/system/conf/system-test.xml               |   40 ++-
 .../apache/hadoop/mapred/HealthScriptHelper.java   |  166 +++++
 .../org/apache/hadoop/mapred/HighRamJobHelper.java |   56 ++
 .../java/org/apache/hadoop/mapred/JobInfoImpl.java |   54 ++-
 .../hadoop/mapred/TestCMExceptionDuringRunJob.java |  122 ++++
 .../hadoop/mapred/TestCacheFileReferenceCount.java |  283 +++++++++
 .../TestChildsKillingOfMemoryExceedsTask.java      |  339 +++++++++++
 .../mapred/TestChildsKillingOfSuspendTask.java     |  319 ++++++++++
 .../java/org/apache/hadoop/mapred/TestCluster.java |    4 +
 .../apache/hadoop/mapred/TestCorruptedDiskJob.java |  177 ++++++
 .../mapred/TestDistributedCacheModifiedFile.java   |   23 +-
 .../mapred/TestDistributedCachePrivateFile.java    |   32 +-
 .../mapred/TestDistributedCacheUnModifiedFile.java |    6 +-
 .../hadoop/mapred/TestHealthScriptError.java       |   76 +++
 .../hadoop/mapred/TestHealthScriptPathError.java   |   65 ++
 .../hadoop/mapred/TestHealthScriptTimeout.java     |   73 +++
 .../hadoop/mapred/TestHiRamJobWithBlackListTT.java |   96 +++
 .../mapred/TestJobCacheDirectoriesCleanUp.java     |  312 ++++++++++
 .../hadoop/mapred/TestJobHistoryLocation.java      |  366 ++++++++++++
 .../java/org/apache/hadoop/mapred/TestJobKill.java |   10 +-
 .../org/apache/hadoop/mapred/TestJobSummary.java   |  285 +++++++++
 .../mapred/TestLinuxTaskControllerOtherUser.java   |  125 ++++
 .../apache/hadoop/mapred/TestLostTaskTracker.java  |  258 ++++++++
 .../hadoop/mapred/TestNodeDecommissioning.java     |  199 ++++++
 .../org/apache/hadoop/mapred/TestPushConfig.java   |  130 +---
 .../org/apache/hadoop/mapred/TestRetiredJobs.java  |  181 ++++++
 .../hadoop/mapred/TestTaskChildsKilling.java       |  551 +++++++++++++++++
 .../apache/hadoop/mapred/TestTaskController.java   |   84 +++
 .../org/apache/hadoop/mapred/TestTaskKilling.java  |  334 +++++------
 .../TestTaskTrackerInfoSuccessfulFailedJobs.java   |  631 ++++++++++++++++++++
 .../mapred/TestTaskTrackerInfoTTProcess.java       |  401 +++++++++++++
 .../hadoop/mapreduce/test/system/JTClient.java     |  119 ++++-
 .../hadoop/mapreduce/test/system/JTProtocol.java   |   86 +++
 .../hadoop/mapreduce/test/system/JobInfo.java      |   25 +-
 .../hadoop/mapreduce/test/system/MRCluster.java    |   69 +++-
 .../hadoop/mapreduce/test/system/TTClient.java     |   71 +++
 .../hadoop/test/system/AbstractDaemonClient.java   |   83 ++-
 .../hadoop/test/system/AbstractDaemonCluster.java  |  315 ++++++++++-
 .../apache/hadoop/test/system/DaemonProtocol.java  |   47 ++-
 .../test/system/process/ClusterProcessManager.java |   18 +-
 .../system/process/HadoopDaemonRemoteCluster.java  |   15 +-
 .../hadoop/test/system/process/RemoteProcess.java  |   12 +-
 .../org/apache/hadoop/common/RemoteExecution.java  |   97 +++
 .../hadoop/mapred/StatisticsCollectionHandler.java |  108 ++++
 src/test/system/scripts/healthScriptError          |   19 +
 src/test/system/scripts/healthScriptTimeout        |   19 +
 68 files changed, 8822 insertions(+), 409 deletions(-)
 create mode 100644 src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixConfig.java
 create mode 100644 src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixRunMode.java
 create mode 100644 src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixDataGeneration.java
 create mode 100644 src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixFilePool.java
 create mode 100644 src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/UtilsForGridmix.java
 create mode 100644 src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUserStreamingJob.java
 create mode 100644 src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestStreamingJobProcessTree.java
 create mode 100644 src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestTaskKillingOfStreamingJob.java
 create mode 100644 src/contrib/streaming/src/test/system/scripts/ProcessTree.sh
 create mode 100644 src/contrib/streaming/src/test/system/scripts/StreamMapper.sh
 create mode 100644 src/test/system/aop/org/apache/hadoop/mapred/StatisticsCollectorAspect.aj
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/HealthScriptHelper.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/HighRamJobHelper.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestCMExceptionDuringRunJob.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestCacheFileReferenceCount.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfMemoryExceedsTask.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfSuspendTask.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestCorruptedDiskJob.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptError.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptPathError.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptTimeout.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestHiRamJobWithBlackListTT.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestJobCacheDirectoriesCleanUp.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestJobHistoryLocation.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestJobSummary.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUser.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestLostTaskTracker.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestNodeDecommissioning.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestRetiredJobs.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestTaskChildsKilling.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestTaskController.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoSuccessfulFailedJobs.java
 create mode 100644 src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoTTProcess.java
 create mode 100644 src/test/system/java/shared/org/apache/hadoop/common/RemoteExecution.java
 create mode 100644 src/test/system/java/shared/org/apache/hadoop/mapred/StatisticsCollectionHandler.java
 create mode 100644 src/test/system/scripts/healthScriptError
 create mode 100644 src/test/system/scripts/healthScriptTimeout

diff --git a/src/contrib/build-contrib.xml b/src/contrib/build-contrib.xml
index 39c3996..8d52e32 100644
--- a/src/contrib/build-contrib.xml
+++ b/src/contrib/build-contrib.xml
@@ -34,9 +34,14 @@
   <property name="src.dir"  location="${root}/src/java"/>
   <property name="src.test" location="${root}/src/test"/>
   <property name="src.examples" location="${root}/src/examples"/>
+  <!-- Property added for contrib system tests -->
+  <property name="src.test.system" location="${root}/src/test/system"/>
 
   <available file="${src.examples}" type="dir" property="examples.available"/>
   <available file="${src.test}" type="dir" property="test.available"/>
+  <!-- Property added for contrib system tests -->
+  <available file="${src.test.system}" type="dir" 
+      property="test.system.available"/>
 
   <property name="conf.dir" location="${hadoop.root}/conf"/>
   <property name="test.junit.output.format" value="plain"/>
@@ -63,6 +68,10 @@
 
   <fileset id="lib.jars" dir="${root}" includes="lib/*.jar"/>
 
+  <!-- Property added for contrib system tests -->
+  <property name="build.test.system" location="${build.dir}/system"/>
+  <property name="build.system.classes" 
+      location="${build.test.system}/classes"/>
 
    <!-- IVY properties set here -->
   <property name="ivy.dir" location="ivy" />
@@ -112,6 +121,36 @@
     <path refid="contrib-classpath"/>
   </path>
 
+  <!-- The system test classpath -->
+  <path id="test.system.classpath">
+    <pathelement location="${hadoop.root}/src/contrib/${name}/src/test/system" />
+    <pathelement location="${build.test.system}" />
+    <pathelement location="${build.test.system}/classes"/>
+    <pathelement location="${build.examples}"/>
+    <pathelement location="${hadoop.root}/build-fi/system/classes" />
+    <pathelement location="${hadoop.root}/build-fi/system/test/classes" />
+    <pathelement location="${hadoop.root}/build-fi" />
+    <pathelement location="${hadoop.root}/build-fi/tools" />
+    <pathelement location="${hadoop.home}"/>
+    <pathelement location="${hadoop.conf.dir}"/>
+    <pathelement location="${hadoop.conf.dir.deployed}"/>
+    <pathelement location="${hadoop.root}/build"/>
+    <pathelement location="${hadoop.root}/build/examples"/>
+    <pathelement location="${hadoop.root}/build-fi/test/classes" />
+    <path refid="contrib-classpath"/>
+    <fileset dir="${hadoop.root}/src/test/lib">
+      <include name="**/*.jar" />
+      <exclude name="**/excluded/" />
+    </fileset>
+    <fileset dir="${hadoop.root}/build-fi/system">
+       <include name="**/*.jar" />
+       <exclude name="**/excluded/" />
+     </fileset>
+    <fileset dir="${hadoop.root}/build-fi/test/testjar">
+      <include name="**/*.jar" />
+      <exclude name="**/excluded/" />
+    </fileset>
+  </path>
 
   <!-- to be overridden by sub-projects -->
   <target name="check-contrib"/>
@@ -125,6 +164,9 @@
     <mkdir dir="${build.dir}"/>
     <mkdir dir="${build.classes}"/>
     <mkdir dir="${build.test}"/>
+    <!-- The below two tags  added for contrib system tests -->
+    <mkdir dir="${build.test.system}"/>
+    <mkdir dir="${build.system.classes}"/> 
     <mkdir dir="${build.examples}"/>
     <mkdir dir="${hadoop.log.dir}"/>
     <antcall target="init-contrib"/>
@@ -173,12 +215,28 @@
      encoding="${build.encoding}"
      srcdir="${src.test}"
      includes="**/*.java"
+     excludes="system/**/*.java"
      destdir="${build.test}"
      debug="${javac.debug}">
     <classpath refid="test.classpath"/>
     </javac>
   </target>
   
+  <!-- ================================================================== -->
+  <!-- Compile system test code                                           -->
+  <!-- ================================================================== -->
+  <target name="compile-test-system" depends="compile-examples"
+     if="test.system.available">
+    <echo message="contrib: ${name}"/>
+    <javac
+       encoding="${build.encoding}"
+       srcdir="${src.test.system}"
+       includes="**/*.java"
+       destdir="${build.system.classes}"
+       debug="${javac.debug}">
+      <classpath refid="test.system.classpath"/>
+    </javac>
+  </target>
 
   <!-- ====================================================== -->
   <!-- Make a Hadoop contrib's jar                            -->
@@ -261,15 +319,65 @@
       <formatter type="${test.junit.output.format}" />
       <batchtest todir="${build.test}" unless="testcase">
         <fileset dir="${src.test}"
-                 includes="**/Test*.java" excludes="**/${test.exclude}.java" />
+                 includes="**/Test*.java" excludes="**/${test.exclude}.java, system/**/*.java" />
       </batchtest>
       <batchtest todir="${build.test}" if="testcase">
-        <fileset dir="${src.test}" includes="**/${testcase}.java"/>
+        <fileset dir="${src.test}" includes="**/${testcase}.java" excludes="system/**/*.java" />
       </batchtest>
     </junit>
     <antcall target="checkfailure"/>
   </target>
 
+  <!-- ================================================================== -->
+  <!-- Run system tests                                                   -->
+  <!-- ================================================================== -->
+  <target name="test-system" depends="compile, compile-test-system"
+     if="test.system.available">
+     <delete dir="${build.test.system}/extraconf"/>
+     <mkdir dir="${build.test.system}/extraconf"/>
+     <property name="test.src.dir" location="${hadoop.root}/src/test"/>
+     <property name="test.junit.printsummary" value="yes" />
+     <property name="test.junit.haltonfailure" value="no" />
+     <property name="test.junit.maxmemory" value="512m" />
+     <property name="test.junit.fork.mode" value="perTest" />
+     <property name="test.all.tests.file" value="${test.src.dir}/all-tests" />
+     <property name="test.build.dir" value="${hadoop.root}/build/test"/>
+     <property name="basedir" value="${hadoop.root}"/>
+     <property name="test.timeout" value="900000"/>
+     <property name="test.junit.output.format" value="plain"/>
+     <property name="test.tools.input.dir" value="${basedir}/src/test/tools/data"/>
+     <property name="c++.src" value="${basedir}/src/c++"/>
+     <property name="test.include" value="Test*"/>
+     <property name="c++.libhdfs.src" value="${c++.src}/libhdfs"/>
+     <property name="test.build.data" value="${build.test.system}/data"/>
+     <property name="test.cache.data" value="${build.test.system}/cache"/>
+     <property name="test.debug.data" value="${build.test.system}/debug"/>
+     <property name="test.log.dir" value="${build.test.system}/logs"/>
+     <patternset id="empty.exclude.list.id" />
+        <exec executable="sed" inputstring="${os.name}"
+            outputproperty="nonspace.os">
+          <arg value="s/ /_/g"/>
+        </exec>
+     <property name="build.platform"
+         value="${nonspace.os}-${os.arch}-${sun.arch.data.model}"/>
+     <property name="build.native" 
+         value="${hadoop.root}/build/native/${build.platform}"/>
+     <property name="lib.dir" value="${hadoop.root}/lib"/>
+     <property name="install.c++.examples"
+         value="${hadoop.root}/build/c++-examples/${build.platform}"/>
+    <condition property="tests.testcase">
+       <and>
+         <isset property="testcase" />
+       </and>
+    </condition>
+    <macro-test-runner test.file="${test.all.tests.file}"
+                       classpath="test.system.classpath"
+                       test.dir="${build.test.system}"
+                       fileset.dir="${hadoop.root}/src/contrib/${name}/src/test/system"
+                       hadoop.conf.dir.deployed="${hadoop.conf.dir.deployed}">
+    </macro-test-runner>
+  </target>
+
   <target name="checkfailure" if="tests.failed">
     <touch file="${build.contrib.dir}/testsfailed"/>
     <fail unless="continueOnFailure">Contrib Tests failed!</fail>
diff --git a/src/contrib/build.xml b/src/contrib/build.xml
index 2fadc41..d2a51d7 100644
--- a/src/contrib/build.xml
+++ b/src/contrib/build.xml
@@ -63,6 +63,29 @@
   
   
   <!-- ====================================================== -->
+  <!-- Test all the contrib system tests                     -->
+  <!-- ====================================================== -->
+  <target name="test-system-contrib">
+    <property name="hadoop.root" location="${root}/../../../"/>
+    <property name="build.contrib.dir" location="${hadoop.root}/build/contrib"/>
+    <delete file="${build.contrib.dir}/testsfailed"/>
+    <subant target="test-system">
+       <property name="continueOnFailure" value="true"/>
+       <property name="hadoop.home" value="${hadoop.home}"/>
+       <property name="hadoop.conf.dir" value="${hadoop.conf.dir}"/>
+       <property name="hadoop.conf.dir.deployed"
+           value="${hadoop.conf.dir.deployed}"/>
+       <fileset dir="." includes="hdfsproxy/build.xml"/>
+       <fileset dir="." includes="streaming/build.xml"/>
+       <fileset dir="." includes="fairscheduler/build.xml"/>
+       <fileset dir="." includes="capacity-scheduler/build.xml"/>
+       <fileset dir="." includes="gridmix/build.xml"/>
+    </subant>
+    <available file="${build.contrib.dir}/testsfailed" property="testsfailed"/>
+    <fail if="testsfailed">Tests failed!</fail>
+  </target>
+
+  <!-- ====================================================== -->
   <!-- Clean all the contribs.                              -->
   <!-- ====================================================== -->
   <target name="clean">
diff --git a/src/contrib/gridmix/build.xml b/src/contrib/gridmix/build.xml
index 40763d8..d848eb3 100644
--- a/src/contrib/gridmix/build.xml
+++ b/src/contrib/gridmix/build.xml
@@ -20,4 +20,14 @@
 
   <import file="../build-contrib.xml"/>
 
+   <!-- Run all unit tests. superdottest -->
+  <target name="test">
+   <antcall target="hadoopbuildcontrib.test" />
+  </target>
+
+  <!--Run all system tests.-->
+  <target name="test-system">
+    <antcall target="hadoopbuildcontrib.test-system" />
+   </target>
+
 </project>
diff --git a/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixConfig.java b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixConfig.java
new file mode 100644
index 0000000..cb0ebf7
--- /dev/null
+++ b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixConfig.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+public class GridMixConfig {
+  /**
+   *  Gridmix logger mode.
+   */
+  public static final String GRIDMIX_LOG_MODE = 
+      "log4j.logger.org.apache.hadoop.mapred.gridmix";
+
+  /**
+   *  Gridmix output directory.
+   */
+  public static final String GRIDMIX_OUTPUT_DIR = 
+      "gridmix.output.directory";
+
+  /**
+   * Gridmix job type (LOADJOB/SLEEPJOB).
+   */
+  public static final String GRIDMIX_JOB_TYPE = 
+      "gridmix.job.type";
+
+  /**
+   *  Gridmix submission use queue.
+   */
+  public static final String GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = 
+      "gridmix.job-submission.use-queue-in-trace";
+  
+  /**
+   *  Gridmix user resolver(RoundRobinUserResolver/
+   *  SubmitterUserResolver/EchoUserResolver).
+   */
+  public static final String GRIDMIX_USER_RESOLVER = 
+      "gridmix.user.resolve.class";
+
+  /**
+   *  Gridmix queue depth.
+   */
+  public static final String GRIDMIX_QUEUE_DEPTH = 
+      "gridmix.client.pending.queue.depth";
+  
+  /**
+   * Gridmix generate bytes per file.
+   */
+  public static final String GRIDMIX_BYTES_PER_FILE = 
+      "gridmix.gen.bytes.per.file";
+  
+  /**
+   *  Gridmix job submission policy(STRESS/REPLAY/SERIAL).
+   */
+  public static final String GRIDMIX_SUBMISSION_POLICY =
+      "gridmix.job-submission.policy";
+
+  /**
+   *  Gridmix minimum file size.
+   */
+  public static final String GRIDMIX_MINIMUM_FILE_SIZE =
+      "gridmix.min.file.size";
+}
diff --git a/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixRunMode.java b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixRunMode.java
new file mode 100644
index 0000000..5cfc6ef
--- /dev/null
+++ b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/GridMixRunMode.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+/**
+ * Gridmix run modes. 
+ *
+ */
+public class GridMixRunMode {
+   public static final int DATA_GENERATION = 1;
+   public static final int RUN_GRIDMIX = 2;
+   public static final int DATA_GENERATION_AND_RUN_GRIDMIX = 3;
+   private static String [] modeStr = {"DATA GENERATION",
+      "RUNNING GRIDMIX",
+      "DATA GENERATION AND RUNNING GRIDMIX"};
+   /**
+    * Get the appropriate message against the mode.
+    * @param mode - grimdix run mode either 1 or 2 or 3.
+    * @return - message as string.
+    */
+   public static String getMode(int mode){
+     return modeStr[mode-1];
+   }
+}
diff --git a/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixDataGeneration.java b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixDataGeneration.java
new file mode 100644
index 0000000..7d79ed6
--- /dev/null
+++ b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixDataGeneration.java
@@ -0,0 +1,203 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.JobStatus;
+import org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver;
+import org.apache.hadoop.mapred.gridmix.EchoUserResolver;
+import org.apache.hadoop.mapred.gridmix.SubmitterUserResolver;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.ContentSummary;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+import org.junit.Assert;
+import java.io.IOException;
+
+/**
+ * Verify the Gridmix data generation with various submission policies and 
+ * user resolver modes.
+ */
+public class TestGridMixDataGeneration {
+  private static final Log LOG = 
+      LogFactory.getLog(TestGridMixDataGeneration.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static JTClient jtClient;
+  private static JTProtocol rtClient;
+  private static Path gridmixDir;
+  private static int cSize;
+
+  @BeforeClass
+  public static void before() throws Exception {
+    String [] excludeExpList = {"java.net.ConnectException", 
+        "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(excludeExpList);
+    cluster.setUp();
+    cSize = cluster.getTTClients().size();
+    jtClient = cluster.getJTClient();
+    rtClient = jtClient.getProxy();
+    gridmixDir = new Path("hdfs:///user/" + UtilsForGridmix.getUserName()
+       + "/herriot-gridmix");
+    UtilsForGridmix.createDirs(gridmixDir, rtClient.getDaemonConf());
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    UtilsForGridmix.cleanup(gridmixDir,conf);
+    cluster.tearDown();
+  }
+  
+  /**
+   * Generate the data in a STRESS submission policy with SubmitterUserResolver 
+   * mode and verify whether the generated data matches with given 
+   * input size or not.
+   * @throws IOException
+   */
+  @Test
+  public void testGenerateDataWithSTRESSSubmission() throws Exception {
+    conf = rtClient.getDaemonConf();
+    final long inputSize = cSize * 128;
+    String [] runtimeValues ={"LOADJOB",
+       SubmitterUserResolver.class.getName(),
+       "STRESS",
+       inputSize+"m",
+       "file:///dev/null"}; 
+
+    int exitCode = UtilsForGridmix.runGridmixJob(gridmixDir, 
+      conf,GridMixRunMode.DATA_GENERATION, runtimeValues);
+    Assert.assertEquals("Data generation has failed.", 0 , exitCode);
+    checkGeneratedDataAndJobStatus(inputSize);
+  }
+  
+  /**
+   * Generate the data in a REPLAY submission policy with RoundRobinUserResolver
+   * mode and verify whether the generated data matches with the given 
+   * input size or not.
+   * @throws Exception
+   */
+  @Test
+  public void testGenerateDataWithREPLAYSubmission() throws Exception {
+    conf = rtClient.getDaemonConf();
+    final long inputSize = cSize * 300;
+    String [] runtimeValues ={"LOADJOB",
+       RoundRobinUserResolver.class.getName(),
+       "REPLAY",
+       inputSize +"m",
+       "file://" + cluster.getProxyUsersFilePath(),
+       "file:///dev/null"};
+    
+    int exitCode = UtilsForGridmix.runGridmixJob(gridmixDir, 
+       conf,GridMixRunMode.DATA_GENERATION, runtimeValues);
+    Assert.assertEquals("Data generation has failed.", 0 , exitCode);
+    checkGeneratedDataAndJobStatus(inputSize); 
+  }
+  
+  /**
+   * Generate the data in a SERIAL submission policy with EchoUserResolver
+   * mode and also set the no.of bytes per file in the data.Verify whether each 
+   * file size matches with given per file size or not and also 
+   * verify the overall size of generated data.
+   * @throws Exception
+   */
+  @Test
+  public void testGenerateDataWithSERIALSubmission() throws Exception {
+    conf = rtClient.getDaemonConf();
+    int perNodeSize = 500; // 500 mb per node data
+    final long inputSize = cSize * perNodeSize;
+    String [] runtimeValues ={"LOADJOB",
+       EchoUserResolver.class.getName(),
+       "SERIAL",
+       inputSize + "m",
+       "file:///dev/null"};
+    int bytesPerFile = 200; // 200 mb per file of data
+    String [] otherArgs = {
+      "-D", GridMixConfig.GRIDMIX_BYTES_PER_FILE + 
+      "=" + (bytesPerFile * 1024 * 1024)
+    };
+    int exitCode = UtilsForGridmix.runGridmixJob(gridmixDir, 
+       conf,GridMixRunMode.DATA_GENERATION, runtimeValues,otherArgs);
+    Assert.assertEquals("Data generation has failed.", 0 , exitCode);
+    LOG.info("Verify the eache file size in a generate data.");
+    verifyEachNodeSize(new Path(gridmixDir,"input"));
+    verifyNumOfFilesGeneratedInEachNode(new Path(gridmixDir,"input"), 
+       perNodeSize, bytesPerFile);
+    checkGeneratedDataAndJobStatus(inputSize);
+  }
+  
+  private void checkGeneratedDataAndJobStatus(long inputSize) 
+     throws IOException {
+    LOG.info("Verify the generated data size.");
+    long dataSize = getDataSize(new Path(gridmixDir,"input"));
+    Assert.assertTrue("Generate data has not matched with given size",
+       dataSize + 0.1 > inputSize || dataSize - 0.1 < inputSize);
+ 
+    JobClient jobClient = jtClient.getClient();
+    LOG.info("Verify the job status after completion of job.");
+    Assert.assertEquals("Job has not succeeded.", JobStatus.SUCCEEDED, 
+       jobClient.getAllJobs()[0].getRunState());
+  }
+  
+  private void verifyEachNodeSize(Path inputDir) throws IOException {
+    FileSystem fs = inputDir.getFileSystem(conf);
+    FileStatus [] fstatus = fs.listStatus(inputDir);
+    for (FileStatus fstat : fstatus) {
+      if ( fstat.isDir()) {
+        long fileSize = getDataSize(fstat.getPath());
+        Assert.assertTrue("The Size has not " + 
+           " matched with given per node file size(500mb)", 
+           fileSize + 0.1 > 500 || fileSize - 0.1 < 500);
+      }
+    }    
+  }
+
+  private void verifyNumOfFilesGeneratedInEachNode(Path inputDir, 
+     int nodeSize, int fileSize) throws IOException {
+    int expFileCount = Math.round(nodeSize/fileSize) + 
+       ((nodeSize%fileSize != 0)? 1:0);
+    FileSystem fs = inputDir.getFileSystem(conf);
+    FileStatus [] fstatus = fs.listStatus(inputDir);
+    for (FileStatus fstat : fstatus) {
+      if ( fstat.isDir()) {
+        FileSystem nodeFs = fstat.getPath().getFileSystem(conf);
+        long actFileCount = nodeFs.getContentSummary(fstat.getPath())
+           .getFileCount();
+        Assert.assertEquals("File count has not matched.", 
+           expFileCount, actFileCount);
+      }
+    }
+  }
+
+  private static long getDataSize(Path inputDir) throws IOException {
+    FileSystem fs = inputDir.getFileSystem(conf);
+    ContentSummary csmry = fs.getContentSummary(inputDir);
+    long dataSize = csmry.getLength();
+    dataSize = dataSize/(1024 * 1024);
+    return dataSize;
+  }
+}
diff --git a/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixFilePool.java b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixFilePool.java
new file mode 100644
index 0000000..bfcb5a9
--- /dev/null
+++ b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/TestGridMixFilePool.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapred.gridmix.FilePool;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileStatus;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+import java.io.IOException;
+import java.util.ArrayList;
+
+public class TestGridMixFilePool {
+  private static final Log LOG = LogFactory
+     .getLog(TestGridMixFilePool.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static JTProtocol remoteClient;
+  private static JTClient jtClient;
+  private static Path gridmixDir;
+  private static int clusterSize; 
+  
+  @BeforeClass
+  public static void before() throws Exception {
+    String []  excludeExpList = {"java.net.ConnectException", 
+       "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(excludeExpList);
+    cluster.setUp();
+    jtClient = cluster.getJTClient();
+    remoteClient = jtClient.getProxy();
+    clusterSize = cluster.getTTClients().size();
+    gridmixDir = new Path("hdfs:///user/" + UtilsForGridmix.getUserName()
+       + "/herriot-gridmix");
+    UtilsForGridmix.createDirs(gridmixDir, remoteClient.getDaemonConf());
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    UtilsForGridmix.cleanup(gridmixDir, conf);
+    cluster.tearDown();
+  }
+  
+  @Test
+  public void testFilesCountAndSizesForSpecifiedFilePool() throws Exception {
+    conf = remoteClient.getDaemonConf();
+    final long inputSize = clusterSize * 200;
+    int [] fileSizesInMB = {50, 100, 400, 50, 300, 10, 60, 40, 20 ,10 , 500};
+    long targetSize = Long.MAX_VALUE;
+    final int expFileCount = 13;
+    String [] runtimeValues ={"LOADJOB",
+       SubmitterUserResolver.class.getName(),
+       "STRESS",
+       inputSize+"m",
+       "file:///dev/null"}; 
+
+    int exitCode = UtilsForGridmix.runGridmixJob(gridmixDir, 
+       conf,GridMixRunMode.DATA_GENERATION, runtimeValues);
+    Assert.assertEquals("Data generation has failed.", 0 , exitCode);
+    // create files for given sizes.
+    createFiles(new Path(gridmixDir,"input"),fileSizesInMB);
+    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 100 * 1024 * 1024);
+    FilePool fpool = new FilePool(conf,new Path(gridmixDir,"input"));
+    fpool.refresh();
+    verifyFilesSizeAndCountForSpecifiedPool(expFileCount,targetSize, fpool);
+  }
+  
+  private void createFiles(Path inputDir, int [] fileSizes) 
+     throws Exception {
+    for (int size : fileSizes) {
+      UtilsForGridmix.createFile(size, inputDir, conf);
+    }
+  }
+  
+  private void verifyFilesSizeAndCountForSpecifiedPool(int expFileCount, 
+     long minFileSize, FilePool pool) throws IOException {
+    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();
+    long  actFilesSize = pool.getInputFiles(minFileSize, files)/(1024 * 1024);
+    long expFilesSize = 3100 ;
+    Assert.assertEquals("Files Size has not matched for specified pool.",
+       expFilesSize, actFilesSize);
+    int actFileCount = files.size();    
+    Assert.assertEquals("File count has not matched.", 
+       expFileCount, actFileCount);
+    int count = 0;
+    for (FileStatus fstat : files) {
+      String fp = fstat.getPath().toString();
+      count = count + ((fp.indexOf("datafile_") > 0)? 0 : 1);
+    }
+    Assert.assertEquals("Total folders are not matched with cluster size", 
+            clusterSize, count);
+  }
+}
diff --git a/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/UtilsForGridmix.java b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/UtilsForGridmix.java
new file mode 100644
index 0000000..09f2ffe
--- /dev/null
+++ b/src/contrib/gridmix/src/test/system/org/apache/hadoop/mapred/gridmix/UtilsForGridmix.java
@@ -0,0 +1,214 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred.gridmix;
+
+import java.io.IOException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.mapred.gridmix.Gridmix;
+import org.apache.hadoop.conf.Configuration;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.OutputStream;
+
+/**
+ * Gridmix utilities.
+ */
+public class UtilsForGridmix {
+  private static final Log LOG = LogFactory.getLog(UtilsForGridmix.class);
+
+  /**
+   * cleanup the folder or file.
+   * @param path - folder or file path.
+   * @param conf - cluster configuration 
+   * @throws IOException - If an I/O error occurs.
+   */
+  public static void cleanup(Path path, Configuration conf) 
+     throws IOException {
+    FileSystem fs = path.getFileSystem(conf);
+    fs.delete(path, true);
+    fs.close();
+  }
+
+  /**
+   * Get the login user.
+   * @return - login user as string..
+   * @throws IOException - if an I/O error occurs.
+   */
+  public static String getUserName() throws IOException {
+    return UserGroupInformation.getLoginUser().getUserName();
+  }
+  
+  /**
+   * Get the argument list for gridmix job.
+   * @param gridmixDir - gridmix parent directory.
+   * @param gridmixRunMode - gridmix modes either 1,2,3.
+   * @param values - gridmix runtime values.
+   * @param otherArgs - gridmix other generic args.
+   * @return - argument list as string array.
+   */
+  public static String [] getArgsList(Path gridmixDir, int gridmixRunMode, 
+    String [] values, String [] otherArgs) {
+    
+    String [] runtimeArgs = {
+       "-D", GridMixConfig.GRIDMIX_LOG_MODE + 
+       "=DEBUG",
+       "-D", GridMixConfig.GRIDMIX_OUTPUT_DIR + 
+       "=" + new Path(gridmixDir,"gridmix").toString(),
+       "-D", GridMixConfig.GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE 
+       + "=true",
+       "-D", GridMixConfig.GRIDMIX_JOB_TYPE 
+       + "=" + values[0],
+       "-D", GridMixConfig.GRIDMIX_USER_RESOLVER + 
+       "=" + values[1],
+       "-D", GridMixConfig.GRIDMIX_SUBMISSION_POLICY + 
+       "=" + values[2]
+    };
+    String [] classArgs;
+    if ((gridmixRunMode == GridMixRunMode.DATA_GENERATION || 
+      gridmixRunMode == GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX) && 
+      values[1].indexOf("RoundRobinUserResolver") > 0) {
+     classArgs = new String[]{
+        "-generate", values[3], 
+        "-users", values[4], 
+        new Path(gridmixDir,"input").toString(), 
+        values[5]};
+    } else if (gridmixRunMode == GridMixRunMode.DATA_GENERATION ||
+       gridmixRunMode == GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX){
+      classArgs = new String[]{
+         "-generate", values[3], new Path(gridmixDir,"input").toString(),
+         values[4]};
+    } else if(gridmixRunMode == GridMixRunMode.RUN_GRIDMIX 
+       && values[1].indexOf("RoundRobinUserResolver") > 0) {
+      classArgs = new String[]{         
+         "-users", values[3], 
+         new Path(gridmixDir,"input").toString(),
+         values[4]};
+    } else {
+      classArgs = new String[]{
+         new Path(gridmixDir,"input").toString(),values[3]};
+    }
+    
+    String [] args = new String [runtimeArgs.length + 
+       classArgs.length + ((otherArgs != null)?otherArgs.length:0)];
+    System.arraycopy(runtimeArgs, 0, args, 0, runtimeArgs.length);
+    if (otherArgs !=null) {
+      System.arraycopy(otherArgs, 0, args, runtimeArgs.length, 
+         otherArgs.length);
+      System.arraycopy(classArgs, 0, args, (runtimeArgs.length + 
+         otherArgs.length), classArgs.length);
+    } else {
+      System.arraycopy(classArgs, 0, args, runtimeArgs.length, 
+         classArgs.length);
+    }
+    return args;
+  }
+  
+  /**
+   * Create a file with specified size in mb.
+   * @param sizeInMB - file size in mb.
+   * @param inputDir - input directory.
+   * @param conf - cluster configuration.
+   * @throws Exception - if an exception occurs.
+   */
+  public static void createFile(int sizeInMB, Path inputDir, 
+     Configuration conf) throws Exception {
+    Date d = new Date();
+    SimpleDateFormat sdf = new SimpleDateFormat("ddMMyy_HHmmssS");
+    String formatDate = sdf.format(d);
+    FileSystem fs = inputDir.getFileSystem(conf);
+    OutputStream out = fs.create(new Path(inputDir,"datafile_" + formatDate));
+    final byte[] b = new byte[1024 * 1024];
+    for (int index = 0; index < sizeInMB; index++) {
+       out.write(b);
+    }    
+    out.close();
+    fs.close();
+  }
+  
+  /**
+   * Create directories for a path.
+   * @param path - directories path.
+   * @param conf  - cluster configuration.
+   * @throws IOException  - if an I/O error occurs.
+   */
+  public static void createDirs(Path path,Configuration conf) 
+     throws IOException {
+    FileSystem fs = path.getFileSystem(conf);
+    if (!fs.exists(path)) {
+       fs.mkdirs(path);
+    }
+  }
+  
+  /**
+   * Run the Gridmix job with given runtime arguments.
+   * @param gridmixDir - Gridmix parent directory.
+   * @param conf - cluster configuration.
+   * @param gridmixRunMode - gridmix run mode either 1,2,3
+   * @param runtimeValues -gridmix runtime values.
+   * @return - gridmix status either 0 or 1.
+   * @throws Exception
+   */
+  public static int runGridmixJob(Path gridmixDir, Configuration conf, 
+     int gridmixRunMode, String [] runtimeValues) throws Exception {
+    return runGridmixJob(gridmixDir, conf, gridmixRunMode, runtimeValues, null);
+  }
+  /**
+   * Run the Gridmix job with given runtime arguments.
+   * @param gridmixDir - Gridmix parent directory
+   * @param conf - cluster configuration.
+   * @param gridmixRunMode - gridmix run mode.
+   * @param runtimeValues - gridmix runtime values.
+   * @param otherArgs - gridmix other generic args.
+   * @return - gridmix status either 0 or 1.
+   * @throws Exception
+   */
+  
+  public static int runGridmixJob(Path gridmixDir, Configuration conf, 
+     int gridmixRunMode, String [] runtimeValues, 
+     String [] otherArgs) throws Exception {
+    Path  outputDir = new Path(gridmixDir, "gridmix");
+    Path inputDir = new Path(gridmixDir, "input");
+    LOG.info("Cleanup the data if data already exists.");
+    switch (gridmixRunMode) {
+      case GridMixRunMode.DATA_GENERATION :
+        cleanup(inputDir, conf);
+        cleanup(outputDir, conf);
+        break;
+      case GridMixRunMode.DATA_GENERATION_AND_RUN_GRIDMIX :
+        cleanup(inputDir, conf);
+        cleanup(outputDir, conf);
+        break;
+      case GridMixRunMode.RUN_GRIDMIX :
+        cleanup(outputDir, conf);
+        break;
+    }
+
+    final String [] args = UtilsForGridmix.getArgsList(gridmixDir,
+       gridmixRunMode, runtimeValues, otherArgs);
+    Gridmix gridmix = new Gridmix();
+    LOG.info("Submit a Gridmix job in " + runtimeValues[1] + 
+    " mode for " + GridMixRunMode.getMode(gridmixRunMode));
+    int exitCode = ToolRunner.run(conf, gridmix, args);
+    return exitCode;
+  }
+}
diff --git a/src/contrib/streaming/build.xml b/src/contrib/streaming/build.xml
index 1251199..fa6dd58 100644
--- a/src/contrib/streaming/build.xml
+++ b/src/contrib/streaming/build.xml
@@ -41,4 +41,10 @@ to call at top-level: ant deploy-contrib compile-core-test
    </antcall>
   </target>  
  
+  <!--Run all system tests.-->
+  <target name="test-system">
+    <antcall target="hadoopbuildcontrib.test-system">
+    </antcall>
+   </target>
+
 </project>
diff --git a/src/contrib/streaming/ivy.xml b/src/contrib/streaming/ivy.xml
index 7357205..5fa8620 100644
--- a/src/contrib/streaming/ivy.xml
+++ b/src/contrib/streaming/ivy.xml
@@ -43,11 +43,7 @@
     <dependency org="org.mortbay.jetty"
       name="jetty"
       rev="${jetty.version}"
-      conf="common->master"/>
-    <dependency org="org.mortbay.jetty"
-      name="servlet-api-2.5"
-      rev="${servlet-api-2.5.version}"
-      conf="common->master"/> 
+      conf="common->default"/>
 <!-- <dependency org="tomcat"
       name="jasper-runtime"
       rev="${jasper.version}"
diff --git a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
index 4f2eaf3..8f62a11 100644
--- a/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
+++ b/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java
@@ -943,7 +943,7 @@ public class StreamJob implements Tool {
       }
       if (!running_.isSuccessful()) {
         jobInfo();
-	LOG.error("Job not Successful!");
+	LOG.error("Job not successful. Error: " + running_.getFailureInfo());
 	return 1;
       }
       LOG.info("Job complete: " + jobId_);
diff --git a/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUserStreamingJob.java b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUserStreamingJob.java
new file mode 100644
index 0000000..a9d3f5e
--- /dev/null
+++ b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUserStreamingJob.java
@@ -0,0 +1,157 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.streaming.StreamJob;
+
+/**
+ * Verifying the job submissions with original user and other user
+ * with Linux task Controller.Original user should succeed and other
+ * user should not.
+ */
+public class TestLinuxTaskControllerOtherUserStreamingJob {
+  private static final Log LOG = LogFactory.
+      getLog(TestLinuxTaskControllerOtherUserStreamingJob.class);
+  private static MRCluster cluster = null;
+  private static JobClient jobClient = null;
+  private static JTProtocol remoteJTClient = null;
+  private static JTClient jtClient = null;
+  private static Configuration conf = new Configuration();
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+
+  @BeforeClass
+  public static void before() throws Exception {
+    cluster = MRCluster.createCluster(conf);
+    cluster.setUp();
+    jtClient = cluster.getJTClient();
+    jobClient = jtClient.getClient();
+    remoteJTClient = cluster.getJTClient().getProxy();
+    conf = remoteJTClient.getDaemonConf();
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cluster.tearDown();
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+  }
+  /**
+   * Submit a Streaming job with a correct user and verify it passes
+   * Submit a streaming job as diferent user and verify it fails
+   * @param none
+   * @return void
+   */
+  @Test
+  public void testStreamingJobSameAndDifferentUser() throws Exception {
+    executeStreamingJob(true);
+    executeStreamingJob(false);
+  }
+
+  //Executes streaming job as original user or other user.
+  private void executeStreamingJob(boolean sameUser) throws Exception {
+      conf = cluster.getConf();
+    if (sameUser == true) {
+      UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+      LOG.info("LoginUser:" + ugi);
+    } else {
+      conf.set("user.name","hadoop1");
+      LOG.info("user name changed is :" + conf.get("user.name"));
+    }
+
+    if (conf.get("mapred.task.tracker.task-controller").
+        equals("org.apache.hadoop.mapred.LinuxTaskController")) {
+      StreamJob streamJob = new StreamJob();
+      String shellFile = System.getProperty("user.dir") +
+        "/src/test/system/scripts/StreamMapper.sh";
+      String runtimeArgs [] = {
+        "-D", "mapred.job.name=Streaming job",
+        "-D", "mapred.map.tasks=1",
+        "-D", "mapred.reduce.tasks=1",
+        "-D", "mapred.tasktracker.tasks.sleeptime-before-sigkill=3000",
+        "-input", inputDir.toString(),
+        "-output", outputDir.toString(),
+        "-mapper", "StreamMapper.sh",
+        "-reducer","/bin/cat",
+        "-file", shellFile
+      };
+
+      createInput(inputDir, conf);
+      cleanup(outputDir, conf);
+
+      //If job submtitted with same user, it should pass
+      //If job submitted with different user, it should fail with assertion
+      //the testcase should assert.
+      if (sameUser == true) {
+        Assert.assertEquals(0, ToolRunner.run(conf, streamJob, runtimeArgs));
+      } else {
+        if ((ToolRunner.run(conf, streamJob, runtimeArgs)) != 0) {
+          LOG.info("Job failed as expected");
+        } else {
+          Assert.fail("Job passed with different user");
+        }
+      }
+    }
+  }
+
+  //Create Input directory in dfs
+  private static void createInput(Path inDir, Configuration conf)
+      throws IOException {
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:"
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL,
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while(i++ < 200) {
+      file.writeBytes(i + "\n");
+    }
+    file.close();
+  }
+
+  //Cleanup directories in dfs.
+  private static void cleanup(Path dir, Configuration conf)
+      throws IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+
+}
diff --git a/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestStreamingJobProcessTree.java b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestStreamingJobProcessTree.java
new file mode 100644
index 0000000..f4090fa
--- /dev/null
+++ b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestStreamingJobProcessTree.java
@@ -0,0 +1,351 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.streaming.StreamJob;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+
+/**
+ * Increase memory usage beyond the memory limits of streaming job and 
+ * verify whether task manager logs the process tree status 
+ * before killing or not.
+ */
+public class TestStreamingJobProcessTree {
+  private static final Log LOG = LogFactory
+     .getLog(TestStreamingJobProcessTree.class);
+  private static MRCluster cluster;
+  private static Configuration conf = new Configuration();
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  
+  @BeforeClass
+  public static void before() throws Exception {
+    String [] excludeExpList = {"java.net.ConnectException",
+      "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(excludeExpList);
+    cluster.setUp();
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+  @AfterClass
+  public static void after() throws Exception {
+   cleanup(inputDir, conf);
+   cleanup(outputDir, conf);
+   cluster.tearDown();
+  }
+  
+  /**
+   * Increase the memory limit for map task and verify whether the 
+   * task manager logs the process tree status before killing or not.
+   * @throws IOException - If an I/O error occurs.
+   */
+  @Test
+  public void testStreamingJobProcTreeCleanOfMapTask() throws
+     IOException {
+    String runtimeArgs [] = {
+        "-D", "mapred.job.name=ProcTreeStreamJob",
+        "-D", "mapred.map.tasks=1",
+        "-D", "mapred.reduce.tasks=0",
+        "-D", "mapred.map.max.attempts=1",
+        "-D", "mapred.cluster.max.map.memory.mb=2048",
+        "-D", "mapred.cluster.reduce.memory.mb=1024",
+        "-D", "mapred.cluster.max.reduce.memory.mb=2048",
+        "-D", "mapred.cluster.map.memory.mb=1024",
+        "-D", "mapred.job.map.memory.mb=512"
+    };
+    
+    String [] otherArgs = new String[] {
+            "-input", inputDir.toString(),
+            "-output", outputDir.toString(),
+            "-mapper", "ProcessTree.sh",
+    };
+    JobID jobId = getJobId(runtimeArgs, otherArgs);
+    LOG.info("Job ID:" + jobId);
+    if (jobId == null) {
+      jobId = getJobId(runtimeArgs, otherArgs);
+    }
+    Assert.assertNotNull("Job ID not found for 1 min", jobId);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        cluster.getJTClient().isJobStarted(jobId));
+    TaskInfo taskInfo = getTaskInfo(jobId, true);
+    Assert.assertNotNull("TaskInfo is null",taskInfo);
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        cluster.getJTClient().isTaskStarted(taskInfo));
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    int counter = 0;
+    TaskInfo tempTaskInfo;
+    while (counter++ < 60) {
+      if (taskInfo.getTaskStatus().length == 0) {
+        UtilsForTests.waitFor(1000);
+        tempTaskInfo = taskInfo;
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      }else if (taskInfo.getTaskStatus()[0].getRunState() ==
+          TaskStatus.State.RUNNING) {
+        UtilsForTests.waitFor(1000);
+        tempTaskInfo = taskInfo;
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else {
+        break;
+      }
+
+      if (taskInfo == null) {
+        taskInfo = tempTaskInfo;
+        break;
+      }
+    }
+
+    verifyProcessTreeOverLimit(taskInfo,jobId);
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter++ < 60) {
+      if (jInfo == null) {
+        break;
+      } else if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(100);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+    UtilsForTests.waitFor(1000);
+  }
+  
+  /**
+   * Increase the memory limit for reduce task and verify whether the 
+   * task manager logs the process tree status before killing or not.
+   * @throws IOException - If an I/O error occurs.
+   */
+  @Test
+  public void testStreamingJobProcTreeCleanOfReduceTask() throws
+     IOException {
+    String runtimeArgs [] = {
+            "-D", "mapred.job.name=ProcTreeStreamJob",
+            "-D", "mapred.reduce.tasks=1",
+            "-D", "mapred.map.tasks=1",
+            "-D", "mapred.reduce.max.attempts=1",
+            "-D", "mapred.cluster.max.map.memory.mb=2048",
+            "-D", "mapred.cluster.map.memory.mb=1024",
+            "-D", "mapred.cluster.max.reduce.memory.mb=20248",
+            "-D", "mapred.cluster.reduce.memory.mb=1024",
+            "-D", "mapred.job.reduce.memory.mb=512"};
+
+    String [] otherArgs = new String[] {
+            "-input", inputDir.toString(),
+            "-output", outputDir.toString(),
+            "-mapper", "/bin/cat",
+            "-reducer", "ProcessTree.sh"
+    };
+
+    cleanup(outputDir, conf);
+    JobID jobId = getJobId(runtimeArgs, otherArgs);
+    if (jobId == null) {
+      jobId = getJobId(runtimeArgs, otherArgs);
+    }
+    Assert.assertNotNull("Job ID not found for 1 min", jobId);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        cluster.getJTClient().isJobStarted(jobId));
+    TaskInfo taskInfo = getTaskInfo(jobId, false);
+    Assert.assertNotNull("TaskInfo is null",taskInfo);
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        cluster.getJTClient().isTaskStarted(taskInfo));    
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    int counter = 0;
+    TaskInfo tempTaskInfo;
+    while (counter++ < 60) {
+      if (taskInfo.getTaskStatus().length == 0) {
+        UtilsForTests.waitFor(1000);
+        tempTaskInfo = taskInfo;
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      }else if (taskInfo.getTaskStatus()[0].getRunState() == 
+          TaskStatus.State.RUNNING) {
+        UtilsForTests.waitFor(1000);
+        tempTaskInfo = taskInfo;
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else {
+        break;
+      }
+      if (taskInfo == null) {
+        taskInfo = tempTaskInfo;
+        break;
+      }
+    }
+    verifyProcessTreeOverLimit(taskInfo,jobId);
+    JobInfo jInfo = wovenClient.getJobInfo(jobId); 
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter++ < 60) {
+      if(jInfo == null) {
+        break;
+      } else if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+  }
+  
+  private void verifyProcessTreeOverLimit(TaskInfo taskInfo, JobID jobId) 
+      throws IOException {
+    String taskOverLimitPatternString = 
+      "TaskTree \\[pid=[0-9]*,tipID=.*\\] is "
+      + "running beyond memory-limits. "
+      + "Current usage : [0-9]*bytes. Limit : %sbytes. Killing task.";
+    Pattern taskOverLimitPattern = 
+        Pattern.compile(String.format(taskOverLimitPatternString, 
+        String.valueOf(512 * 1024 * 1024L)));
+    LOG.info("Task OverLimit Pattern:" + taskOverLimitPattern);
+    TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);
+    JobClient jobClient = cluster.getJTClient().getClient();
+    RunningJob runJob = jobClient.getJob(jobId);
+    String[] taskDiagnostics = runJob.getTaskDiagnostics(taskAttID);
+    Assert.assertNotNull("Task diagnostics is null.", taskDiagnostics);
+    for (String strVal : taskDiagnostics) {
+      Matcher mat = taskOverLimitPattern.matcher(strVal);
+      Assert.assertTrue("Taskover limit error message is not matched.", 
+          mat.find());
+    }
+  }
+  
+  private String[] buildArgs(String [] runtimeArgs, String[] otherArgs) {
+    String shellFile = System.getProperty("user.dir") + 
+        "/src/test/system/scripts/ProcessTree.sh";
+    
+    String fileArgs[] = new String[] {"-files", shellFile };
+    int size = fileArgs.length + runtimeArgs.length + otherArgs.length;
+    String args[]= new String[size];
+    int index = 0;
+    for (String fileArg : fileArgs) {
+      args[index++] = fileArg;
+    }
+    for (String runtimeArg : runtimeArgs) {
+      args[index++] = runtimeArg;
+    }
+    for (String otherArg : otherArgs) {
+      args[index++] = otherArg;
+    }
+    return args;
+  }
+  
+  private JobID getJobId(String [] runtimeArgs, String [] otherArgs) 
+      throws IOException {
+    JobID jobId = null;
+    final RunStreamJob runSJ;
+    StreamJob streamJob = new StreamJob();
+    int counter = 0;
+    JTClient jtClient = cluster.getJTClient();
+    JobClient jobClient = jtClient.getClient();
+    int totalJobs = jobClient.getAllJobs().length;
+    String [] args = buildArgs(runtimeArgs, otherArgs);
+    cleanup(outputDir, conf);
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    runSJ = new RunStreamJob(conf, streamJob, args);
+    runSJ.start();
+    while (counter++ < 60) {
+      if (jobClient.getAllJobs().length - totalJobs == 0) {
+        UtilsForTests.waitFor(1000);
+      } else if (jobClient.getAllJobs()[0].getRunState() == JobStatus.RUNNING) {
+        jobId = jobClient.getAllJobs()[0].getJobID();
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+      }
+    }  
+    return jobId;
+  }
+  
+  private TaskInfo getTaskInfo(JobID jobId, boolean isMap) 
+      throws IOException {
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(jobId);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        if (taskinfo.getTaskID().isMap() == isMap) {
+          return taskinfo;
+        }
+      }
+    }
+    return null;
+  }
+
+  private static void createInput(Path inDir, Configuration conf) 
+     throws IOException {
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+    FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    String input="Process tree cleanup of Streaming job tasks.";
+    file.writeBytes(input + "\n");
+    file.close();
+  }
+
+  private static void cleanup(Path dir, Configuration conf) 
+      throws IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+  
+  class RunStreamJob extends Thread {
+    Configuration jobConf;
+    Tool tool;
+    String [] args;
+    public RunStreamJob(Configuration jobConf, Tool tool, 
+        String [] args) {
+      this.jobConf = jobConf;
+      this.tool = tool;
+      this.args = args;
+    }
+    public void run() {
+      try {
+        ToolRunner.run(jobConf, tool, args);
+      } catch(InterruptedException iexp) {
+        LOG.warn("Thread is interrupted:" + iexp.getMessage());
+      } catch(Exception exp) {
+        LOG.warn("Exception:" + exp.getMessage());
+      }
+    }
+  }
+}
diff --git a/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestTaskKillingOfStreamingJob.java b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestTaskKillingOfStreamingJob.java
new file mode 100644
index 0000000..a06f9ae
--- /dev/null
+++ b/src/contrib/streaming/src/test/system/org/apache/hadoop/mapred/TestTaskKillingOfStreamingJob.java
@@ -0,0 +1,310 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapred.JobClient.NetworkedJob;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.streaming.StreamJob;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.Assert;
+
+public class TestTaskKillingOfStreamingJob {
+  private static final Log LOG = LogFactory
+          .getLog(TestTaskKillingOfStreamingJob.class);
+  private static MRCluster cluster;
+  private static Configuration conf = new Configuration();
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private JTClient jtClient = null;
+  private JobClient client = null;
+  private JTProtocol wovenClient = null;
+
+  @BeforeClass
+  public static void before() throws Exception {
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+  }
+  
+  /**
+   * Set the sleep time for the tasks is 3 seconds and kill the task using sigkill.
+   * Verify whether task is killed after 3 seconds or not. 
+   */
+  @Test
+  public void testStatusOfKilledTaskWithSignalSleepTime() 
+      throws IOException, Exception {
+    String runtimeArgs [] = {
+        "-D", "mapred.job.name=Numbers Sum",
+        "-D", "mapred.map.tasks=1",
+        "-D", "mapred.reduce.tasks=1",
+        "-D", "mapred.tasktracker.tasks.sleeptime-before-sigkill=3000" };
+
+    JobID jobId = getJobIdOfRunningStreamJob(runtimeArgs);    
+    Assert.assertNotNull("Job ID not found for 1 min", jobId);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    
+    TaskInfo taskInfo = getTaskInfoOfRunningStreamJob(jobId);
+    Assert.assertNotNull("TaskInfo is null",taskInfo);
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    JobInfo jInfo = wovenClient.getJobInfo(jobId); 
+    NetworkedJob networkJob = client.new NetworkedJob(jInfo.getStatus());
+    TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);
+    networkJob.killTask(taskAttID, false);
+
+    int counter = 0;
+    while (counter++ < 60) {
+      if (taskInfo.getTaskStatus().length == 0) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else if (taskInfo.getTaskStatus()[0].getRunState() == 
+          TaskStatus.State.RUNNING) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else if (taskInfo.getTaskStatus()[0].getRunState() == 
+          TaskStatus.State.KILLED_UNCLEAN) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else {
+        break;
+      }
+    }
+    Assert.assertTrue("Task has been killed before sigkill " + 
+        "sleep time of 3 secs.", counter > 3 && TaskStatus.State.KILLED == 
+        taskInfo.getTaskStatus()[0].getRunState());
+
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+    Assert.assertEquals("Job has not been succeeded.", 
+            jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+  }
+ 
+  /**
+   * Set the maximum attempts for the maps and reducers are one.
+   * Failed the task and verify whether streaming job is failed or not.
+   */
+  @Test
+  public void testStreamingJobStatusForFailedTask() throws IOException {
+    String runtimeArgs [] = {
+        "-D", "mapred.job.name=Numbers Sum",
+        "-D", "mapred.map.tasks=1",
+        "-D", "mapred.reduce.tasks=1",
+        "-D", "mapred.map.max.attempts=1",
+        "-D", "mapred.reduce.max.attempts=1"};
+
+    JobID jobId = getJobIdOfRunningStreamJob(runtimeArgs);
+    Assert.assertNotNull("Job ID not found for 1 min", jobId);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+
+    TaskInfo taskInfo = getTaskInfoOfRunningStreamJob(jobId);
+    Assert.assertNotNull("TaskInfo is null",taskInfo);
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+    
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    NetworkedJob networkJob = client.new NetworkedJob(jInfo.getStatus());
+    TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);
+    networkJob.killTask(taskAttID, true);
+
+    int counter = 0;
+    while (counter++ < 60) {
+      if (taskInfo.getTaskStatus().length == 0) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      }else if (taskInfo.getTaskStatus()[0].getRunState() == 
+          TaskStatus.State.RUNNING) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else if (taskInfo.getTaskStatus()[0].getRunState() == 
+          TaskStatus.State.FAILED_UNCLEAN) {
+        UtilsForTests.waitFor(1000);
+        taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      } else {
+        break;
+      }
+    }
+    Assert.assertTrue("Task has not been Failed" , TaskStatus.State.FAILED == 
+        taskInfo.getTaskStatus()[0].getRunState());
+
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+    Assert.assertEquals("Job has not been failed", 
+        jInfo.getStatus().getRunState(), JobStatus.FAILED);
+  }
+
+  private TaskInfo getTaskInfoOfRunningStreamJob(JobID jobId) 
+      throws IOException {
+    TaskInfo taskInfo = null;
+    wovenClient = cluster.getJTClient().getProxy();
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    JobStatus jobStatus = jInfo.getStatus();
+    // Make sure that map is running and start progress 10%. 
+    while (jobStatus.mapProgress() < 0.1f) {
+      UtilsForTests.waitFor(100);
+      jobStatus = wovenClient.getJobInfo(jobId).getStatus();
+    }
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(jobId);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+      }
+    }
+    return taskInfo;
+  }
+  
+  private JobID getJobIdOfRunningStreamJob(String [] runtimeArgs) 
+      throws IOException {
+    JobID jobId = null;
+    StreamJob streamJob = new StreamJob();
+    int counter = 0;
+    jtClient = cluster.getJTClient();
+    client = jtClient.getClient();
+    int totalJobs = client.getAllJobs().length;
+    String [] streamingArgs = generateArgs(runtimeArgs);
+    cleanup(outputDir, conf);
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    final RunStreamingJob streamJobThread = new RunStreamingJob(conf,
+        streamJob,streamingArgs);
+    streamJobThread.start();
+    while (counter++ < 60) {
+      if (client.getAllJobs().length - totalJobs == 0) {
+        UtilsForTests.waitFor(1000);
+      } else if (client.getAllJobs()[0].getRunState() == JobStatus.RUNNING) {
+        jobId = client.getAllJobs()[0].getJobID();
+        break;
+      } else {
+       UtilsForTests.waitFor(1000);
+      }
+    }  
+    return jobId;
+  }
+  
+
+  private static void createInput(Path inDir, Configuration conf) 
+      throws IOException {
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while(i++ < 200) {
+      file.writeBytes(i + "\n");
+    }
+    file.close();
+  }
+  
+  private static void cleanup(Path dir, Configuration conf) 
+      throws IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+
+  private String[] generateArgs(String [] runtimeArgs) {
+    String shellFile = System.getProperty("user.dir") + 
+        "/src/test/system/scripts/StreamMapper.sh";
+    String [] otherArgs = new String[] {
+            "-input", inputDir.toString(),
+            "-output", outputDir.toString(),
+            "-mapper", "StreamMapper.sh",
+            "-reducer", "/bin/cat"
+    };
+    String fileArgs[] = new String[] {"-files", shellFile };
+    int size = fileArgs.length + runtimeArgs.length + otherArgs.length;
+    String args[]= new String[size];
+    int index = 0;
+    for (String fileArg : fileArgs) {
+      args[index++] = fileArg;
+    }
+
+    for (String runtimeArg : runtimeArgs) {
+      args[index++] = runtimeArg;
+    }
+
+    for (String otherArg : otherArgs) {
+      args[index++] = otherArg;
+    }
+    return args;
+  }
+  
+  class RunStreamingJob extends Thread {
+    Configuration jobConf;
+    Tool tool;
+    String [] args;
+    public RunStreamingJob(Configuration jobConf, Tool tool, String [] args) {
+      this.jobConf = jobConf;
+      this.tool = tool;
+      this.args = args;
+    }
+    public void run() {
+      try {
+        runStreamingJob();
+      } catch(InterruptedException iexp) {
+        LOG.warn("Thread is interrupted:" + iexp.getMessage());
+      } catch(Exception exp) {
+        LOG.warn("Exception:" + exp.getMessage());
+      }
+    }
+    private void runStreamingJob() throws Exception{
+       ToolRunner.run(jobConf, tool, args);
+    }
+  }
+}
diff --git a/src/contrib/streaming/src/test/system/scripts/ProcessTree.sh b/src/contrib/streaming/src/test/system/scripts/ProcessTree.sh
new file mode 100644
index 0000000..6b5adf6
--- /dev/null
+++ b/src/contrib/streaming/src/test/system/scripts/ProcessTree.sh
@@ -0,0 +1,40 @@
+#!/bin/sh
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+##############################################################
+## It creates the subprocess and keep increasing the memory ##
+##############################################################
+set -x
+if [ $# -eq 0 ]
+then
+  StrVal="Hadoop is framework for data intensive distributed applications. \
+Hadoop enables applications to work with thousands of nodes."
+  i=1
+else
+  StrVal=$1
+  i=$2
+fi
+
+if [ $i -lt 5 ]
+then
+   sh $0 "$StrVal-AppendingStr" `expr $i + 1`
+else
+   echo $StrVal
+   while [ 1 ]
+   do 
+    sleep 5
+   done
+fi
diff --git a/src/contrib/streaming/src/test/system/scripts/StreamMapper.sh b/src/contrib/streaming/src/test/system/scripts/StreamMapper.sh
new file mode 100644
index 0000000..f5169ca
--- /dev/null
+++ b/src/contrib/streaming/src/test/system/scripts/StreamMapper.sh
@@ -0,0 +1,41 @@
+#!/bin/sh
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#Sum the numbers from 1 to given digit as input.
+cat > mapfile 2>&1
+output="X"
+for digit in `cat mapfile`
+do
+  num=1
+  while [ $num -le $digit ] 
+  do
+    if [ $num -eq 1 ]
+    then
+      indata=$num
+    else
+      indata=`expr ${indata} + ${num}`
+    fi
+    num=`expr $num + 1`
+  done
+
+  if [ "${output}" == "X" ]
+  then
+    output="$digit:$indata"
+  else
+    output="$output $digit:$indata"
+  fi
+done
+echo $output
diff --git a/src/test/aop/build/aop.xml b/src/test/aop/build/aop.xml
index f5b308c..34508c6 100644
--- a/src/test/aop/build/aop.xml
+++ b/src/test/aop/build/aop.xml
@@ -113,12 +113,21 @@
   <!-- ================ -->
   <!-- run system tests -->
   <!-- ================ -->
-  <target name="test-system" depends="ivy-retrieve-common"
-    description="Run system tests">
+  <target name="test-system" depends="ivy-retrieve-common,
+    compile-core, tools-jar, compile-c++-libhdfs"
+    description="Run system tests, contrib project system tests">
     <subant buildpath="build.xml" target="jar-test-system"/>
     <macro-jar-examples build.dir="${system-test-build-dir}"
                         basedir="${system-test-build-dir}/examples">
     </macro-jar-examples>
+    <!-- For contrib project system tests -->
+    <subant target="test-system-contrib">
+       <property name="hadoop.home" value="${hadoop.home}"/>
+       <property name="hadoop.conf.dir" value="${hadoop.conf.dir}"/>
+       <property name="hadoop.conf.dir.deployed"
+           value="${hadoop.conf.dir.deployed}"/>
+       <fileset file="${contrib.dir}/build.xml"/>
+    </subant>
     <macro-test-runner test.file="${test.all.tests.file}"
                        classpath="test.system.classpath"
                        test.dir="${system-test-build-dir}/test"
diff --git a/src/test/system/aop/org/apache/hadoop/mapred/JTProtocolAspect.aj b/src/test/system/aop/org/apache/hadoop/mapred/JTProtocolAspect.aj
index 52e5d0e..73c2f0c 100644
--- a/src/test/system/aop/org/apache/hadoop/mapred/JTProtocolAspect.aj
+++ b/src/test/system/aop/org/apache/hadoop/mapred/JTProtocolAspect.aj
@@ -19,7 +19,6 @@
 package org.apache.hadoop.mapred;
 
 import java.io.IOException;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.TaskID;
@@ -27,6 +26,7 @@ import org.apache.hadoop.mapreduce.test.system.JTProtocol;
 import org.apache.hadoop.mapreduce.test.system.JobInfo;
 import org.apache.hadoop.mapreduce.test.system.TTInfo;
 import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapred.StatisticsCollectionHandler;
 
 /**
  * Aspect which injects the basic protocol functionality which is to be
@@ -79,4 +79,49 @@ public aspect JTProtocolAspect {
   public String JTProtocol.getJobHistoryLocationForRetiredJob(JobID jobID) throws IOException {
     return "";
   }
+  
+  public boolean JTProtocol.isBlackListed(String trackerID) throws IOException {
+    return false;
+  }
+  
+  public String JTProtocol.getJobSummaryFromLog(JobID jobId, 
+      String filePattern) throws IOException {
+    return null;
+  }
+
+  public String JTProtocol.getJobSummaryInfo(JobID jobId) throws IOException {
+    return null;
+  }
+  
+  public int JTProtocol.getTaskTrackerLevelStatistics(TaskTrackerStatus
+      ttStatus, String timePeriod, String totalTasksOrSucceededTasks)
+      throws IOException {
+    return 0;
+  }
+
+  public int JTProtocol.getInfoFromAllClients(String timePeriod,
+      String totalTasksOrSucceededTasks) throws IOException {
+    return 0;
+  }
+
+  public StatisticsCollectionHandler JTProtocol.
+      getInfoFromAllClientsForAllTaskType() throws Exception {
+    return null;
+  }
+
+  public int JTProtocol.getTaskTrackerHeartbeatInterval()
+      throws Exception {
+    return -1;
+  }
+  
+  public void JTProtocol.accessHistoryData(JobID jobId) throws Exception{
+    
+  }
+
+  public boolean JTProtocol.isNodeDecommissioned(String ttClientHostName) 
+       throws IOException {
+   return false;
+  }
+
+
 }
diff --git a/src/test/system/aop/org/apache/hadoop/mapred/JobInProgressAspect.aj b/src/test/system/aop/org/apache/hadoop/mapred/JobInProgressAspect.aj
index ac2373f..060fbff 100644
--- a/src/test/system/aop/org/apache/hadoop/mapred/JobInProgressAspect.aj
+++ b/src/test/system/aop/org/apache/hadoop/mapred/JobInProgressAspect.aj
@@ -35,24 +35,62 @@ privileged aspect JobInProgressAspect {
    */
   public JobInfo JobInProgress.getJobInfo() {
     String historyLoc = getHistoryPath();
-    if (tasksInited.get()) {
-      return new JobInfoImpl(
+    JobInfoImpl jobInfoImpl;
+    if (tasksInited) {
+      jobInfoImpl = new JobInfoImpl(
           this.getJobID(), this.isSetupLaunched(), this.isSetupFinished(), this
               .isCleanupLaunched(), this.runningMaps(), this.runningReduces(),
           this.pendingMaps(), this.pendingReduces(), this.finishedMaps(), this
               .finishedReduces(), this.getStatus(), historyLoc, this
               .getBlackListedTrackers(), false, this.numMapTasks,
-          this.numReduceTasks, this.isHistoryFileCopied());
+          this.numReduceTasks);
     } else {
-      return new JobInfoImpl(
+      jobInfoImpl = new JobInfoImpl(
           this.getJobID(), false, false, false, 0, 0, this.pendingMaps(), this
               .pendingReduces(), this.finishedMaps(), this.finishedReduces(),
           this.getStatus(), historyLoc, this.getBlackListedTrackers(), this
-              .isComplete(), this.numMapTasks, this.numReduceTasks, 
-              this.isHistoryFileCopied());
+              .isComplete(), this.numMapTasks, this.numReduceTasks);
     }
+    jobInfoImpl.setFinishTime(getJobFinishTime());
+    jobInfoImpl.setLaunchTime(getJobLaunchTime());
+    jobInfoImpl.setNumSlotsPerReduce(getJobNumSlotsPerReduce());
+    jobInfoImpl.setNumSlotsPerMap(getJobNumSlotsPerMap());
+    return jobInfoImpl;
   }
   
+  private long JobInProgress.getJobFinishTime() {
+    long finishTime = 0;
+    if (this.isComplete()) {
+      finishTime = this.getFinishTime();
+    }
+    return finishTime;
+  }
+
+  private long JobInProgress.getJobLaunchTime() {
+    long LaunchTime = 0;
+    if (this.isComplete()) {
+      LaunchTime = this.getLaunchTime();
+    }
+    return LaunchTime;
+  }
+
+  private int JobInProgress.getJobNumSlotsPerReduce() {
+    int numSlotsPerReduce = 0;
+    if (this.isComplete()) {
+      numSlotsPerReduce = this.getNumSlotsPerReduce();
+    }
+    return numSlotsPerReduce;
+  }
+
+  private int JobInProgress.getJobNumSlotsPerMap() {
+    int numSlotsPerMap = 0;
+    if (this.isComplete()) {
+      numSlotsPerMap = this.getNumSlotsPerMap();
+    }
+    return numSlotsPerMap;
+ }
+
+
   private String JobInProgress.getHistoryPath() {
     String historyLoc = "";
     if(this.isComplete()) {
diff --git a/src/test/system/aop/org/apache/hadoop/mapred/JobTrackerAspect.aj b/src/test/system/aop/org/apache/hadoop/mapred/JobTrackerAspect.aj
index 8261184..8b2efaa 100644
--- a/src/test/system/aop/org/apache/hadoop/mapred/JobTrackerAspect.aj
+++ b/src/test/system/aop/org/apache/hadoop/mapred/JobTrackerAspect.aj
@@ -21,20 +21,34 @@ package org.apache.hadoop.mapred;
 import java.io.IOException;
 import java.util.List;
 import java.util.ArrayList;
+import java.util.LinkedList;
 import java.util.Set;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobStatus;
+import org.apache.hadoop.mapred.JobHistory.Keys;
 import org.apache.hadoop.mapred.JobTracker.RetireJobInfo;
+import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.TaskID;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapred.TaskTrackerStatus;
+import org.apache.hadoop.mapred.StatisticsCollector;
+import org.apache.hadoop.mapred.StatisticsCollectionHandler;
 import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;
 import org.apache.hadoop.mapreduce.test.system.JTProtocol;
 import org.apache.hadoop.mapreduce.test.system.JobInfo;
 import org.apache.hadoop.mapreduce.test.system.TTInfo;
 import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.ClusterMetrics;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.test.system.DaemonProtocol;
+import org.apache.hadoop.util.Shell.ShellCommandExecutor;
+import org.apache.hadoop.util.Shell;
+import org.apache.hadoop.util.StringUtils;
 
 /**
  * Aspect class which injects the code for {@link JobTracker} class.
@@ -43,6 +57,8 @@ import org.apache.hadoop.test.system.DaemonProtocol;
 public privileged aspect JobTrackerAspect {
 
 
+  private static JobTracker tracker;
+  
   public Configuration JobTracker.getDaemonConf() throws IOException {
     return conf;
   }
@@ -155,6 +171,10 @@ public privileged aspect JobTrackerAspect {
     return retireJobs.get(
         org.apache.hadoop.mapred.JobID.downgrade(id))!=null?true:false;
   }
+  
+  public boolean JobTracker.isBlackListed(String trackerName) throws IOException {
+    return isBlacklisted(trackerName);
+  }
 
   public String JobTracker.getJobHistoryLocationForRetiredJob(
       JobID id) throws IOException {
@@ -203,6 +223,7 @@ public privileged aspect JobTrackerAspect {
       tracker.LOG.warn("Unable to get the user information for the " +
       		"Jobtracker");
     }
+    this.tracker = tracker;
     tracker.setReady(true);
   }
   
@@ -226,4 +247,309 @@ public privileged aspect JobTrackerAspect {
             .isJobCleanupTask()), trackers);
     return info;
   }
+  
+  /**
+   * Get the job summary details from the jobtracker log files.
+   * @param jobId - job id
+   * @param filePattern - jobtracker log file pattern.
+   * @return String - Job summary details of given job id.
+   * @throws IOException if any I/O error occurs.
+   */
+  public String JobTracker.getJobSummaryFromLogs(JobID jobId,
+      String filePattern) throws IOException {
+    String pattern = "JobId=" + jobId.toString() + ",submitTime";
+    String[] cmd = new String[] {
+                   "bash",
+                   "-c",
+                   "grep -i " 
+                 + pattern + " " 
+                 + filePattern + " " 
+                 + "| sed s/'JobSummary: '/'^'/g | cut -d'^' -f2"};
+    ShellCommandExecutor shexec = new ShellCommandExecutor(cmd);
+    shexec.execute();
+    return shexec.getOutput();
+  }
+  
+  /**
+   * Get the job summary information for given job id.
+   * @param jobId - job id.
+   * @return String - Job summary details as key value pair.
+   * @throws IOException if any I/O error occurs.
+   */
+  public String JobTracker.getJobSummaryInfo(JobID jobId) throws IOException {
+    StringBuffer jobSummary = new StringBuffer();
+    JobInProgress jip = jobs.
+        get(org.apache.hadoop.mapred.JobID.downgrade(jobId));
+    if (jip == null) {
+      LOG.warn("Job has not been found - " + jobId);
+      return null;
+    }
+    JobProfile profile = jip.getProfile();
+    JobStatus status = jip.getStatus();
+    final char[] charsToEscape = {StringUtils.COMMA, '=', 
+        StringUtils.ESCAPE_CHAR};
+    String user = StringUtils.escapeString(profile.getUser(), 
+        StringUtils.ESCAPE_CHAR, charsToEscape);
+    String queue = StringUtils.escapeString(profile.getQueueName(), 
+        StringUtils.ESCAPE_CHAR, charsToEscape);
+    Counters jobCounters = jip.getJobCounters();
+    long mapSlotSeconds = (jobCounters.getCounter(
+        JobInProgress.Counter.SLOTS_MILLIS_MAPS) + 
+        jobCounters.getCounter(JobInProgress.
+        Counter.FALLOW_SLOTS_MILLIS_MAPS)) / 1000;
+    long reduceSlotSeconds = (jobCounters.getCounter(
+        JobInProgress.Counter.SLOTS_MILLIS_REDUCES) + 
+       jobCounters.getCounter(JobInProgress.
+       Counter.FALLOW_SLOTS_MILLIS_REDUCES)) / 1000;
+    jobSummary.append("jobId=");
+    jobSummary.append(jip.getJobID());
+    jobSummary.append(",");
+    jobSummary.append("startTime=");
+    jobSummary.append(jip.getStartTime());
+    jobSummary.append(",");
+    jobSummary.append("launchTime=");
+    jobSummary.append(jip.getLaunchTime());
+    jobSummary.append(",");
+    jobSummary.append("finishTime=");
+    jobSummary.append(jip.getFinishTime());
+    jobSummary.append(",");
+    jobSummary.append("numMaps=");
+    jobSummary.append(jip.getTasks(TaskType.MAP).length);
+    jobSummary.append(",");
+    jobSummary.append("numSlotsPerMap=");
+    jobSummary.append(jip.getNumSlotsPerMap() );
+    jobSummary.append(",");
+    jobSummary.append("numReduces=");
+    jobSummary.append(jip.getTasks(TaskType.REDUCE).length);
+    jobSummary.append(",");
+    jobSummary.append("numSlotsPerReduce=");
+    jobSummary.append(jip.getNumSlotsPerReduce());
+    jobSummary.append(",");
+    jobSummary.append("user=");
+    jobSummary.append(user);
+    jobSummary.append(",");
+    jobSummary.append("queue=");
+    jobSummary.append(queue);
+    jobSummary.append(",");
+    jobSummary.append("status=");
+    jobSummary.append(JobStatus.getJobRunState(status.getRunState()));
+    jobSummary.append(",");
+    jobSummary.append("mapSlotSeconds=");
+    jobSummary.append(mapSlotSeconds);
+    jobSummary.append(",");
+    jobSummary.append("reduceSlotsSeconds=");
+    jobSummary.append(reduceSlotSeconds);
+    jobSummary.append(",");
+    jobSummary.append("clusterMapCapacity=");
+    jobSummary.append(tracker.getClusterMetrics().getMapSlotCapacity());
+    jobSummary.append(",");
+    jobSummary.append("clusterReduceCapacity=");
+    jobSummary.append(tracker.getClusterMetrics().getReduceSlotCapacity());
+    return jobSummary.toString();
+  }
+
+  /**
+   * This gets the value of one task tracker window in the tasktracker page. 
+   *
+   * @param TaskTrackerStatus, 
+   * timePeriod and totalTasksOrSucceededTasks, which are requried to 
+   * identify the window
+   * @return The number of tasks info in a particular window in 
+   * tasktracker page. 
+   */
+  public int JobTracker.getTaskTrackerLevelStatistics( 
+      TaskTrackerStatus ttStatus, String timePeriod,
+      String totalTasksOrSucceededTasks) throws IOException {
+
+    LOG.info("ttStatus host :" + ttStatus.getHost());
+    if (timePeriod.matches("since_start")) {
+      StatisticsCollector.TimeWindow window = getStatistics().
+          collector.DEFAULT_COLLECT_WINDOWS[0];
+      return(getNumberOfTasks(window, ttStatus , 
+          totalTasksOrSucceededTasks));
+    } else if (timePeriod.matches("last_day")) {
+      StatisticsCollector.TimeWindow window = getStatistics().
+          collector.DEFAULT_COLLECT_WINDOWS[1];
+      return(getNumberOfTasks(window, ttStatus, 
+          totalTasksOrSucceededTasks));
+    } else if (timePeriod.matches("last_hour")) {
+      StatisticsCollector.TimeWindow window = getStatistics().
+          collector.DEFAULT_COLLECT_WINDOWS[2];
+      return(getNumberOfTasks(window, ttStatus , 
+          totalTasksOrSucceededTasks));
+    }
+    return -1;
+  }
+
+  /**
+   * Get Information for Time Period and TaskType box
+   * from all tasktrackers
+   *
+   * @param 
+   * timePeriod and totalTasksOrSucceededTasks, which are requried to
+   * identify the window
+   * @return The total number of tasks info for a particular column in
+   * tasktracker page.
+   */
+  public int JobTracker.getInfoFromAllClients(String timePeriod,
+      String totalTasksOrSucceededTasks) throws IOException {
+   
+    int totalTasksCount = 0;
+    int totalTasksRanForJob = 0;
+    for (TaskTracker tt : taskTrackers.values()) {
+      TaskTrackerStatus ttStatus = tt.getStatus();
+      String tasktrackerName = ttStatus.getHost();
+      List<Integer> taskTrackerValues = new LinkedList<Integer>();
+      JobTrackerStatistics.TaskTrackerStat ttStat = getStatistics().
+             getTaskTrackerStat(ttStatus.getTrackerName());
+      int totalTasks = getTaskTrackerLevelStatistics(
+          ttStatus, timePeriod, totalTasksOrSucceededTasks);
+      totalTasksCount += totalTasks;
+    }
+    return totalTasksCount;
+  }
+
+  private int JobTracker.getNumberOfTasks(StatisticsCollector.TimeWindow 
+    window, TaskTrackerStatus ttStatus, String totalTasksOrSucceededTasks ) { 
+    JobTrackerStatistics.TaskTrackerStat ttStat = getStatistics().
+             getTaskTrackerStat(ttStatus.getTrackerName());
+    if (totalTasksOrSucceededTasks.matches("total_tasks")) {
+      return ttStat.totalTasksStat.getValues().
+          get(window).getValue();
+    } else if (totalTasksOrSucceededTasks.matches("succeeded_tasks")) {
+      return ttStat.succeededTasksStat.getValues().
+          get(window).getValue();
+    }
+    return -1;
+  }
+
+  /**
+   * This gets the value of all task trackers windows in the tasktracker page.
+   *
+   * @param none,
+   * @return StatisticsCollectionHandler class which holds the number
+   * of all jobs ran from all tasktrackers, in the sequence given below
+   * "since_start - total_tasks"
+   * "since_start - succeeded_tasks"
+   * "last_hour - total_tasks"
+   * "last_hour - succeeded_tasks"
+   * "last_day - total_tasks"
+   * "last_day - succeeded_tasks"
+   */
+  public StatisticsCollectionHandler JobTracker.
+    getInfoFromAllClientsForAllTaskType() throws Exception { 
+
+    //The outer list will have a list of each tasktracker list.
+    //The inner list will have a list of all number of tasks in 
+    //one tasktracker.
+    List<List<Integer>> ttInfoList = new LinkedList<List<Integer>>();
+
+    // Go through each tasktracker and get all the number of tasks
+    // six window's values of that tasktracker.Each window points to 
+    // specific value for that tasktracker.  
+    //"since_start - total_tasks"
+    //"since_start - succeeded_tasks"
+    //"last_hour - total_tasks"
+    //"last_hour - succeeded_tasks"
+    //"last_day - total_tasks"
+    //"last_day - succeeded_tasks"
+
+    for (TaskTracker tt : taskTrackers.values()) {
+      TaskTrackerStatus ttStatus = tt.getStatus();
+      String tasktrackerName = ttStatus.getHost();
+      List<Integer> taskTrackerValues = new LinkedList<Integer>(); 
+      JobTrackerStatistics.TaskTrackerStat ttStat = getStatistics().
+             getTaskTrackerStat(ttStatus.getTrackerName());
+
+      int value;
+      int totalCount = 0;
+      for (int i = 0; i < 3; i++) { 
+        StatisticsCollector.TimeWindow window = getStatistics().
+          collector.DEFAULT_COLLECT_WINDOWS[i];
+        value=0;
+        value = ttStat.totalTasksStat.getValues().
+          get(window).getValue();
+        taskTrackerValues.add(value);
+        value=0;
+        value  = ttStat.succeededTasksStat.getValues().
+          get(window).getValue(); 
+        taskTrackerValues.add(value);
+      }
+      ttInfoList.add(taskTrackerValues);
+    }
+
+    //The info is collected in the order described above  by going 
+    //through each tasktracker list 
+    int totalInfoValues = 0; 
+    StatisticsCollectionHandler statisticsCollectionHandler = 
+      new StatisticsCollectionHandler();
+    for (int i = 0; i < 6; i++) {
+      totalInfoValues = 0;
+      for (int j = 0; j < ttInfoList.size(); j++) { 
+         List<Integer> list = ttInfoList.get(j);
+         totalInfoValues += list.get(i); 
+      }
+      switch (i) {
+        case 0: statisticsCollectionHandler.
+          setSinceStartTotalTasks(totalInfoValues);
+          break;
+        case 1: statisticsCollectionHandler.
+          setSinceStartSucceededTasks(totalInfoValues);
+          break;
+        case 2: statisticsCollectionHandler.
+          setLastHourTotalTasks(totalInfoValues);
+          break;
+        case 3: statisticsCollectionHandler.
+          setLastHourSucceededTasks(totalInfoValues);
+          break;
+        case 4: statisticsCollectionHandler.
+          setLastDayTotalTasks(totalInfoValues);
+          break;
+        case 5: statisticsCollectionHandler.
+          setLastDaySucceededTasks(totalInfoValues);
+          break;
+      }
+    } 
+      return statisticsCollectionHandler;
+  }
+
+  /*
+   * Get the Tasktrcker Heart beat interval 
+   */
+  public int JobTracker.getTaskTrackerHeartbeatInterval()
+      throws Exception {
+    return (getNextHeartbeatInterval());
+  }
+  
+  //access the job data the method only does a get on read-only data
+  //it does not return anything purposely, since the test case
+  //does not require this but this can be extended in future
+  public void JobTracker.accessHistoryData(JobID id) throws Exception {
+    String location = getJobHistoryLocationForRetiredJob(id);
+    Path logFile = new Path(location);
+    FileSystem fs = logFile.getFileSystem(getConf());
+    JobHistory.JobInfo jobInfo  = new JobHistory.JobInfo(id.toString());
+    DefaultJobHistoryParser.parseJobTasks(location,
+        jobInfo, fs);
+    //Now read the info so two threads can access the info at the
+    //same time from client side
+    LOG.info("user " +jobInfo.get(Keys.USER));
+    LOG.info("jobname "+jobInfo.get(Keys.JOBNAME));
+    jobInfo.get(Keys.JOBCONF);
+    jobInfo.getJobACLs();
+  }
+  
+  /**
+   * Verifies whether Node is decommissioned or not
+   * @param
+   * tasktracker Client host name
+   * @return boolean true for Decommissoned and false for not decommissioned.
+   */
+  public boolean JobTracker.isNodeDecommissioned(String ttClientHostName)
+      throws IOException {
+    Set<String> excludedNodes = hostsReader.getExcludedHosts();
+    LOG.info("ttClientHostName is :" + ttClientHostName);
+    boolean b =  excludedNodes.contains(ttClientHostName);
+    return b;
+  }
 }
diff --git a/src/test/system/aop/org/apache/hadoop/mapred/StatisticsCollectorAspect.aj b/src/test/system/aop/org/apache/hadoop/mapred/StatisticsCollectorAspect.aj
new file mode 100644
index 0000000..e826a25
--- /dev/null
+++ b/src/test/system/aop/org/apache/hadoop/mapred/StatisticsCollectorAspect.aj
@@ -0,0 +1,57 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.mapred.StatisticsCollector.TimeWindow;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+/*
+ * This class will change the number of jobs time windows 
+ * of all task trackers <br/> 
+ * Last Day time window will be changed from 24 hours to 2 minutes <br/> 
+ * Last Hour time window will be changed from 1 hour to 1 minute <br/>
+ */
+
+public privileged aspect StatisticsCollectorAspect {
+
+  //last day is changed to 120 seconds instead of 24 hours, 
+  //with a 10 seconds refresh rate 
+  static final TimeWindow 
+    LAST_DAY_ASPECT = new TimeWindow("Last Day", 2 * 60, 10);
+
+  //last day is changed to 60 seconds instead of 1 hour, 
+  //with a 10 seconds refresh rate 
+  static final TimeWindow 
+    LAST_HOUR_ASPECT = new TimeWindow("Last Hour", 60, 10);
+
+  private static final Log LOG = LogFactory
+      .getLog(StatisticsCollectorAspect.class);
+
+  pointcut createStatExecution(String name, TimeWindow[] window) : 
+     call(* StatisticsCollector.createStat(String, TimeWindow[])) 
+    && args(name, window);
+
+  //This will change the timewindow to have last day and last hour changed.
+  before(String name, TimeWindow[] window) : createStatExecution(name, window) {
+      window[1] = LAST_DAY_ASPECT;
+      window[2] = LAST_HOUR_ASPECT;
+  }
+
+}
diff --git a/src/test/system/aop/org/apache/hadoop/test/system/DaemonProtocolAspect.aj b/src/test/system/aop/org/apache/hadoop/test/system/DaemonProtocolAspect.aj
index 1f3567e..b4b392d 100644
--- a/src/test/system/aop/org/apache/hadoop/test/system/DaemonProtocolAspect.aj
+++ b/src/test/system/aop/org/apache/hadoop/test/system/DaemonProtocolAspect.aj
@@ -31,8 +31,11 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.Shell.ShellCommandExecutor;
+import org.apache.hadoop.util.Shell;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.conf.Configuration;
 
@@ -52,6 +55,10 @@ public aspect DaemonProtocolAspect {
     new HashMap<Object, List<ControlAction>>();
   private static final Log LOG = LogFactory.getLog(
       DaemonProtocolAspect.class.getName());
+
+  private static FsPermission defaultPermission = new FsPermission(
+     FsAction.READ_WRITE, FsAction.READ_WRITE, FsAction.READ_WRITE);
+
   /**
    * Set if the daemon process is ready or not, concrete daemon protocol should
    * implement pointcuts to determine when the daemon is ready and use the
@@ -126,6 +133,50 @@ public aspect DaemonProtocolAspect {
     return cloneFileStatus(fileStatus);
   }
 
+  /**
+   * Create a file with given permissions in a file system.
+   * @param path - source path where the file has to create.
+   * @param fileName - file name.
+   * @param permission - file permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void DaemonProtocol.createFile(String path, String fileName, 
+     FsPermission permission, boolean local) throws IOException {
+    Path p = new Path(path); 
+    FileSystem fs = getFS(p, local);
+    Path filePath = new Path(path, fileName);
+    fs.create(filePath);
+    if (permission == null) {
+      fs.setPermission(filePath, defaultPermission);
+    } else {
+      fs.setPermission(filePath, permission);
+    }
+    fs.close();
+  }
+
+  /**
+   * Create a folder with given permissions in a file system.
+   * @param path - source path where the file has to be creating.
+   * @param folderName - folder name.
+   * @param permission - folder permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void DaemonProtocol.createFolder(String path, String folderName, 
+     FsPermission permission, boolean local) throws IOException {
+    Path p = new Path(path);
+    FileSystem fs = getFS(p, local);
+    Path folderPath = new Path(path, folderName);
+    fs.mkdirs(folderPath);
+    if (permission ==  null) {
+      fs.setPermission(folderPath, defaultPermission);
+    } else {
+      fs.setPermission(folderPath, permission);
+    }
+    fs.close();
+  }
+
   public FileStatus[] DaemonProtocol.listStatus(String path, boolean local) 
     throws IOException {
     Path p = new Path(path);
@@ -255,24 +306,86 @@ public aspect DaemonProtocolAspect {
   public int DaemonProtocol.getNumberOfMatchesInLogFile(String pattern,
       String[] list) throws IOException {
     StringBuffer filePattern = new StringBuffer(getFilePattern());    
-    if(list != null){
-      for(int i =0; i < list.length; ++i)
-      {
-        filePattern.append(" | grep -v " + list[i] );
+    String[] cmd = null;
+    if (list != null) {
+      StringBuffer filterExpPattern = new StringBuffer();
+      int index=0;
+      for (String excludeExp : list) {
+        if (index++ < list.length -1) {
+           filterExpPattern.append("grep -v " + excludeExp + " | ");
+        } else {
+           filterExpPattern.append("grep -v " + excludeExp + " | wc -l");
+        }
       }
-    }  
-    String[] cmd =
-        new String[] {
-            "bash",
-            "-c",
-            "grep -c "
+      cmd = new String[] {
+                "bash",
+                "-c",
+                "grep "
+                + pattern + " " + filePattern + " | "
+                + filterExpPattern};
+    } else {
+      cmd = new String[] {
+                "bash",
+                "-c",
+                "grep -c "
                 + pattern + " " + filePattern
                 + " | awk -F: '{s+=$2} END {print s}'" };    
+    }
     ShellCommandExecutor shexec = new ShellCommandExecutor(cmd);
     shexec.execute();
     String output = shexec.getOutput();
     return Integer.parseInt(output.replaceAll("\n", "").trim());
   }
+  
+  /**
+   * This method is used for suspending the process.
+   * @param pid process id
+   * @throws IOException if an I/O error occurs.
+   * @return true if process is suspended otherwise false.
+   */
+  public boolean DaemonProtocol.suspendProcess(String pid) throws IOException {
+    String suspendCmd = getDaemonConf().get("test.system.hdrc.suspend.cmd",
+        "kill -SIGSTOP");
+    String [] command = {"bash", "-c", suspendCmd + " " + pid};
+    ShellCommandExecutor shexec = new ShellCommandExecutor(command);
+    try {
+      shexec.execute();
+    } catch (Shell.ExitCodeException e) {
+      LOG.warn("suspended process throws an exitcode "
+          + "exception for not being suspended the given process id.");
+      return false;
+    }
+    LOG.info("The suspend process command is :"
+        + shexec.toString()
+        + " and the output for the command is "
+        + shexec.getOutput());
+    return true;
+  }
+
+  /**
+   * This method is used for resuming the process
+   * @param pid process id of suspended process.
+   * @throws IOException if an I/O error occurs.
+   * @return true if suspeneded process is resumed otherwise false.
+   */
+  public boolean DaemonProtocol.resumeProcess(String pid) throws IOException {
+    String resumeCmd = getDaemonConf().get("test.system.hdrc.resume.cmd",
+        "kill -SIGCONT");
+    String [] command = {"bash", "-c", resumeCmd + " " + pid};
+    ShellCommandExecutor shexec = new ShellCommandExecutor(command);
+    try {
+      shexec.execute();
+    } catch(Shell.ExitCodeException e) {
+        LOG.warn("Resume process throws an exitcode "
+          + "exception for not being resumed the given process id.");
+      return false;
+    }
+    LOG.info("The resume process command is :"
+        + shexec.toString()
+        + " and the output for the command is "
+        + shexec.getOutput());
+    return true;
+  }
 
   private String DaemonProtocol.user = null;
   
@@ -284,4 +397,3 @@ public aspect DaemonProtocolAspect {
     this.user = user;
   }
 }
-
diff --git a/src/test/system/conf/system-test.xml b/src/test/system/conf/system-test.xml
index cc7b8cb..48f57fc 100644
--- a/src/test/system/conf/system-test.xml
+++ b/src/test/system/conf/system-test.xml
@@ -54,7 +54,7 @@ neutral at the forward-port stage -->
 </property>
 <property>
    <name>test.system.hdrc.deployed.scripts.dir</name>
-   <value>./src/test/system/scripts</value>
+   <value>$(YINST_ROOT)/share/hadoop-current/src/test/system/scripts</value>
    <description>
      This directory hosts the scripts in the deployed location where
      the system test client runs.
@@ -68,6 +68,37 @@ neutral at the forward-port stage -->
   the clusters is pointed out this directory. 
   </description>
 </property>
+<property>
+  <name>test.system.hdrc.hadoop.local.confdir</name>
+  <value>$(TO_DO_GLOBAL_TMP_DIR)/localconf</value>
+  <description>
+  It's a local conf directory where the new config file is store temporarily
+  before pushing into new config folder location in the cluster.
+  </description>
+</property>
+<property>
+  <name>test.system.hdrc.suspend.cmd</name>
+  <value>kill -SIGSTOP</value>
+  <description>
+    Command for suspending the given process.
+  </description>
+</property>
+<property>
+  <name>test.system.hdrc.resume.cmd</name>
+  <value>kill -SIGCONT</value>
+  <description>
+  Command for resuming the given suspended process.
+  </description>
+</property>
+<property>
+  <name>test.system.hdrc.healthscript.path</name>
+  <value>/tmp</value>
+  <description>
+  This location is used by health script test cases to configure the
+  health script in remote cluster where the error inducing health script
+  will be copied.
+  </description>
+</property>
 
 <!-- Mandatory keys to be set for the multi user support to be enabled.  -->
 
@@ -86,6 +117,13 @@ neutral at the forward-port stage -->
   </description>
 </property>
 <property>
+  <name>test.system.hdrc.multi-user.list.path</name>
+  <value>$(YINST_ROOT)/conf/hadoop/proxyusers</value>
+  <description>
+  Multi user list for creating the proxy users.
+  </description>
+</property>
+<property>
   <name>test.system.hdrc.multi-user.binary.path</name>
   <value>$(YINST_ROOT)/conf/hadoop/runAs</value>
   <description>
diff --git a/src/test/system/java/org/apache/hadoop/mapred/HealthScriptHelper.java b/src/test/system/java/org/apache/hadoop/mapred/HealthScriptHelper.java
new file mode 100644
index 0000000..e827957
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/HealthScriptHelper.java
@@ -0,0 +1,166 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import org.apache.hadoop.mapred.UtilsForTests;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.apache.hadoop.util.Shell.ShellCommandExecutor;
+import org.apache.hadoop.mapred.TaskTrackerStatus;
+import org.apache.hadoop.test.system.process.HadoopDaemonRemoteCluster;
+import org.junit.AfterClass;
+import org.junit.Assert;
+
+
+/** 
+ * This is helper class that is used by the health script test cases
+ *
+ */
+public class HealthScriptHelper  {
+
+  static final Log LOG = LogFactory.getLog(HealthScriptHelper.class);
+  
+  /**
+   * Will verify that given task tracker is not blacklisted
+   * @param client tasktracker info
+   * @param conf modified configuration object
+   * @param cluster mrcluster instance
+   * @throws IOException thrown if verification fails
+   */
+  public void verifyTTNotBlackListed(TTClient client, Configuration conf,
+      MRCluster cluster) throws IOException {        
+    int interval = conf.getInt("mapred.healthChecker.interval",0);
+    Assert.assertTrue("Interval cannot be zero.",interval != 0);
+    UtilsForTests.waitFor(interval+2000);
+    String defaultHealthScript = conf.get("mapred.healthChecker.script.path");    
+    Assert.assertTrue("Task tracker is not healthy",
+        nodeHealthStatus(client, true) == true);
+    TaskTrackerStatus status = client.getStatus();
+    JTClient jclient = cluster.getJTClient();
+    Assert.assertTrue("Failed to move task tracker to healthy list",
+        jclient.getProxy().isBlackListed(status.getTrackerName()) == false);        
+    Assert.assertTrue("Health script was not set",defaultHealthScript != null);
+    
+  }
+  
+  /**
+   * Verifies that the given task tracker is blacklisted
+   * @param conf modified Configuration object
+   * @param client tasktracker info
+   * @param errorMessage that needs to be asserted
+   * @param cluster mr cluster instance
+   * @throws IOException is thrown when verification fails
+   */
+  public void verifyTTBlackList(Configuration conf, TTClient client,
+      String errorMessage, MRCluster cluster) throws IOException{   
+    int interval = conf.getInt("mapred.healthChecker.interval",0);
+    Assert.assertTrue("Interval cannot be zero.",interval != 0);
+    UtilsForTests.waitFor(interval+2000);
+    //TaskTrackerStatus status = client.getStatus();
+    Assert.assertTrue("Task tracker was never blacklisted ",
+        nodeHealthStatus(client, false) == true);
+    TaskTrackerStatus status = client.getStatus();
+    Assert.assertTrue("The custom error message did not appear",
+        status.getHealthStatus().getHealthReport().trim().
+        equals(errorMessage));
+    JTClient jClient = cluster.getJTClient();    
+    Assert.assertTrue("Failed to move task tracker to blacklisted list",
+        jClient.getProxy().isBlackListed(status.getTrackerName()) == true);    
+  }
+  
+  /**
+   * The method return true from the task tracker if it is unhealthy/healthy
+   * depending the blacklisted status
+   * @param client the tracker tracker instance
+   * @param health status information. 
+   * @return status of task tracker
+   * @throws IOException failed to get the status of task tracker
+   */
+  public boolean nodeHealthStatus(TTClient client,boolean hStatus) throws IOException {
+    int counter = 0;
+    TaskTrackerStatus status = client.getStatus();
+    while (counter < 60) {
+      LOG.info("isNodeHealthy "+status.getHealthStatus().isNodeHealthy());
+      if (status.getHealthStatus().isNodeHealthy() == hStatus) {
+        break;
+      } else {
+        UtilsForTests.waitFor(3000);
+        status = client.getStatus();
+        Assert.assertNotNull("Task tracker status is null",status);
+      }
+      counter++;
+    }
+    if(counter != 60) {
+      return true;
+    }
+    return false;
+  }
+  
+  /**
+   * This will copy the error inducing health script from local node running
+   * the system tests to the node where the task tracker runs
+   * @param scriptName name of the scirpt to be copied
+   * @param hostname identifies the task tracker
+   * @param remoteLocation location in remote task tracker node
+   * @param cluster mrcluster instance
+   * @throws IOException thrown if copy file fails. 
+   */
+  public void copyFileToRemoteHost(String scriptName, String hostname,
+      String remoteLocation,MRCluster cluster) throws IOException {        
+    ArrayList<String> cmdArgs = new ArrayList<String>();
+    String scriptDir = cluster.getConf().get(
+        HadoopDaemonRemoteCluster.CONF_SCRIPTDIR);
+    StringBuffer localFile = new StringBuffer();    
+    localFile.append(scriptDir).append(File.separator).append(scriptName);
+    cmdArgs.add("scp");
+    cmdArgs.add(localFile.toString());
+    StringBuffer remoteFile = new StringBuffer();
+    remoteFile.append(hostname).append(":");
+    remoteFile.append(remoteLocation).append(File.separator).append(scriptName);
+    cmdArgs.add(remoteFile.toString());
+    executeCommand(cmdArgs);
+  }
+  
+  private void executeCommand(ArrayList<String> cmdArgs) throws IOException{
+    String[] cmd = (String[]) cmdArgs.toArray(new String[cmdArgs.size()]);
+    ShellCommandExecutor executor = new ShellCommandExecutor(cmd);
+    LOG.info(executor.toString());
+    executor.execute();
+    String output = executor.getOutput();    
+    if (!output.isEmpty()) { //getOutput() never returns null value
+      if (output.toLowerCase().contains("error")) {
+        LOG.warn("Error is detected.");
+        throw new IOException("Start error\n" + output);
+      }
+    }
+  }
+  
+  /**
+   * cleans up the error inducing health script in the remote node
+   * @param path the script that needs to be deleted
+   * @param hostname where the script resides. 
+   */
+  public void deleteFileOnRemoteHost(String path, String hostname) {
+    try {
+      ArrayList<String> cmdArgs = new ArrayList<String>();
+      cmdArgs.add("ssh");
+      cmdArgs.add(hostname);
+      cmdArgs.add("if [ -f "+ path+
+      "  ];\n then echo Will remove existing file "+path+";  rm -f "+
+      path+";\n  fi");
+      executeCommand(cmdArgs);
+    }
+    catch (IOException io) {
+      LOG.error("Failed to remove the script "+path+" on remote host "+hostname);
+    }
+  }
+
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/HighRamJobHelper.java b/src/test/system/java/org/apache/hadoop/mapred/HighRamJobHelper.java
new file mode 100644
index 0000000..4b3726a
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/HighRamJobHelper.java
@@ -0,0 +1,56 @@
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.junit.Assert;
+
+/**
+ * This is a helper class that is used by test cases to run a high ram job
+ * the intention behind creatint this class is reuse code. 
+ *
+ */
+public class HighRamJobHelper {
+
+  public  HighRamJobHelper () {
+    
+  }
+  
+  /**
+   * The method runs the high ram job
+   * @param conf configuration for unning the job
+   * @param jobClient instance
+   * @param remoteJTClient instance
+   * @return the job id of the high ram job
+   * @throws Exception is thrown when the method fails to run the high ram job
+   */
+  public JobID runHighRamJob (Configuration conf, JobClient jobClient, 
+      JTProtocol remoteJTClient,String assertMessage) throws Exception {
+    SleepJob job = new SleepJob();
+    String jobArgs []= {"-D","mapred.cluster.max.map.memory.mb=2048", 
+                        "-D","mapred.cluster.max.reduce.memory.mb=2048", 
+                        "-D","mapred.cluster.map.memory.mb=1024", 
+                        "-D","mapreduce.job.complete.cancel.delegation.tokens=false",
+                        "-D","mapred.cluster.reduce.memory.mb=1024",
+                        "-m", "6", 
+                        "-r", "2", 
+                        "-mt", "2000", 
+                        "-rt", "2000",
+                        "-recordt","100"};
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setMemoryForMapTask(2048);
+    jobConf.setMemoryForReduceTask(2048);
+    int exitCode = ToolRunner.run(jobConf, job, jobArgs);
+    Assert.assertEquals("Exit Code:", 0, exitCode);
+    UtilsForTests.waitFor(1000); 
+    JobID jobId = jobClient.getAllJobs()[0].getJobID();
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertEquals(assertMessage, 
+        jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+    return jobId;
+  }
+  
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/JobInfoImpl.java b/src/test/system/java/org/apache/hadoop/mapred/JobInfoImpl.java
index 28b2e72..846002d 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/JobInfoImpl.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/JobInfoImpl.java
@@ -24,7 +24,6 @@ import java.io.IOException;
 import java.util.LinkedList;
 import java.util.List;
 
-import org.apache.hadoop.mapred.JobStatus;
 import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.test.system.JobInfo;
 
@@ -50,7 +49,10 @@ class JobInfoImpl implements JobInfo {
   private int finishedReduces;
   private int numMaps;
   private int numReduces;
-  private boolean historyCopied;
+  private long finishTime;
+  private long launchTime;
+  private int numOfSlotsPerMap;
+  private int numOfSlotsPerReduce;
 
   public JobInfoImpl() {
     id = new JobID();
@@ -65,7 +67,7 @@ class JobInfoImpl implements JobInfo {
       int waitingMaps, int waitingReduces, int finishedMaps,
       int finishedReduces, JobStatus status, String historyUrl,
       List<String> blackListedTracker, boolean isComplete, int numMaps,
-      int numReduces, boolean historyCopied) {
+      int numReduces) {
     super();
     this.blackListedTracker = blackListedTracker;
     this.historyUrl = historyUrl;
@@ -82,7 +84,6 @@ class JobInfoImpl implements JobInfo {
     this.finishedReduces = finishedReduces;
     this.numMaps = numMaps;
     this.numReduces = numReduces;
-    this.historyCopied = historyCopied;
   }
 
   @Override
@@ -159,10 +160,41 @@ class JobInfoImpl implements JobInfo {
   public int numReduces() {
     return numReduces;
   }
+
+  public void setFinishTime(long finishTime) {
+    this.finishTime = finishTime;
+  }
   
+  public void setLaunchTime(long launchTime) {
+    this.launchTime = launchTime;
+  }
+
+  @Override
+  public long getFinishTime() {
+    return finishTime;
+  }
+
+  @Override
+  public long getLaunchTime() {
+    return launchTime;
+  }
+
+  public void setNumSlotsPerMap(int numOfSlotsPerMap) {
+    this.numOfSlotsPerMap = numOfSlotsPerMap;
+  } 
+
+  public void setNumSlotsPerReduce(int numOfSlotsPerReduce) {
+    this.numOfSlotsPerReduce = numOfSlotsPerReduce;
+  }
+
+  @Override
+  public int getNumSlotsPerMap() {
+    return numOfSlotsPerMap;
+  }
+
   @Override
-  public boolean isHistoryFileCopied() {
-    return historyCopied;
+  public int getNumSlotsPerReduce() {
+    return numOfSlotsPerReduce;
   }
   
   @Override
@@ -185,7 +217,10 @@ class JobInfoImpl implements JobInfo {
     finishedReduces = in.readInt();
     numMaps = in.readInt();
     numReduces = in.readInt();
-    historyCopied = in.readBoolean();
+    finishTime = in.readLong();
+    launchTime = in.readLong();
+    numOfSlotsPerMap = in.readInt();
+    numOfSlotsPerReduce = in.readInt();
   }
 
   @Override
@@ -208,7 +243,10 @@ class JobInfoImpl implements JobInfo {
     out.writeInt(finishedReduces);
     out.writeInt(numMaps);
     out.writeInt(numReduces);
-    out.writeBoolean(historyCopied);
+    out.writeLong(finishTime);
+    out.writeLong(launchTime);
+    out.writeInt(numOfSlotsPerMap);
+    out.writeInt(numOfSlotsPerReduce);
   }
 
 
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestCMExceptionDuringRunJob.java b/src/test/system/java/org/apache/hadoop/mapred/TestCMExceptionDuringRunJob.java
new file mode 100644
index 0000000..da7fc62
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestCMExceptionDuringRunJob.java
@@ -0,0 +1,122 @@
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import java.util.Hashtable;
+import java.lang.Integer;
+
+public class TestCMExceptionDuringRunJob {
+  
+  private static MRCluster cluster = null;
+  static final Log LOG = LogFactory.getLog(TestCMExceptionDuringRunJob.class);
+  private JTProtocol remoteJTClient=null;
+  private Configuration conf =null;
+  @BeforeClass
+  public static void setUp() throws java.lang.Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+  }
+  
+  /**
+   * The objective of the test is, when the user accesses the retired job data
+   * or the running job data simultaneously in different threads there
+   * is no concurrent modification exceptions gets thrown
+   */
+  @Test
+  public void testNoCMExcepRunningJob() throws Exception {
+    
+    remoteJTClient = cluster.getJTClient().getProxy();
+    
+    conf = remoteJTClient.getDaemonConf();
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    //set interval to 30 secs, check to 10 secs, check determines the frequency
+    //of the jobtracker thread that will check for retired jobs, and interval
+    //will determine how long it will take before a job retires.
+    //conf.setInt("mapred.jobtracker.retirejob.interval",30*1000);
+    //conf.setInt("mapred.jobtracker.retirejob.check",10*1000);
+    //cluster.restartDaemonWithNewConfig(cluster.getJTClient(), "mapred-site.xml",
+    //    conf, Role.JT);
+    Hashtable<String,Long> props = new Hashtable<String,Long>();
+    props.put("mapred.jobtracker.retirejob.interval",30000L);
+    props.put("mapred.jobtracker.retirejob.check",10000L);
+    cluster.restartClusterWithNewConfig(props,"mapred-site.xml");
+    JobID jobid1 = runSleepJob(true);
+    JobID jobid2 = runSleepJob(true);
+    JobID jobid3 = runSleepJob(false);
+    //Waiting for a minute for the job to retire
+    UtilsForTests.waitFor(60*1000);
+    RunAccessHistoryData access1 = new RunAccessHistoryData(jobid1);
+    RunAccessHistoryData access2 = new RunAccessHistoryData(jobid2);
+    new Thread(access1).start();
+    new Thread(access2).start();
+    remoteJTClient.getJobSummaryInfo(jobid3);
+    cluster.signalAllTasks(jobid3);
+    cluster.getJTClient().isJobStopped(jobid3);
+    //cluster.restart(cluster.getJTClient(), Role.JT);
+    cluster.restart();
+  }
+  
+  
+  public class RunAccessHistoryData implements Runnable {
+    private  JobID jobId = null;
+    
+    public RunAccessHistoryData (JobID jobId) {
+      this.jobId =jobId; 
+    }
+    
+     public void run () {
+       try {
+         remoteJTClient.accessHistoryData(jobId);
+       }
+       catch (Exception ex) {
+         ex.printStackTrace();
+       }
+     }
+  }
+
+  
+  public JobID runSleepJob(boolean signalJob) throws Exception{
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    conf = job.setupJobConf(5, 1, 100, 5, 100, 5);
+    JobConf jconf = new JobConf(conf);
+    //Controls the job till all verification is done 
+    FinishTaskControlAction.configureControlActionForJob(conf);
+    //Submitting the job
+    RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+    JobID jobId = rJob.getID();
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    LOG.info("jInfo is :" + jInfo);
+    boolean jobStarted = cluster.getJTClient().isJobStarted(jobId);
+    Assert.assertTrue("Job has not started even after a minute", 
+        jobStarted );
+      
+    if(signalJob) {
+      cluster.signalAllTasks(jobId);
+      Assert.assertTrue("Job has not stopped yet",
+          cluster.getJTClient().isJobStopped(jobId));
+    }
+    return jobId;
+  }
+  
+  @AfterClass
+  public static void tearDown() throws Exception {    
+    cluster.tearDown();
+  }
+
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestCacheFileReferenceCount.java b/src/test/system/java/org/apache/hadoop/mapred/TestCacheFileReferenceCount.java
new file mode 100644
index 0000000..a5b82fa
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestCacheFileReferenceCount.java
@@ -0,0 +1,283 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import java.net.URI;
+import java.io.DataOutputStream;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Assert;
+import org.junit.Test;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Verify the job cache files localizations.
+ */
+public class TestCacheFileReferenceCount {
+  private static final Log LOG = LogFactory
+          .getLog(TestCacheFileReferenceCount.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static Path tmpFolderPath = null;
+  private static String cacheFile1 = "cache1.txt";
+  private static String cacheFile2 = "cache2.txt";
+  private static String cacheFile3 = "cache3.txt";
+  private static String cacheFile4 = "cache4.txt";
+  private static JTProtocol wovenClient = null;
+  private static JobClient jobClient = null;
+  private static JTClient jtClient = null;
+  private static URI cacheFileURI1;
+  private static URI cacheFileURI2;
+  private static URI cacheFileURI3;
+  private static URI cacheFileURI4;
+  
+  @BeforeClass
+  public static void before() throws Exception {
+    cluster = MRCluster.createCluster(conf);
+    cluster.setUp();
+    tmpFolderPath = new Path("hdfs:///tmp");
+    jtClient = cluster.getJTClient();
+    jobClient = jtClient.getClient();
+    wovenClient = cluster.getJTClient().getProxy();
+    cacheFileURI1 = createCacheFile(tmpFolderPath, cacheFile1);
+    cacheFileURI2 = createCacheFile(tmpFolderPath, cacheFile2);
+  }
+  
+  @AfterClass
+  public static void after() throws Exception {
+    deleteCacheFile(new Path(tmpFolderPath, cacheFile1));
+    deleteCacheFile(new Path(tmpFolderPath, cacheFile2));
+    deleteCacheFile(new Path(tmpFolderPath, cacheFile4));
+    cluster.tearDown();
+  }
+  
+  /**
+   * Run the job with two distributed cache files and verify
+   * whether job is succeeded or not.
+   * @throws Exception
+   */
+  @Test
+  public void testCacheFilesLocalization() throws Exception {
+    conf = wovenClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    JobConf jobConf = job.setupJobConf(4, 1, 4000, 4000, 1000, 1000);
+    DistributedCache.createSymlink(jobConf);
+    DistributedCache.addCacheFile(cacheFileURI1, jobConf);
+    DistributedCache.addCacheFile(cacheFileURI2, jobConf);
+    RunningJob runJob = jobClient.submitJob(jobConf);
+    JobID jobId = runJob.getID();
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(jobId);
+    Assert.assertTrue("Cache File1 has not been localize",
+        checkLocalization(taskInfos,cacheFile1));
+    Assert.assertTrue("Cache File2 has not been localize",
+            checkLocalization(taskInfos,cacheFile2));
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+    Assert.assertEquals("Job has not been succeeded", 
+        jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+  }
+  
+  /**
+   * Run the job with distributed cache files and remove one cache
+   * file from the DFS when it is localized.verify whether the job
+   * is failed or not.
+   * @throws Exception
+   */
+  @Test
+  public void testDeleteCacheFileInDFSAfterLocalized() throws Exception {
+    conf = wovenClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    JobConf jobConf = job.setupJobConf(4, 1, 4000, 4000, 1000, 1000);
+    cacheFileURI3 = createCacheFile(tmpFolderPath, cacheFile3);
+    DistributedCache.createSymlink(jobConf);
+    DistributedCache.addCacheFile(cacheFileURI3, jobConf);
+    RunningJob runJob = jobClient.submitJob(jobConf);
+    JobID jobId = runJob.getID();
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(jobId);
+    boolean iscacheFileLocalized = checkLocalization(taskInfos,cacheFile3);
+    Assert.assertTrue("CacheFile has not been localized", 
+        iscacheFileLocalized);
+    deleteCacheFile(new Path(tmpFolderPath, cacheFile3));
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = wovenClient.getJobInfo(jobId);
+    }
+    Assert.assertEquals("Job has not been failed", 
+        jInfo.getStatus().getRunState(), JobStatus.FAILED);
+  }
+  
+  /**
+   * Run the job with two distribute cache files and the size of
+   * one file should be larger than local.cache.size.Verify 
+   * whether job is succeeded or not.
+   * @throws Exception
+   */
+  @Test
+  public void testCacheSizeExceeds() throws Exception {
+    conf = wovenClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    String jobArgs []= {"-D","local.cache.size=1024", 
+                        "-m", "4", 
+                        "-r", "2", 
+                        "-mt", "2000", 
+                        "-rt", "2000",
+                        "-recordt","100"};
+    JobConf jobConf = new JobConf(conf);
+    cacheFileURI4 = createCacheFile(tmpFolderPath, cacheFile4);
+    DistributedCache.createSymlink(jobConf);
+    DistributedCache.addCacheFile(cacheFileURI4, jobConf);
+    int countBeforeJS = jtClient.getClient().getAllJobs().length;
+    JobID prvJobId = jtClient.getClient().getAllJobs()[0].getJobID();
+    int exitCode = ToolRunner.run(jobConf,job,jobArgs);
+    Assert.assertEquals("Exit Code:", 0, exitCode);
+    int countAfterJS = jtClient.getClient().getAllJobs().length;
+    int counter = 0;
+    while (counter++ < 30 ) {
+      if (countBeforeJS == countAfterJS) {
+        UtilsForTests.waitFor(1000);
+        countAfterJS = jtClient.getClient().getAllJobs().length;
+      } else {
+        break;
+      } 
+    }
+    JobID jobId = jtClient.getClient().getAllJobs()[0].getJobID();
+    counter = 0;
+    while (counter++ < 30) {
+      if (jobId.toString().equals(prvJobId.toString())) {
+        UtilsForTests.waitFor(1000); 
+        jobId = jtClient.getClient().getAllJobs()[0].getJobID();
+      } else { 
+        break;
+      }
+    }
+    JobInfo jInfo = wovenClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been succeeded", 
+          jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+  }
+  
+  private boolean checkLocalization(TaskInfo[] taskInfos, String cacheFile) 
+      throws Exception {
+    boolean iscacheFileLocalized = false;
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        String[] taskTrackers = taskinfo.getTaskTrackers();
+        List<TTClient> ttList = getTTClients(taskTrackers);
+        for (TTClient ttClient : ttList) {
+          iscacheFileLocalized = checkCacheFile(ttClient,cacheFile);
+          if(iscacheFileLocalized) {
+            return true;
+          }
+        } 
+      }
+    }
+    return false;
+  }
+  
+  private List<TTClient> getTTClients(String[] taskTrackers) 
+      throws Exception {
+    List<TTClient> ttClientList= new ArrayList<TTClient>();
+    for (String taskTracker: taskTrackers) {
+      taskTracker = UtilsForTests.getFQDNofTT(taskTracker);
+      TTClient ttClient = cluster.getTTClient(taskTracker);
+      if (ttClient != null) {
+        ttClientList.add(ttClient);
+      }
+    }
+    return ttClientList;
+  }
+  
+  private boolean checkCacheFile(TTClient ttClient, String cacheFile) 
+      throws IOException {
+    String[] localDirs = ttClient.getMapredLocalDirs();
+    for (String localDir : localDirs) {
+      localDir = localDir + Path.SEPARATOR + 
+          TaskTracker.getPublicDistributedCacheDir();
+      FileStatus[] fileStatuses = ttClient.listStatus(localDir, 
+          true, true);
+      for (FileStatus  fileStatus : fileStatuses) {
+        Path path = fileStatus.getPath();
+        if ((path.toString()).endsWith(cacheFile)) {
+          return true;
+        }
+      }
+    }
+    return false;
+  }
+  
+  private static void deleteCacheFile(Path cacheFilePath) 
+      throws IOException {
+    FileSystem dfs = jobClient.getFs();
+    dfs.delete(cacheFilePath, true);
+  }
+
+  private static URI createCacheFile(Path tmpFolderPath, String cacheFile) 
+      throws IOException {
+    String input = "distribute cache content...";
+    FileSystem dfs = jobClient.getFs();
+    conf = wovenClient.getDaemonConf();
+    FileSystem fs = tmpFolderPath.getFileSystem(conf);
+    if (!fs.mkdirs(tmpFolderPath)) {
+        throw new IOException("Failed to create the temp directory:" 
+            + tmpFolderPath.toString());
+      }
+    deleteCacheFile(new Path(tmpFolderPath, cacheFile));
+    DataOutputStream file = fs.create(new Path(tmpFolderPath, cacheFile));
+    int i = 0;
+    while(i++ < 100) {
+      file.writeBytes(input);
+    }
+    file.close();
+    dfs.setPermission(new Path(tmpFolderPath, cacheFile), new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    URI uri = URI.create(new Path(tmpFolderPath, cacheFile).toString());
+    return uri;
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfMemoryExceedsTask.java b/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfMemoryExceedsTask.java
new file mode 100644
index 0000000..08b4357
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfMemoryExceedsTask.java
@@ -0,0 +1,339 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTTaskInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Assert;
+import org.junit.Test;
+import java.io.IOException;
+import java.io.DataOutputStream;
+import java.util.Collection;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import testjar.GenerateTaskChildProcess;
+import java.util.Hashtable;
+
+/**
+ * Submit a job which would spawn child processes and 
+ * verify whether the task child processes are cleaned up 
+ * or not after either job killed or task killed or task failed.
+ */
+public class TestChildsKillingOfMemoryExceedsTask {
+  private static final Log LOG = LogFactory
+      .getLog(TestChildsKillingOfMemoryExceedsTask.class);
+  private static MRCluster cluster;
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static Configuration conf = new Configuration();
+  private static String confFile = "mapred-site.xml";
+
+  @BeforeClass
+  public static void before() throws Exception {
+    Hashtable<String,Object> prop = new Hashtable<String,Object>();
+    prop.put("mapred.cluster.max.map.memory.mb", 2 * 1024L);
+    prop.put("mapred.cluster.map.memory.mb", 1024L);
+    prop.put("mapred.cluster.max.reduce.memory.mb", 2 * 1024L);
+    prop.put("mapred.cluster.reduce.memory.mb", 1024L);
+    prop.put("mapred.map.max.attempts", 1L);
+    prop.put("mapreduce.job.complete.cancel.delegation.tokens", false);
+
+    String [] expExcludeList = {"java.net.ConnectException",
+    "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+    conf =  cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+    cluster.restart();
+  }
+
+  /**
+   * Verifying the process tree clean up of a task after fails 
+   * due to memory limit and also job is killed while in progress.
+   */
+  @Test
+  public void testProcessTreeCleanupAfterJobKilled() throws IOException {
+    TaskInfo taskInfo = null;
+    long PER_TASK_LIMIT = 500L;
+    Matcher mat = null;
+    TTTaskInfo[] ttTaskinfo = null;
+    String pid = null;
+    TTClient ttClientIns = null; 
+    TTProtocol ttIns = null;
+    TaskID tID = null;
+    int counter = 0;
+
+    String taskOverLimitPatternString = 
+        "TaskTree \\[pid=[0-9]*,tipID=.*\\] is "
+        + "running beyond memory-limits. "
+        + "Current usage : [0-9]*bytes. Limit : %sbytes. Killing task.";
+    
+    Pattern taskOverLimitPattern = Pattern.compile(String.format(
+        taskOverLimitPatternString, 
+            String.valueOf(PER_TASK_LIMIT * 1024 * 1024L)));
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("String Appending");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrAppendMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+    jobConf.setMemoryForMapTask(PER_TASK_LIMIT);
+    jobConf.setMemoryForReduceTask(PER_TASK_LIMIT);
+    
+    JTClient jtClient = cluster.getJTClient(); 
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null",jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.",
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+    Assert.assertTrue("Map process is not alive before task fails.", 
+        ttIns.isProcessTreeAlive(pid));
+    
+    while (ttIns.getTask(tID).getTaskStatus().getRunState() 
+        == TaskStatus.State.RUNNING) {
+      UtilsForTests.waitFor(1000);
+      ttIns = ttClientIns.getProxy();
+    }
+
+    String[] taskDiagnostics = runJob.getTaskDiagnostics(tAttID);
+    Assert.assertNotNull("Task diagnostics is null", taskDiagnostics);
+
+    for (String strVal : taskDiagnostics) {
+      mat = taskOverLimitPattern.matcher(strVal);
+      Assert.assertTrue("Taskover limit error message is not matched.", 
+          mat.find());
+    }
+
+    runJob.killJob();
+
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(id);
+      counter ++;
+    }
+    Assert.assertTrue("Job has not been completed...", counter != 60);
+    UtilsForTests.waitFor(1000);
+    ttIns = ttClientIns.getProxy();
+    ttIns.sendAction(action);
+    UtilsForTests.waitFor(1000);
+    Assert.assertTrue("Map process is still alive after task has been failed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  /**
+   * Verifying the process tree clean up of a task after it fails
+   * due to exceeding memory limit of mapper.
+   */
+  @Test
+  public void testProcessTreeCleanupOfFailedTask() throws IOException {
+    TaskInfo taskInfo = null;
+    long PER_TASK_LIMIT = 500L;
+    Matcher mat = null;
+    TTTaskInfo[] ttTaskinfo = null;
+    String pid = null;
+    TTClient ttClientIns = null; 
+    TTProtocol ttIns = null;
+    TaskID tID = null;
+    int counter = 0;
+    
+    String taskOverLimitPatternString = 
+        "TaskTree \\[pid=[0-9]*,tipID=.*\\] is "
+        + "running beyond memory-limits. "
+        + "Current usage : [0-9]*bytes. Limit : %sbytes. Killing task.";
+
+    Pattern taskOverLimitPattern = Pattern.compile(String.format(
+        taskOverLimitPatternString, 
+            String.valueOf(PER_TASK_LIMIT * 1024 * 1024L)));
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("String Appending");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrAppendMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+    jobConf.setMemoryForMapTask(PER_TASK_LIMIT);
+    jobConf.setMemoryForReduceTask(PER_TASK_LIMIT);
+
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null", jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+    Assert.assertNotNull("Task information is null.", taskInfo);
+
+    Assert.assertTrue("Task has not been started for 1 min.",
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+    Assert.assertTrue("Map process is not alive before task fails.", 
+        ttIns.isProcessTreeAlive(pid));
+
+    Assert.assertTrue("Task did not stop " + tID, 
+        ttClientIns.isTaskStopped(tID));
+
+    String[] taskDiagnostics = runJob.getTaskDiagnostics(tAttID);
+    Assert.assertNotNull("Task diagnostics is null.", taskDiagnostics);
+
+    for (String strVal : taskDiagnostics) {
+      mat = taskOverLimitPattern.matcher(strVal);
+      Assert.assertTrue("Taskover limit error message is not matched.", 
+          mat.find());
+    }
+
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    Assert.assertTrue("Job has not been completed...", 
+        cluster.getJTClient().isJobStopped(id));
+    ttIns = ttClientIns.getProxy();
+    ttIns.sendAction(action);
+    UtilsForTests.waitFor(1000);
+    Assert.assertTrue("Map process is still alive after task has been failed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  private static void cleanup(Path dir, Configuration conf) throws 
+      IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+
+  private static void createInput(Path inDir, Configuration conf) throws 
+      IOException {
+    String input = "Hadoop is framework for data intensive distributed " 
+        + "applications.\nHadoop enables applications to" 
+        + " work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+            + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    file.writeBytes(input);
+    file.close();
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfSuspendTask.java b/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfSuspendTask.java
new file mode 100644
index 0000000..8e701d3
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestChildsKillingOfSuspendTask.java
@@ -0,0 +1,319 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.junit.Test;
+import org.junit.Assert;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.common.RemoteExecution;
+
+import java.util.Collection;
+import java.util.Hashtable;
+
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTTaskInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import testjar.GenerateTaskChildProcess;
+
+public class TestChildsKillingOfSuspendTask {
+  private static final Log LOG = LogFactory
+      .getLog(TestChildsKillingOfSuspendTask.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static String confFile = "mapred-site.xml"; 
+  
+  @BeforeClass
+  public static void before() throws Exception {
+    Hashtable<String,Object> prop = new Hashtable<String,Object>();
+    prop.put("mapred.map.max.attempts",1L);
+    prop.put("mapred.task.timeout",30000L);
+    prop.put("mapreduce.job.complete.cancel.delegation.tokens", false);
+    String [] expExcludeList = {"java.net.ConnectException",
+    "java.io.IOException","org.apache.hadoop.metrics2.MetricsException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+   // cluster.restart();
+  }
+  
+  /**
+   * Verify the process tree clean up of a task after
+   * task is suspended and wait till the task is 
+   * terminated based on timeout. 
+   */
+  @Test
+  public void testProcessTreeCleanupOfSuspendTask() throws 
+      Exception {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrDisplayMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    jobConf.setMaxMapAttempts(1);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null",jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+	jtClient.isJobStarted(id));
+    JobStatus[] jobStatus = client.getAllJobs();
+    String userName = jobStatus[0].getUsername();
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+    Assert.assertTrue("Map process tree is not alive before task suspend.", 
+        ttIns.isProcessTreeAlive(pid));
+    LOG.info("Suspend the task of process id " + pid);
+    ExecuteShellCommand execcmd = new ExecuteShellCommand(userName, 
+           ttClientIns.getHostName(), "kill -SIGSTOP " + pid);
+    execcmd.start();
+    execcmd.join();
+    UtilsForTests.waitFor(30000);
+    Assert.assertTrue("Process(" + pid + ") has not been suspended", 
+        execcmd.getStatus());
+    ttIns = ttClientIns.getProxy();
+    UtilsForTests.waitFor(1000);
+    Assert.assertTrue("Map process is still alive after task has been failed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  /**
+   * Verify the process tree cleanup of task after task 
+   * is suspended and resumed the task before the timeout.
+   */
+  @Test
+  public void testProcessTreeCleanupOfSuspendAndResumeTask() throws
+      Exception {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrDisplayMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    jobConf.setMaxMapAttempts(1);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null",jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    JobStatus[] jobStatus = client.getAllJobs();
+    String userName = jobStatus[0].getUsername();
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+    
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+    Assert.assertTrue("Map process tree is not alive before task suspend.", 
+        ttIns.isProcessTreeAlive(pid));
+    LOG.info("Suspend the task of process id " + pid);
+    ExecuteShellCommand execcmd = new ExecuteShellCommand(userName, 
+           ttClientIns.getHostName(), "kill -SIGSTOP " + pid);
+    execcmd.start();
+    execcmd.join();
+
+    Assert.assertTrue("Process(" + pid + ") has not been suspended", 
+        execcmd.getStatus());
+    Assert.assertTrue("Map process is not alive after task "
+        + "has been suspended.", ttIns.isProcessTreeAlive(pid));
+    UtilsForTests.waitFor(5000);
+    ExecuteShellCommand execcmd1 = new ExecuteShellCommand(userName, 
+           ttClientIns.getHostName(), "kill -SIGCONT " + pid);
+    execcmd1.start();
+    execcmd1.join();
+    Assert.assertTrue("Suspended process(" + pid + ") has not been resumed", 
+        execcmd1.getStatus());
+    UtilsForTests.waitFor(5000);
+    Assert.assertTrue("Map process tree is not alive after task is resumed.", 
+        ttIns.isProcessTreeAlive(pid));
+  }
+  
+  private static void cleanup(Path dir, Configuration conf) throws 
+      IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+  
+  private static void createInput(Path inDir, Configuration conf) throws 
+      IOException {
+    String input = "Hadoop is framework for data intensive distributed " 
+        + "applications.\n Hadoop enables applications " 
+        + "to work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while(i < 10) {
+      file.writeBytes(input);
+      i++;
+    }
+    file.close();
+  }
+
+  class ExecuteShellCommand extends Thread {
+    String userName;
+    String cmd;
+    String hostName;
+    boolean exitStatus;
+    public ExecuteShellCommand(String userName, String hostName, String cmd) {
+      this.userName = userName;
+      this.hostName = hostName;
+      this.cmd = cmd;
+    }
+    public void run() {
+      try {
+        RemoteExecution.executeCommand(hostName, userName, cmd);
+        exitStatus = true;
+      } catch(InterruptedException iexp) {
+        LOG.warn("Thread is interrupted:" + iexp.getMessage());
+        exitStatus = false;
+      } catch(Exception exp) {
+        LOG.warn("Exception:" + exp.getMessage());
+        exitStatus = false;
+      }
+    }
+    public boolean getStatus(){
+      return exitStatus;
+    }
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestCluster.java b/src/test/system/java/org/apache/hadoop/mapred/TestCluster.java
index efe6fce..29f27ca 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestCluster.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestCluster.java
@@ -58,7 +58,11 @@ public class TestCluster {
 
   @BeforeClass
   public static void before() throws Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
     cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
     cluster.setUp();
   }
 
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestCorruptedDiskJob.java b/src/test/system/java/org/apache/hadoop/mapred/TestCorruptedDiskJob.java
new file mode 100644
index 0000000..2402326
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestCorruptedDiskJob.java
@@ -0,0 +1,177 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import com.jcraft.jsch.*;
+import java.util.List;
+import org.apache.hadoop.common.RemoteExecution;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.examples.RandomWriter;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.util.Tool;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Assert;
+import org.junit.Test;
+import java.io.IOException;
+import java.util.Hashtable;
+
+/**
+ * Submit a job. Corrupt some disks, when job is running 
+ * job should continue running and pass successfully.
+ */
+public class TestCorruptedDiskJob {
+  private static final Log LOG = LogFactory
+      .getLog(TestCorruptedDiskJob.class);
+  private static MRCluster cluster;
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static Configuration conf = new Configuration();
+  private static String confFile = "mapred-site.xml";
+  private static FileSystem dfs = null;
+  private static final int RW_BYTES_PER_MAP = 25 * 1024 * 1024;
+  private static final int RW_MAPS_PER_HOST = 2;
+  private static JobClient client = null;
+  int count = 0;
+  String userName = null;
+  JobStatus[] jobStatus = null;
+  private static List<TTClient> ttClients = null;
+
+  @BeforeClass
+  public static void before() throws Exception {
+    cluster = MRCluster.createCluster(conf);
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException"};
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    String newConfDir = cluster.
+        getConf().get("test.system.hdrc.hadoopnewconfdir");
+    LOG.info("newConfDir is :" + newConfDir);
+    String newMapredLocalDirPath = conf.get("mapred.local.dir");
+
+    //One of the disk is made corrupted by making the path inaccessible.
+    newMapredLocalDirPath  = newMapredLocalDirPath.replaceAll("1", "11");
+    LOG.info("newMapredLocalDirPath is :" + newMapredLocalDirPath);
+
+    Hashtable<String,String> prop = new Hashtable<String,String>();
+    prop.put("mapred.local.dir", newMapredLocalDirPath);
+
+    String userName = System.getProperty("user.name");
+    LOG.info("user name is :" + userName);
+
+    //Creating the string to modify taskcontroller.cfg
+    String replaceTaskControllerCommand = "cat " + newConfDir +
+        "/taskcontroller.cfg | grep -v mapred.local.dir  > " + 
+        newConfDir + "/tmp1.cfg;echo mapred.local.dir=" + 
+        newMapredLocalDirPath + " >> " + newConfDir + 
+        "/tmp2.cfg;cat " + newConfDir + 
+        "/tmp2.cfg  > " + newConfDir + 
+        "/taskcontroller.cfg;cat " + newConfDir + 
+        "/tmp1.cfg  >> " + newConfDir + "/taskcontroller.cfg;";
+      
+    ttClients = cluster.getTTClients();
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+
+    //Changing the taskcontroller.cfg file in all taktracker nodes.
+    //This is required as mapred.local.dir should match 
+    //in both mapred-site.xml and taskcontroller.cfg.
+    //This change can be done after cluster is brought up as
+    //Linux task controller will access taskcontroller.cfg
+    //when a job's task starts.
+    for ( int i = 0;i < ttClients.size();i++ ) {
+      TTClient ttClient = (TTClient)ttClients.get(i);
+      String ttClientHostName = ttClient.getHostName();
+      try {
+        RemoteExecution.executeCommand(ttClientHostName, userName,
+          replaceTaskControllerCommand);
+      } catch (Exception e) { e.printStackTrace(); };
+    }
+
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    client = cluster.getJTClient().getClient();
+    dfs = client.getFs();
+    dfs.delete(inputDir, true);
+    dfs.delete(outputDir, true);
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cluster.tearDown();
+    cluster.restart();
+    UtilsForTests.waitFor(1000);
+    dfs.delete(inputDir, true);
+    dfs.delete(outputDir, true);
+  }
+
+  /**
+   * This tests the corrupted disk. If a disk does not exist, still
+   * the job should run successfully.
+   */
+  @Test
+  public void testCorruptedDiskJob() throws 
+      Exception {
+
+    // Scale down the default settings for RandomWriter for the test-case
+    // Generates NUM_HADOOP_SLAVES * RW_MAPS_PER_HOST * RW_BYTES_PER_MAP
+    conf.setInt("test.randomwrite.bytes_per_map", RW_BYTES_PER_MAP);
+    conf.setInt("test.randomwriter.maps_per_host", RW_MAPS_PER_HOST);
+    String[] rwArgs = {inputDir.toString()};
+
+    // JTProtocol remoteJTClient
+    JTProtocol remoteJTClient = cluster.getJTClient().getProxy();
+
+    // JobInfo jInfo;
+    JobInfo jInfo = null;
+
+    dfs.delete(inputDir, true);
+
+    // Run RandomWriter
+    Assert.assertEquals(ToolRunner.run(conf, new RandomWriter(), rwArgs),
+        0);
+
+    jobStatus = client.getAllJobs();
+    JobID id = null;
+    //Getting the jobId of the just submitted job
+    id = jobStatus[0].getJobID();
+
+    LOG.info("jobid is :" + id.toString());
+
+    Assert.assertTrue("Failed to complete the job",
+    cluster.getJTClient().isJobStopped(id));
+
+    jInfo = remoteJTClient.getJobInfo(id);
+    JobStatus jStatus = jInfo.getStatus();
+
+    if (jStatus != null) {
+      Assert.assertEquals("Job has not succeeded...",
+        JobStatus.SUCCEEDED, jStatus.getRunState());
+    }
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheModifiedFile.java b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheModifiedFile.java
index e617bf8..86add22 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheModifiedFile.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheModifiedFile.java
@@ -88,6 +88,10 @@ public class TestDistributedCacheModifiedFile {
   @BeforeClass
   public static void setUp() throws Exception {
     cluster = MRCluster.createCluster(new Configuration());
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException","org.apache.hadoop.metrics2.MetricsException"};
+    cluster.setExcludeExpList(expExcludeList);
+
     cluster.setUp();
     client = cluster.getJTClient().getClient();
     dfs = client.getFs();
@@ -98,10 +102,12 @@ public class TestDistributedCacheModifiedFile {
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
     //Waiting for 5 seconds to make sure tasktrackers are ready 
     Thread.sleep(5000);
@@ -109,18 +115,20 @@ public class TestDistributedCacheModifiedFile {
 
   @AfterClass
   public static void tearDown() throws Exception {
-    cluster.tearDown();
     dfs.delete(URIPATH, true);
     
     Collection<TTClient> tts = cluster.getTTClients();
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
+    cluster.tearDown();
   }
 
   @Test
@@ -151,6 +159,7 @@ public class TestDistributedCacheModifiedFile {
       SleepJob job = new SleepJob();
       job.setConf(conf);
       conf = job.setupJobConf(5, 1, 1000, 1000, 100, 100);
+      conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
 
       //Before starting, Modify the file
       String input = "This will be the content of\n" + "distributed cache\n";
@@ -283,9 +292,6 @@ public class TestDistributedCacheModifiedFile {
           if (distributedFileCount != 2 && taskTrackerFound) {
             Assert.fail("The distributed cache file has to be two. " +
             		"But found was " + distributedFileCount);
-          } else if (distributedFileCount > 1 && !taskTrackerFound) {
-            Assert.fail("The distributed cache file cannot more than one." +
-            		" But found was " + distributedFileCount);
           } else if (distributedFileCount < 1)
             Assert.fail("The distributed cache file is less than one. " +
             		"But found was " + distributedFileCount);
@@ -296,14 +302,7 @@ public class TestDistributedCacheModifiedFile {
         }
       }
       //Allow the job to continue through MR control job.
-      for (TaskInfo taskInfoRemaining : taskInfos) {
-        FinishTaskControlAction action = new FinishTaskControlAction(TaskID
-           .downgrade(taskInfoRemaining.getTaskID()));
-        Collection<TTClient> tts = cluster.getTTClients();
-        for (TTClient cli : tts) {
-          cli.getProxy().sendAction(action);
-        }
-      }
+      cluster.signalAllTasks(rJob.getID());
 
       //Killing the job because all the verification needed
       //for this testcase is completed.
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCachePrivateFile.java b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCachePrivateFile.java
index 79765e3..a800aa8 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCachePrivateFile.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCachePrivateFile.java
@@ -85,10 +85,12 @@ public class TestDistributedCachePrivateFile {
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
 
     String input = "This will be the content of\n" + "distributed cache\n";
@@ -106,10 +108,12 @@ public class TestDistributedCachePrivateFile {
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
   }
 
@@ -123,6 +127,10 @@ public class TestDistributedCachePrivateFile {
     Configuration conf = new Configuration(cluster.getConf());
     JTProtocol wovenClient = cluster.getJTClient().getProxy();
 
+    String jobTrackerUserName = wovenClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
     //This counter will check for count of a loop,
     //which might become infinite.
     int count = 0;
@@ -224,8 +232,16 @@ public class TestDistributedCachePrivateFile {
               fileStatusMapredLocalDirUserName.getPath();
           FsPermission fsPermMapredLocalDirUserName =
               fileStatusMapredLocalDirUserName.getPermission();
-          Assert.assertTrue("Directory Permission is not 700",
-            fsPermMapredLocalDirUserName.equals(new FsPermission("700")));
+          //If userName of Jobtracker is same as username
+          //of jobSubmission, then the permissions are 770.
+          //Otherwise 570
+          if ( userName.compareTo(jobTrackerUserName) == 0 ) {
+            Assert.assertTrue("Directory Permission is not 770",
+              fsPermMapredLocalDirUserName.equals(new FsPermission("770")));
+          } else {
+            Assert.assertTrue("Directory Permission is not 570",
+              fsPermMapredLocalDirUserName.equals(new FsPermission("570")));
+          }
 
           //Get file status of all the directories 
           //and files under that path.
@@ -245,8 +261,16 @@ public class TestDistributedCachePrivateFile {
               distributedFileCount++;
               String filename = path.getName();
               FsPermission fsPerm = fileStatus.getPermission();
-              Assert.assertTrue("File Permission is not 777",
-                fsPerm.equals(new FsPermission("777")));
+              //If userName of Jobtracker is same as username
+              //of jobSubmission, then the permissions are 770.
+              //Otherwise 570
+              if ( userName.compareTo(jobTrackerUserName) == 0 ) {
+                Assert.assertTrue("File Permission is not 770",
+                  fsPerm.equals(new FsPermission("770")));
+              } else {
+                Assert.assertTrue("File Permission is not 570",
+                  fsPerm.equals(new FsPermission("570")));
+              }
             }
           }
         }
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheUnModifiedFile.java b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheUnModifiedFile.java
index 73a5740..75f97e9 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheUnModifiedFile.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestDistributedCacheUnModifiedFile.java
@@ -97,10 +97,12 @@ public class TestDistributedCacheUnModifiedFile {
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
    
     //Waiting for 5 seconds to make sure tasktrackers are ready
@@ -121,10 +123,12 @@ public class TestDistributedCacheUnModifiedFile {
     //Stopping all TTs
     for (TTClient tt : tts) {
       tt.kill();
+      tt.waitForTTStop();
     }
     //Starting all TTs
     for (TTClient tt : tts) {
       tt.start();
+      tt.waitForTTStart();
     }
   }
 
@@ -154,7 +158,7 @@ public class TestDistributedCacheUnModifiedFile {
       SleepJob job = new SleepJob();
       job.setConf(conf);
       conf = job.setupJobConf(5, 1, 1000, 1000, 100, 100);
-
+      conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
       DistributedCache.createSymlink(conf);
       URI uri = URI.create(uriPath);
       DistributedCache.addCacheFile(uri, conf);
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptError.java b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptError.java
new file mode 100644
index 0000000..0041ddc
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptError.java
@@ -0,0 +1,76 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestHealthScriptError {
+  public static MRCluster cluster;
+  public static HealthScriptHelper helper;
+  public static String remotePath;
+  public static String healthScriptError="healthScriptError";
+  public static String remoteHSPath = "test.system.hdrc.healthscript.path";
+  static final Log LOG = LogFactory.getLog(TestHealthScriptError.class);
+  
+  @BeforeClass
+  public static void setUp() throws java.lang.Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    remotePath = cluster.getConf().get(remoteHSPath);
+    helper = new HealthScriptHelper();
+  }
+  
+  /**
+   * The test will induce the ERROR with health script, asserts the task tracker
+   * is unhealthy and then revert backs to non error condition and verifies
+   * the task tracker is healthy. When the task tracker is marked unhealthy
+   * also verifies that is marked as blacklisted, and reverse is true when it is
+   * healthy. Also this verifies the custom error message that is set when
+   * the task tracker is marked unhealthy.
+   * @throws Exception in case of test errors
+   */
+  @Test
+  public void testInduceError() throws Exception { 
+    LOG.info("running testInduceError");
+    TTClient client = cluster.getTTClient();
+    Configuration tConf= client.getProxy().getDaemonConf();    
+    tConf.set("mapred.task.tracker.report.address",
+        cluster.getConf().get("mapred.task.tracker.report.address"));
+    String defaultHealthScript = tConf.get("mapred.healthChecker.script.path");
+    Assert.assertTrue("Health script was not set", defaultHealthScript != null);        
+    tConf.set("mapred.healthChecker.script.path", remotePath+File.separator+
+        healthScriptError);
+    tConf.setInt("mapred.healthChecker.interval", 1000);
+    helper.copyFileToRemoteHost(healthScriptError, client.getHostName(), 
+        remotePath, cluster);
+    cluster.restartDaemonWithNewConfig(client, "mapred-site.xml", tConf, 
+        Role.TT);
+    //make sure the TT is blacklisted
+    helper.verifyTTBlackList(tConf, client,
+        "ERROR Task Tracker status is fatal", cluster);
+    //Now put back the task tracker in a healthy state
+    cluster.restart(client, Role.TT);
+    //now do the opposite of blacklist verification
+    tConf = client.getProxy().getDaemonConf();
+    helper.deleteFileOnRemoteHost(remotePath+File.separator+healthScriptError,
+        client.getHostName());
+  } 
+  
+  @AfterClass
+  public static void tearDown() throws Exception {    
+    cluster.tearDown();
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptPathError.java b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptPathError.java
new file mode 100644
index 0000000..82e8f9b
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptPathError.java
@@ -0,0 +1,65 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestHealthScriptPathError {
+  public static MRCluster cluster;
+  public static HealthScriptHelper helper;
+  public static String remotePath;
+  public String invalidHealthScript="invalidHealthScript";
+  public static String remoteHSPath = "test.system.hdrc.healthscript.path";
+  static final Log LOG = LogFactory.getLog(TestHealthScriptPathError.class);
+  
+  @BeforeClass
+  public static void setUp() throws java.lang.Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    remotePath = cluster.getConf().get(remoteHSPath);
+    helper = new HealthScriptHelper();
+  }
+  /**
+   * Error in the test path and script will not run, the TT will not be marked
+   * unhealthy
+   * @throws Exception in case of test errors
+   */
+  @Test
+  public void testHealthScriptPathError() throws Exception {
+    LOG.info("running testHealthScriptPathError");
+    TTClient client = cluster.getTTClient();
+    Configuration tConf= client.getProxy().getDaemonConf();    
+    tConf.set("mapred.task.tracker.report.address",
+        cluster.getConf().get("mapred.task.tracker.report.address"));
+    String defaultHealthScript = tConf.get("mapred.healthChecker.script.path");
+    Assert.assertTrue("Health script was not set", defaultHealthScript != null);    
+    tConf.set("mapred.healthChecker.script.path", remotePath+File.separator+
+        invalidHealthScript);
+    tConf.setInt("mapred.healthChecker.interval",1000);
+    cluster.restartDaemonWithNewConfig(client, "mapred-site.xml", tConf, 
+        Role.TT);
+    //For a invalid health script the TT remains healthy
+    helper.verifyTTNotBlackListed( client, tConf, cluster);
+    cluster.restart(client, Role.TT);    
+    tConf = client.getProxy().getDaemonConf();
+  } 
+  
+  @AfterClass
+  public static void tearDown() throws Exception {    
+    cluster.tearDown();
+  }
+  
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptTimeout.java b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptTimeout.java
new file mode 100644
index 0000000..e042523
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestHealthScriptTimeout.java
@@ -0,0 +1,73 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestHealthScriptTimeout {
+  public static String remotePath;
+  public static MRCluster cluster;
+  public static HealthScriptHelper helper;
+  public String healthScriptTimeout="healthScriptTimeout";
+  public static String remoteHSPath = "test.system.hdrc.healthscript.path";
+  static final Log LOG = LogFactory.getLog(TestHealthScriptTimeout.class);
+  
+  @BeforeClass
+  public static void setUp() throws java.lang.Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    remotePath = cluster.getConf().get(remoteHSPath);    
+    helper = new HealthScriptHelper();
+  }
+  
+  /**
+   * In this case the test times out the task tracker will get blacklisted  . 
+   * @throws Exception in case of test errors 
+   */
+  @Test
+  public void testScriptTimeout() throws Exception {
+    LOG.info("running testScriptTimeout");
+    TTClient client = cluster.getTTClient();
+    Configuration tConf= client.getProxy().getDaemonConf();
+    int defaultTimeout = tConf.getInt("mapred.healthChecker.script.timeout", 0);
+    tConf.set("mapred.task.tracker.report.address",
+        cluster.getConf().get("mapred.task.tracker.report.address"));
+    Assert.assertTrue("Health script timeout was not set",defaultTimeout != 0);     
+    tConf.set("mapred.healthChecker.script.path", remotePath+File.separator+
+        healthScriptTimeout);
+    tConf.setInt("mapred.healthChecker.script.timeout", 100);
+    tConf.setInt("mapred.healthChecker.interval",1000);    
+    helper.copyFileToRemoteHost(healthScriptTimeout, client.getHostName(),
+        remotePath, cluster);
+    cluster.restartDaemonWithNewConfig(client, "mapred-site.xml", tConf, 
+        Role.TT);
+    //make sure the TT is blacklisted
+    helper.verifyTTBlackList(tConf, client, "Node health script timed out",
+        cluster);
+    //Now put back the task tracker in a health state
+    cluster.restart(client, Role.TT);
+    tConf = client.getProxy().getDaemonConf();
+    //now do the opposite of blacklist verification
+    helper.deleteFileOnRemoteHost(remotePath+File.separator+healthScriptTimeout,
+        client.getHostName());
+    
+  } 
+  
+  @AfterClass
+  public static void tearDown() throws Exception {    
+    cluster.tearDown();
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestHiRamJobWithBlackListTT.java b/src/test/system/java/org/apache/hadoop/mapred/TestHiRamJobWithBlackListTT.java
new file mode 100644
index 0000000..036d569
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestHiRamJobWithBlackListTT.java
@@ -0,0 +1,96 @@
+package org.apache.hadoop.mapred;
+
+import java.io.File;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster.Role;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import java.util.List;
+import java.util.ArrayList;
+
+public class TestHiRamJobWithBlackListTT {
+  static final Log LOG = LogFactory.getLog(TestHiRamJobWithBlackListTT.class);
+  private static HealthScriptHelper bListHelper = null;
+  public static String remotePath;
+  public static MRCluster cluster;
+  
+  @BeforeClass
+  public static void setUp() throws java.lang.Exception {
+    String [] expExcludeList = new String[2];
+    expExcludeList[0] = "java.net.ConnectException";
+    expExcludeList[1] = "java.io.IOException";
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    bListHelper = new HealthScriptHelper();
+    remotePath = cluster.getConf().get(TestHealthScriptError.remoteHSPath);
+  }
+  
+  /** Black List more than 25 % of task trackers , run the high ram
+   * job and make sure that no exception is thrown. 
+   * @throws Exception If fails to blacklist TT or run high ram high
+   */
+  @Test
+  public void testHiRamJobBlackListedTaskTrackers() throws Exception {
+    final HighRamJobHelper hRamHelper = new HighRamJobHelper();
+    List<TTClient> bListedTT = new ArrayList<TTClient>();
+    List<TTClient> tClient = cluster.getTTClients();
+    int count = tClient.size();
+    int moreThan25Per = count / 4 +1;
+    LOG.info ("More than 25 % of TTclient is "+moreThan25Per);
+    for (int i=0; i < moreThan25Per ; ++i) {
+      TTClient client = tClient.get(i);
+      bListedTT.add(client);
+      blackListTT(client);
+    }
+    //Now run the high ram job
+    JobClient jobClient = cluster.getJTClient().getClient();
+    JTProtocol remoteJTClient = cluster.getJTClient().getProxy();
+    Configuration conf = remoteJTClient.getDaemonConf();    
+    hRamHelper.runHighRamJob(conf, jobClient, remoteJTClient,
+        "Job did not succeed");
+    //put the task tracker back in healthy state
+    for( int i =0; i < bListedTT.size() ; ++i) {
+      unBlackListTT(bListedTT.get(i));
+    }
+  }
+  
+  @AfterClass
+  public static void tearDown() throws Exception {    
+    cluster.tearDown();
+  }
+  
+  private void unBlackListTT (TTClient client) throws Exception{
+    //Now put back the task tracker in a healthy state
+    cluster.restart(client, Role.TT);
+    bListHelper.deleteFileOnRemoteHost(remotePath + File.separator +
+        TestHealthScriptError.healthScriptError, client.getHostName());
+  }
+  
+  private void blackListTT(TTClient client) throws Exception {
+    Configuration tConf= client.getProxy().getDaemonConf();    
+    tConf.set("mapred.task.tracker.report.address",
+        cluster.getConf().get("mapred.task.tracker.report.address"));
+    String defaultHealthScript = tConf.get("mapred.healthChecker.script.path");
+    Assert.assertTrue("Health script was not set", defaultHealthScript != null);        
+    tConf.set("mapred.healthChecker.script.path", remotePath+File.separator+
+        TestHealthScriptError.healthScriptError);
+    tConf.setInt("mapred.healthChecker.interval", 1000);
+    bListHelper.copyFileToRemoteHost(TestHealthScriptError.healthScriptError, 
+        client.getHostName(), remotePath, cluster);
+    cluster.restartDaemonWithNewConfig(client, "mapred-site.xml", tConf, 
+        Role.TT);
+    //make sure the TT is blacklisted
+    bListHelper.verifyTTBlackList(tConf, client,
+        "ERROR Task Tracker status is fatal", cluster);
+  }
+
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestJobCacheDirectoriesCleanUp.java b/src/test/system/java/org/apache/hadoop/mapred/TestJobCacheDirectoriesCleanUp.java
new file mode 100644
index 0000000..f8ab73c
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestJobCacheDirectoriesCleanUp.java
@@ -0,0 +1,312 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapred.JobClient.NetworkedJob;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.Assert;
+
+import java.io.IOException;
+import java.util.Random;
+import java.util.HashMap;
+import java.util.ArrayList;
+
+/**
+ * Verifying the non-writable cache folders and files for various jobs. 
+ */
+public class TestJobCacheDirectoriesCleanUp {
+  private static final Log LOG = LogFactory
+      .getLog(TestJobCacheDirectoriesCleanUp.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static JTClient jtClient;
+  private static TTClient ttClient;
+  private static JTProtocol rtClient;
+  private static String CUSTOM_CREATION_FILE = "_custom_file_";
+  private static String CUSTOM_CREATION_FOLDER = "_custom_folder_";
+  private static FsPermission permission = new FsPermission(FsAction.READ, 
+          FsAction.READ, FsAction.READ);
+
+  @BeforeClass
+  public static void before() throws Exception {
+    conf = new Configuration();
+    cluster = MRCluster.createCluster(conf);
+    cluster.setUp();
+    jtClient = cluster.getJTClient();
+    rtClient = jtClient.getProxy();
+    
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cluster.tearDown();
+  }
+
+  /**
+   * Submit a job and create folders and files in work folder with 
+   * non-writable permissions under task attempt id folder.
+   * Wait till the job completes and verify whether the files 
+   * and folders are cleaned up or not.
+   * @throws IOException
+   */
+  @Test
+  public void testJobCleanupAfterJobCompletes() throws IOException {
+    HashMap<TTClient,ArrayList<String>> map = 
+        new HashMap<TTClient,ArrayList<String>>();
+    JobID jobId = createJobAndSubmit().getID();
+    Assert.assertTrue("Job has not been started for 1 min", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo [] taskInfos = rtClient.getTaskInfo(jobId);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        Assert.assertTrue("Task has not been started for 1 min ",
+            jtClient.isTaskStarted(taskinfo));
+        String tasktracker = getTaskTracker(taskinfo);
+        Assert.assertNotNull("TaskTracker has not been found", tasktracker);
+        TTClient ttclient = getTTClient(tasktracker);
+        UtilsForTests.waitFor(100);
+        map.put(ttClient, getTTClientMapRedLocalDirs(ttClient, 
+            taskinfo, jobId));
+      }
+    }
+
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min",
+        jtClient.isJobStopped(jobId));
+    UtilsForTests.waitFor(3000);
+    Assert.assertTrue("Job directories have not been cleaned up properly " + 
+        "after completion of job", verifyJobDirectoryCleanup(map));
+  }
+
+  /**
+   * Submit a job and create folders and files in work folder with 
+   * non-writable permissions under task attempt id folder.
+   * Kill the job and verify whether the files and folders
+   * are cleaned up or not.
+   * @throws IOException
+   */
+  @Test
+  public void testJobCleanupAfterJobKill() throws IOException {
+    HashMap<TTClient,ArrayList<String>> map = 
+        new HashMap<TTClient,ArrayList<String>>();
+    JobID jobId = createJobAndSubmit().getID();
+    Assert.assertTrue("Job has not been started for 1 min", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo [] taskInfos = rtClient.getTaskInfo(jobId);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        Assert.assertTrue("Task has not been started for 1 min ",
+            jtClient.isTaskStarted(taskinfo));
+        String tasktracker = getTaskTracker(taskinfo);
+        Assert.assertNotNull("TaskTracker has not been found", tasktracker);
+        TTClient ttclient = getTTClient(tasktracker);
+        map.put(ttClient, getTTClientMapRedLocalDirs(ttClient, 
+            taskinfo, jobId));
+      }
+    }
+    jtClient.getClient().killJob(jobId);
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min",
+        jtClient.isJobStopped(jobId));
+    JobInfo jobInfo = rtClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been killed", 
+            jobInfo.getStatus().getRunState(), JobStatus.KILLED);
+    UtilsForTests.waitFor(3000);
+    Assert.assertTrue("Job directories have not been cleaned up properly " + 
+        "after completion of job", verifyJobDirectoryCleanup(map));
+  }
+  
+  /**
+   * Submit a job and create folders and files in work folder with 
+   * non-writable permissions under task attempt id folder.
+   * Fail the job and verify whether the files and folders
+   * are cleaned up or not.
+   * @throws IOException
+   */
+  @Test
+  public void testJobCleanupAfterJobFail() throws IOException {
+    HashMap<TTClient,ArrayList<String>> map = 
+        new HashMap<TTClient,ArrayList<String>>();
+    conf = rtClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    JobConf jobConf = job.setupJobConf(1, 0, 10000,0, 10, 10);
+    JobClient client = jtClient.getClient();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID jobId = runJob.getID();
+    JobInfo jobInfo = rtClient.getJobInfo(jobId);
+    Assert.assertTrue("Job has not been started for 1 min", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo [] taskInfos = rtClient.getTaskInfo(jobId);
+    boolean isFailTask = false;
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {        
+        Assert.assertTrue("Task has not been started for 1 min ",
+            jtClient.isTaskStarted(taskinfo));
+        String tasktracker = getTaskTracker(taskinfo);
+        Assert.assertNotNull("TaskTracker has not been found", tasktracker);
+        TTClient ttclient = getTTClient(tasktracker);        
+        map.put(ttClient, getTTClientMapRedLocalDirs(ttClient, 
+            taskinfo, jobId));
+        if (!isFailTask) {
+          Assert.assertNotNull("TaskInfo is null.", taskinfo);
+          TaskID taskId = TaskID.downgrade(taskinfo.getTaskID());
+          TaskAttemptID taskAttID = new TaskAttemptID(taskId, 
+              taskinfo.numFailedAttempts());
+          int MAX_MAP_TASK_ATTEMPTS = Integer.
+               parseInt(jobConf.get("mapred.map.max.attempts"));
+          while(taskinfo.numFailedAttempts() < MAX_MAP_TASK_ATTEMPTS) {
+            NetworkedJob networkJob = jtClient.getClient().
+               new NetworkedJob(jobInfo.getStatus());
+            networkJob.killTask(taskAttID, true);
+            taskinfo = rtClient.getTaskInfo(taskinfo.getTaskID());
+            taskAttID = new TaskAttemptID(taskId, taskinfo.numFailedAttempts());
+            jobInfo = rtClient.getJobInfo(jobId);
+          }
+          isFailTask=true;
+        }
+      }
+    }
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min",
+        jtClient.isJobStopped(jobId));
+    jobInfo = rtClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been failed", 
+            jobInfo.getStatus().getRunState(), JobStatus.FAILED);
+    UtilsForTests.waitFor(3000); 
+    Assert.assertTrue("Directories have not been cleaned up " + 
+        "after completion of job", verifyJobDirectoryCleanup(map));
+  }
+  
+  private static ArrayList <String> getTTClientMapRedLocalDirs(
+      TTClient ttClient, TaskInfo taskinfo, JobID jobId) throws IOException {
+    ArrayList <String> fileList = null;
+    TaskID taskId = TaskID.downgrade(taskinfo.getTaskID());
+    FinishTaskControlAction action = new FinishTaskControlAction(taskId);
+    if (ttClient != null ) {
+      String localDirs[] = ttClient.getMapredLocalDirs();
+      TaskAttemptID taskAttID = new TaskAttemptID(taskId, 0);
+      fileList = createFilesInTaskDir(localDirs, jobId, taskAttID, ttClient);
+    }
+    ttClient.getProxy().sendAction(action);
+    return fileList;
+  }
+  
+  private static boolean verifyJobDirectoryCleanup(HashMap<TTClient, 
+    ArrayList<String>> map) throws IOException {
+    boolean status = true;
+    for (TTClient ttClient : map.keySet()) {
+      if (map.get(ttClient) != null) {
+      for(String path : map.get(ttClient)){
+        FileStatus [] fs = ttClient.listStatus(path, true);
+        if (fs.length > 0) {
+          status = false;
+        }
+      }
+      }
+    }
+    return status;
+  }
+  
+  private static ArrayList<String> createFilesInTaskDir(String [] localDirs, 
+      JobID jobId, TaskAttemptID taskAttID, TTClient ttClient) throws IOException {
+    Random random = new Random(100);
+    ArrayList<String> list = new ArrayList<String>();
+    String customFile = CUSTOM_CREATION_FILE + random.nextInt();
+    String customFolder = CUSTOM_CREATION_FOLDER + random.nextInt();
+    int index = 0;
+    for (String localDir : localDirs) {
+      String localTaskDir = localDir + "/" + 
+          TaskTracker.getLocalTaskDir(getUser(), jobId.toString(), 
+          taskAttID.toString() + "/work/");
+      boolean fstatus = false;
+      try {
+        fstatus = ttClient.getFileStatus(localTaskDir,true).isDir(); 
+      } catch(Exception exp) {
+        fstatus = false;
+      }
+      if (fstatus) {
+         ttClient.createFile(localTaskDir, customFile, permission, true);
+         ttClient.createFolder(localTaskDir, customFolder, permission, true);
+         list.add(localTaskDir + customFile);
+         list.add(localTaskDir + customFolder);
+      }
+     }
+     
+    return list;
+  }
+  
+  private static RunningJob createJobAndSubmit() throws IOException {
+    conf = rtClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    JobConf jobConf = job.setupJobConf(3, 1, 12000, 12000, 100, 100);
+    JobClient client = jtClient.getClient();
+    RunningJob runJob = client.submitJob(jobConf);
+    return runJob;
+  }
+  
+  private static String getUser() throws IOException {
+    JobStatus[] jobStatus = jtClient.getClient().getAllJobs();
+    String userName = jobStatus[0].getUsername();
+    return userName;
+  }
+  
+  private static String getTaskTracker(TaskInfo taskInfo) 
+      throws IOException {
+    String taskTracker = null;
+    String taskTrackers [] = taskInfo.getTaskTrackers();
+    int counter = 0;
+    while (counter < 30) {
+      if (taskTrackers.length != 0) {
+        taskTracker = taskTrackers[0];
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = rtClient.getTaskInfo(taskInfo.getTaskID());
+      taskTrackers = taskInfo.getTaskTrackers();
+      counter ++;
+    }
+    return taskTracker;
+  }
+
+  private static TTClient getTTClient(String taskTracker) {
+    String hostName = taskTracker.split("_")[1];
+    hostName = hostName.split(":")[0];
+    ttClient = cluster.getTTClient(hostName);
+    return ttClient;
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestJobHistoryLocation.java b/src/test/system/java/org/apache/hadoop/mapred/TestJobHistoryLocation.java
new file mode 100644
index 0000000..b33362c
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestJobHistoryLocation.java
@@ -0,0 +1,366 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.UtilsForTests;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.examples.SleepJob;
+import java.io.DataOutputStream;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+/**
+ * Verify the retired job history Location.
+ */
+
+public class TestJobHistoryLocation {
+
+  private static MRCluster cluster = null;
+  private static FileSystem dfs = null;
+  private static JobClient jobClient = null;
+  private static String jobHistoryDonePathString = null;
+  private static int count = 0;
+  private static int fileCount = 0;
+  private static boolean jobFileFound = false;
+  private static int retiredJobInterval = 0;
+  private static Configuration conf = null;
+
+  static final Log LOG = LogFactory.
+      getLog(TestJobHistoryLocation.class);
+
+  public TestJobHistoryLocation() throws Exception {
+  }
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    cluster = MRCluster.createCluster(new Configuration());
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException", "org.apache.hadoop.metrics2.MetricsException"};
+    cluster.setExcludeExpList(expExcludeList);
+
+    conf = new Configuration(cluster.getConf());
+    cluster.setUp();
+    jobClient = cluster.getJTClient().getClient();
+    dfs = jobClient.getFs();
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    cluster.tearDown();
+  }
+
+  /**
+   * This tests when successful / failed jobs are retired, the location
+   * of the retired jobs are as according to 
+   * mapred.job.tracker.history.completed.location.
+   * This tests when there are 100 files in the done directory,
+   * still the retired jobs are as according to
+   * mapred.job.tracker.history.completed.location.
+   * @param none
+   * @return void
+   */
+  @Test
+  public void testRetiredJobsHistoryLocation() throws Exception {
+    JTProtocol remoteJTClient = cluster.getJTClient().getProxy();
+    int testIterationLoop = 0;
+
+    do {
+      SleepJob job = null;
+      testIterationLoop++;
+      job = new SleepJob();
+      job.setConf(conf);
+      conf = job.setupJobConf(5, 1, 100, 100, 100, 100);
+      //Get the value of mapred.jobtracker.retirejob.check. If not
+      //found then use 60000 milliseconds, which is the application default.
+      retiredJobInterval = 
+        conf.getInt("mapred.jobtracker.retirejob.check", 60000);
+      //Assert if retiredJobInterval is 0
+      if ( retiredJobInterval == 0 ) {
+        Assert.fail("mapred.jobtracker.retirejob.check is 0");
+      }
+
+      conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+      jobFileFound = false;
+
+      JobConf jconf = new JobConf(conf);
+      jobHistoryDonePathString = null;
+      jobHistoryDonePathString = jconf.
+          get("mapred.job.tracker.history.completed.location");
+      //Assert if jobHistoryDonePathString is null
+      Assert.assertNotNull("mapred.job.tracker.history.completed.location " +
+          "is null", jobHistoryDonePathString); 
+
+      LOG.info("jobHistoryDonePath location is :" + jobHistoryDonePathString);
+
+      FileStatus[] jobHistoryDoneFileStatuses = dfs.
+          listStatus(new Path (jobHistoryDonePathString));
+      String jobHistoryPathString = jconf.get("hadoop.job.history.location");
+
+      //Submitting the job
+      RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+
+      JobID jobID = rJob.getID();
+      JobInfo jInfo = remoteJTClient.getJobInfo(jobID);
+      String jobIDString = jobID.toString();
+      LOG.info("jobIDString is :" + jobIDString);
+
+      //Assert if jobInfo is null
+      Assert.assertNotNull("jobInfo is null", jInfo);
+
+      waitTillRunState(jInfo, jobID, remoteJTClient);
+
+      if (jobHistoryPathString != null) {
+        FileStatus[] jobHistoryFileStatuses = dfs.
+          listStatus(new Path (jobHistoryPathString));
+        jobFileFound = false;
+        for (FileStatus jobHistoryFileStatus : jobHistoryFileStatuses) {
+          if ((jobHistoryFileStatus.getPath().toString()).
+              matches(jobIDString)) {
+            jobFileFound = true;
+            break;
+          }
+        }
+        Assert.assertTrue("jobFileFound is false", jobFileFound);
+      }
+
+      TaskInfo[] taskInfos = cluster.getJTClient().getProxy()
+          .getTaskInfo(rJob.getID());
+
+      //Killing this job will happen only in the second iteration.
+      if (testIterationLoop == 2) {
+        //Killing the job because all the verification needed
+        //for this testcase is completed.
+        rJob.killJob();
+      }
+
+      //Making sure that the job is complete.
+      count = 0;
+      while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
+        UtilsForTests.waitFor(10000);
+        count++;
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+        //If the count goes beyond 100 seconds, then break; This is to avoid
+        //infinite loop.
+        if (count > 10) {
+          Assert.fail("job has not reached running state for more than" +
+              "100 seconds. Failing at this point");
+        }
+      }
+
+      //After checking for Job Completion, waiting for 4 times of 
+      //retiredJobInterval seconds for the job to go to retired state 
+      UtilsForTests.waitFor(retiredJobInterval * 4);
+
+
+      jobHistoryDoneFileStatuses = dfs.
+          listStatus(new Path (jobHistoryDonePathString));
+
+      checkJobHistoryFileInformation( jobHistoryDoneFileStatuses, jobIDString);  
+      Assert.assertTrue("jobFileFound is false. Job History " +
+        "File is not found in the done directory",
+          jobFileFound);
+
+      Assert.assertEquals("Both the job related files are not found",
+        fileCount, 2);
+
+    } while ( testIterationLoop < 2 );
+  }
+
+  /**
+   * This tests when multiple instances of successful / failed jobs are 
+   * retired, the location of the retired jobs are as according to 
+   * mapred.job.tracker.history.completed.location 
+   * @param none
+   * @return void
+   */
+  @Test
+  public void testRetiredMultipleJobsHistoryLocation() throws Exception {
+    Configuration conf = new Configuration(cluster.getConf());
+    JTProtocol remoteJTClient = cluster.getJTClient().getProxy();
+    int testIterationLoop = 0;
+    FileStatus[] jobHistoryDoneFileStatuses;
+    RunningJob[] rJobCollection = new RunningJob[4];
+    JobID[] rJobIDCollection = new JobID[4];
+    String jobHistoryDonePathString = null;
+    JobInfo jInfo = null;
+    for ( int noOfJobs = 0; noOfJobs < 4; noOfJobs++ ) {
+      SleepJob job = null;
+      testIterationLoop++;
+      job = new SleepJob();
+      job.setConf(conf);
+      conf = job.setupJobConf(5, 1, 100, 100, 100, 100);
+      conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", 
+        false);
+      JobConf jconf = new JobConf(conf);
+
+      jobHistoryDonePathString = null;
+      jobHistoryDonePathString = jconf.
+          get("mapred.job.tracker.history.completed.location");
+      //Assert if jobHistoryDonePathString is null
+      Assert.assertNotNull("mapred.job.tracker.history.completed.location "
+          + "is null", jobHistoryDonePathString);
+
+      LOG.info("jobHistoryDonePath location is :" + 
+          jobHistoryDonePathString);
+
+      //Submitting the job
+      RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+      JobID jobID = rJob.getID();
+     
+      rJobCollection[noOfJobs] = rJob;
+      rJobIDCollection[noOfJobs] = jobID;
+
+      jInfo = remoteJTClient.getJobInfo(jobID);
+      LOG.info("jobIDString is :" + jobID.toString());
+      //Assert if jobInfo is null
+      Assert.assertNotNull("jobInfo is null", jInfo);
+    }
+
+    //Wait for the jobs to start running.
+    for (int noOfJobs = 0; noOfJobs < 4; noOfJobs++) {
+      waitTillRunState(jInfo, rJobIDCollection[noOfJobs], remoteJTClient);
+    }
+
+    //Killing two jobs 
+    (rJobCollection[0]).killJob();
+    (rJobCollection[3]).killJob();
+
+    //Making sure that the jobs are complete.
+    for (int noOfJobs = 0; noOfJobs < 4; noOfJobs++) {
+      count = 0;
+      while (remoteJTClient.getJobInfo(rJobIDCollection[noOfJobs]) != null && 
+          !(remoteJTClient.getJobInfo(rJobIDCollection[noOfJobs])).
+          getStatus().isJobComplete()) {
+        UtilsForTests.waitFor(10000);
+        count++;
+        //If the count goes beyond 100 seconds, then break; This is to avoid
+        //infinite loop.
+        if (count > 20) {
+          Assert.fail("job has not reached completed state for more than" +
+              "200 seconds. Failing at this point");
+        }
+      }
+    }
+
+    //After checking for Job Completion, waiting for 4 times 
+    // of retiredJobInterval seconds for the job to go to retired state 
+    UtilsForTests.waitFor(retiredJobInterval * 4);
+
+    jobHistoryDoneFileStatuses = dfs.
+        listStatus(new Path (jobHistoryDonePathString));
+
+    for (int noOfJobs = 0; noOfJobs < 4; noOfJobs++) {
+      checkJobHistoryFileInformation( jobHistoryDoneFileStatuses, 
+          (rJobIDCollection[noOfJobs]).toString());
+    Assert.assertTrue("jobFileFound is false. Job History " +
+        "File is not found in the done directory",
+          jobFileFound);
+    Assert.assertEquals("Both the job related files are not found",
+        fileCount, 2);
+
+    }
+  }
+
+  //Waiting till job starts running
+  private void waitTillRunState(JobInfo jInfo, JobID jobID, 
+      JTProtocol remoteJTClient) throws Exception { 
+    int count = 0;
+    while (jInfo != null && jInfo.getStatus().getRunState()
+        != JobStatus.RUNNING) {
+      UtilsForTests.waitFor(10000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(jobID);
+      //If the count goes beyond 100 seconds, then break; This is to avoid
+      //infinite loop.
+      if (count > 10) {
+        Assert.fail("job has not reached running state for more than" +
+            "100 seconds. Failing at this point");
+      }
+    }
+  }
+ 
+  //Checking for job file information in done directory
+  //Since done directory has sub directories search under all 
+  //the sub directories.
+  private void checkJobHistoryFileInformation( FileStatus[] 
+      jobHistoryDoneFileStatuses, String jobIDString ) throws Exception {
+    fileCount = 0;
+    jobFileFound = false;
+    for (FileStatus jobHistoryDoneFileStatus : jobHistoryDoneFileStatuses) {
+      FileStatus[] jobHistoryDoneFileStatuses1 = dfs.
+          listStatus(jobHistoryDoneFileStatus.getPath());
+      for (FileStatus jobHistoryDoneFileStatus1 : jobHistoryDoneFileStatuses1) {
+        FileStatus[] jobHistoryDoneFileStatuses2 = dfs.
+          listStatus(jobHistoryDoneFileStatus1.getPath());
+        for (FileStatus jobHistoryDoneFileStatus2 : 
+          jobHistoryDoneFileStatuses2) {
+
+          FileStatus[] jobHistoryDoneFileStatuses3 = dfs.
+            listStatus(jobHistoryDoneFileStatus2.getPath());
+          for (FileStatus jobHistoryDoneFileStatus3 : 
+            jobHistoryDoneFileStatuses3) {
+
+            FileStatus[] jobHistoryDoneFileStatuses4 = dfs.
+              listStatus(jobHistoryDoneFileStatus3.getPath());
+            for (FileStatus jobHistoryDoneFileStatus4 : 
+              jobHistoryDoneFileStatuses4) {
+
+              FileStatus[] jobHistoryDoneFileStatuses5 = dfs.
+                listStatus(jobHistoryDoneFileStatus4.getPath());
+              for (FileStatus jobHistoryDoneFileStatus5 : 
+                jobHistoryDoneFileStatuses5) {
+      
+                FileStatus[] jobHistoryDoneFileStatuses6 = dfs.
+                  listStatus(jobHistoryDoneFileStatus5.getPath());
+                for (FileStatus jobHistoryDoneFileStatus6 : 
+                  jobHistoryDoneFileStatuses6) {
+
+                  if ( (jobHistoryDoneFileStatus6.getPath().toString()).
+                      indexOf(jobIDString) != -1 ) {
+                    jobFileFound = true;
+                    fileCount++;
+                    //Both the conf file and the job file has to be present
+                    if (fileCount == 2) {
+                      break;
+                    }
+                  }
+                }
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestJobKill.java b/src/test/system/java/org/apache/hadoop/mapred/TestJobKill.java
index 099f824..79ebad9 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestJobKill.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestJobKill.java
@@ -136,12 +136,12 @@ public class TestJobKill {
       RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
       JobInfo info = wovenClient.getJobInfo(rJob.getID());
       Assert.assertNotNull("Job Info is null",info);
-      JobID id = rJob.getID();
-      while (info.runningMaps() != 1) {
-        Thread.sleep(1000);
-        info = wovenClient.getJobInfo(id);
-      }
+      JobID id = rJob.getID();      
+      Assert.assertTrue("Failed to start the job",
+          cluster.getJTClient().isJobStarted(id));
       rJob.killJob();
+      Assert.assertTrue("Failed to kill the job",
+          cluster.getJTClient().isJobStopped(id));
     }
     checkCleanup(jconf);
     deleteOutputDir();
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestJobSummary.java b/src/test/system/java/org/apache/hadoop/mapred/TestJobSummary.java
new file mode 100644
index 0000000..d6a44bc
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestJobSummary.java
@@ -0,0 +1,285 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.ClusterMetrics;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapred.JobQueueInfo;
+import org.apache.hadoop.mapred.JobHistory.Keys;
+import org.apache.hadoop.mapred.lib.IdentityReducer;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import testjar.GenerateTaskChildProcess;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.security.UserGroupInformation;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+/**
+ * Verifying the job-summary information at the end of a job.
+ */
+public class TestJobSummary {
+  private static final Log LOG = LogFactory.getLog(TestJobSummary.class);
+  private static MRCluster cluster;
+  private static JobClient jobClient = null;
+  private static JTProtocol remoteJTClient = null;
+  private static JTClient jtClient;
+  private static Configuration conf = new Configuration();
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static String queueName = null;
+  
+  @BeforeClass
+  public static void before() throws Exception {
+    String [] expExcludeList = {"java.net.ConnectException",
+                                "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    createInput(inputDir, conf);
+    jtClient = cluster.getJTClient();
+    jobClient = jtClient.getClient();
+    remoteJTClient = cluster.getJTClient().getProxy();
+    conf = remoteJTClient.getDaemonConf();
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+  }
+
+  /**
+   * Verifying the job summary information for killed job.
+   */
+  @Test
+  public void testJobSummaryInfoOfKilledJob() throws IOException, 
+          InterruptedException {
+    SleepJob job = new SleepJob();
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    job.setConf(conf);
+    conf = job.setupJobConf(2, 1, 4000, 4000, 100, 100);
+    JobConf jobConf = new JobConf(conf);
+    RunningJob runJob = jobClient.submitJob(jobConf);
+    JobID jobId = runJob.getID();
+    Assert.assertTrue("Job has not been started for 1 min.", 
+            jtClient.isJobStarted(jobId));
+    jobClient.killJob(jobId);
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min.", 
+        jtClient.isJobStopped(jobId));
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been succeeded", 
+        jInfo.getStatus().getRunState(), JobStatus.KILLED);
+    verifyJobSummaryInfo(jInfo,jobId);
+  }
+  
+  /**
+   * Verifying the job summary information for failed job.
+   */
+  @Test
+  public void testJobSummaryInfoOfFailedJob() throws IOException, 
+          InterruptedException {
+    conf = remoteJTClient.getDaemonConf();
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Fail Job");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.FailMapper.class);
+    jobConf.setReducerClass(IdentityReducer.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(1);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+    RunningJob runJob = jobClient.submitJob(jobConf);
+    JobID jobId = runJob.getID();  
+    Assert.assertTrue("Job has not been started for 1 min.", 
+            jtClient.isJobStarted(jobId));
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min.",
+        jtClient.isJobStopped(jobId));
+    JobInfo  jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been failed", 
+        jInfo.getStatus().getRunState(), JobStatus.FAILED);
+    verifyJobSummaryInfo(jInfo,jobId);
+  }
+  
+  /**
+   * Submit the job in different queue and verifying 
+   * the job queue information in job summary 
+   * after job is completed.
+   */
+  @Test
+  public void testJobQueueInfoInJobSummary() throws IOException, 
+  InterruptedException {
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    conf = job.setupJobConf(2, 1, 4000, 4000, 100, 100);
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    JobConf jobConf = new JobConf(conf);
+    JobQueueInfo [] queues = jobClient.getQueues();
+    for (JobQueueInfo queueInfo : queues ){
+      if (!queueInfo.getQueueName().equals("default")) {
+        queueName = queueInfo.getQueueName();
+        break;
+      }
+    }
+    Assert.assertNotNull("No multiple queues in the cluster.",queueName);
+    LOG.info("queueName:" + queueName);
+    jobConf.setQueueName(queueName);
+    RunningJob runJob = jobClient.submitJob(jobConf);
+    JobID jobId = runJob.getID();    
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min.",
+        jtClient.isJobStopped(jobId));
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been succeeded", 
+        jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+    verifyJobSummaryInfo(jInfo,jobId);
+  }
+
+  /**
+   * Verify the job summary information for high RAM jobs.
+   */
+  @Test
+  public void testJobSummaryInfoOfHighMemoryJob() throws IOException,
+      Exception {
+    final HighRamJobHelper helper = new HighRamJobHelper();
+    JobID jobId = helper.runHighRamJob(conf, jobClient, remoteJTClient,
+        "Job did not succeed");
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    verifyJobSummaryInfo(jInfo,jobId);
+  }
+
+  @Test
+  public void testJobSummaryInfoForDifferentUser() throws Exception {
+    UserGroupInformation proxyUGI;
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+    ArrayList<String> users = cluster.getHadoopMultiUsersList();
+    Assert.assertTrue("proxy users are not found.", users.size() > 0);
+    if (conf.get("hadoop.security.authentication").equals("simple")) {
+      proxyUGI = UserGroupInformation.createRemoteUser(
+          users.get(0));
+    } else {
+      proxyUGI = UserGroupInformation.createProxyUser(
+      users.get(0), ugi);
+    }
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    final JobConf jobConf = job.setupJobConf(2, 1, 2000, 2000, 100, 100);
+    final JobClient jClient =
+    	proxyUGI.doAs(new PrivilegedExceptionAction<JobClient>() {
+          public JobClient run() throws IOException {
+            return new JobClient(jobConf);
+          }
+        });
+    RunningJob runJob = proxyUGI.doAs(
+        new PrivilegedExceptionAction<RunningJob>() {
+      public RunningJob run() throws IOException {
+        return jClient.submitJob(jobConf);
+      }
+    });
+    JobID jobId = runJob.getID();
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    LOG.info("Waiting till the job is completed...");
+    Assert.assertTrue("Job has not been completed for 1 min.",
+        jtClient.isJobStopped(jobId));
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertEquals("Job has not been succeeded", 
+        jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+     verifyJobSummaryInfo(jInfo,jobId);  
+  }
+  
+
+  private void verifyJobSummaryInfo(JobInfo jInfo, JobID id) 
+      throws IOException {
+    java.util.HashMap<String,String> map = jtClient.getJobSummary(id);
+    Assert.assertEquals("Job id has not been matched", id.toString(),
+        map.get("jobId"));
+    Assert.assertEquals("User name has not been matched in JobSummary", 
+        jInfo.getStatus().getUsername(), map.get("user"));    
+    Assert.assertEquals("StartTime has not been  matched in JobSummary", 
+        String.valueOf(jInfo.getStatus().getStartTime()), 
+        map.get("startTime"));
+    Assert.assertEquals("LaunchTime has not been matched in JobSummary", 
+        String.valueOf(jInfo.getLaunchTime()), 
+        map.get("launchTime"));
+    Assert.assertEquals("FinshedTime has not been matched in JobSummary", 
+        String.valueOf(jInfo.getFinishTime()), 
+        map.get("finishTime"));
+    Assert.assertEquals("Maps are not matched in Job summary", 
+        String.valueOf(jInfo.numMaps()) , map.get("numMaps"));
+    Assert.assertEquals("Reducers are not matched in Job summary", 
+        String.valueOf(jInfo.numReduces()), map.get("numReduces"));
+    Assert.assertEquals("Number of slots per map is not matched in Job summary", 
+        String.valueOf(jInfo.getNumSlotsPerMap()), map.get("numSlotsPerMap"));
+    Assert.assertEquals("Number of slots per reduce is not matched in Job summary", 
+        String.valueOf(jInfo.getNumSlotsPerReduce()), map.get("numSlotsPerReduce"));
+  }
+  
+
+  private static void cleanup(Path dir, Configuration conf) throws 
+          IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+
+  private static void createInput(Path inDir, Configuration conf) throws 
+    IOException {
+    String input = "Hadoop is framework for data intensive distributed " 
+        + "applications.\n" 
+        + "Hadoop enables applications to work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+    FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while (i < 2) {
+      file.writeBytes(input);
+      i++;
+    }
+    file.close();
+  }
+  
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUser.java b/src/test/system/java/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUser.java
new file mode 100644
index 0000000..7b63bbc
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestLinuxTaskControllerOtherUser.java
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+import java.io.IOException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.security.UserGroupInformation;
+import java.security.PrivilegedExceptionAction;
+
+/**
+ * Verifying the job submissions with original user and other user
+ * with Linux task Controller.Original user should succeed and other
+ * user should not.
+ */
+public class TestLinuxTaskControllerOtherUser {
+  private static final Log LOG = LogFactory.
+      getLog(TestLinuxTaskControllerOtherUser.class);
+  private static MRCluster cluster = null;
+  private static JobClient jobClient = null;
+  private static JTProtocol remoteJTClient = null;
+  private static JTClient jtClient = null;
+  private static Configuration conf = new Configuration();
+  private static UserGroupInformation proxyUGI = null;
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output"); 
+
+  @BeforeClass
+  public static void before() throws Exception {
+    cluster = MRCluster.createCluster(conf);
+    cluster.setUp();
+    jtClient = cluster.getJTClient();
+    jobClient = jtClient.getClient();
+    remoteJTClient = cluster.getJTClient().getProxy();
+    conf = remoteJTClient.getDaemonConf();
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cluster.tearDown();
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+  }
+ 
+  /**
+   * Submit a Sleep Job with a diferent user id and verify it failure
+   * @param none
+   * @return void
+   */
+  @Test
+  public void testSubmitJobDifferentUserJobClient() throws Exception {
+    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
+    LOG.info("LoginUser:" + ugi);
+    if (conf.get("mapred.task.tracker.task-controller").
+        equals("org.apache.hadoop.mapred.LinuxTaskController")) {
+      //Changing the User name
+      proxyUGI = UserGroupInformation.createRemoteUser(
+          "hadoop1");
+ 
+      SleepJob job = new SleepJob();
+      job.setConf(conf);
+      final JobConf jobConf = job.setupJobConf(2, 1, 2000, 2000, 100, 100);
+      String error = null;
+      RunningJob runJob = null;
+      //Getting the jobClient with the changed remote user and 
+      //then submit the command.
+      try {
+        final JobClient jClient =
+            proxyUGI.doAs(new PrivilegedExceptionAction<JobClient>() {
+          public JobClient run() throws IOException {
+            return new JobClient(jobConf);
+          }
+        });
+
+        runJob = proxyUGI.doAs(
+          new PrivilegedExceptionAction<RunningJob>() {
+          public RunningJob run() throws IOException {
+            return jClient.submitJob(jobConf);
+          }
+        });
+      } catch (Exception e) {error = e.toString();}
+      //A error is expected to be thrown
+      if (error.indexOf("No valid credentials provided") != -1) {
+        LOG.info("e's value is :" + error);
+      } else {
+        Assert.fail("Some unknown error is thrown :" + error);
+      }
+      Assert.assertNull("Job is still running", runJob);
+    }
+  }
+
+  //Cleanup directories in dfs.
+  private static void cleanup(Path dir, Configuration conf)
+      throws IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestLostTaskTracker.java b/src/test/system/java/org/apache/hadoop/mapred/TestLostTaskTracker.java
new file mode 100644
index 0000000..6458269
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestLostTaskTracker.java
@@ -0,0 +1,258 @@
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTClient; 
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.TTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Hashtable;
+
+public class TestLostTaskTracker {
+  private static final Log LOG = LogFactory
+      .getLog(TestLostTaskTracker.class);
+  private static MRCluster cluster;
+  private static Configuration conf = new Configuration();
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static String confFile = "mapred-site.xml";
+  private JTProtocol wovenClient = null;
+  private JobID jID = null;
+  private JobInfo jInfo = null;
+  private JTClient jtClient = null;
+
+  @BeforeClass
+  public static void before() throws Exception {
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    Hashtable<String,Object> prop = new Hashtable<String,Object>();
+    prop.put("mapred.tasktracker.expiry.interval",30000L);
+    prop.put("mapreduce.job.complete.cancel.delegation.tokens",false);
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+    cluster.restart();
+  }
+  /**
+   * Verify the job status whether it is succeed or not when 
+   * lost task tracker is alive before the timeout.
+   * @throws IOException if an I/O error occurs.
+   */
+  @Test
+  public void testJobStatusOfLostTaskTracker1() throws
+      Exception{
+    String testName = "LTT1";
+    setupJobAndRun();
+    JobStatus jStatus = verifyLostTaskTrackerJobStatus(testName);    
+    Assert.assertEquals("Job has not been succeeded...", 
+         JobStatus.SUCCEEDED, jStatus.getRunState());
+  }
+  
+  /**
+   * Verify the job status whether it is succeeded or not when 
+   * the lost task trackers time out for all four attempts of a task. 
+   * @throws IOException if an I/O error occurs.
+   */
+  @Test
+  public void testJobStatusOfLostTracker2()  throws 
+      Exception {
+    String testName = "LTT2";
+    setupJobAndRun();
+    JobStatus jStatus = verifyLostTaskTrackerJobStatus(testName);
+    Assert.assertEquals("Job has not been failed...", 
+            JobStatus.SUCCEEDED, jStatus.getRunState());
+  }
+
+  private void setupJobAndRun() throws IOException { 
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    conf = job.setupJobConf(3, 1, 60000, 100, 60000, 100);
+    JobConf jobConf = new JobConf(conf);
+    cleanup(outputDir, conf);
+    jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    jID = runJob.getID();
+    jInfo = wovenClient.getJobInfo(jID);
+    Assert.assertNotNull("Job information is null",jInfo);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jID));
+    JobStatus jobStatus = jInfo.getStatus();
+    // Make sure that job should run and completes 40%. 
+    while (jobStatus.getRunState() != JobStatus.RUNNING && 
+      jobStatus.mapProgress() < 0.4f) {
+      UtilsForTests.waitFor(100);
+      jobStatus = wovenClient.getJobInfo(jID).getStatus();
+    }
+  }
+  
+  private JobStatus verifyLostTaskTrackerJobStatus(String testName) 
+      throws IOException{
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    String[] taskTrackers = null;
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(jID);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+    Assert.assertTrue("Task has not been started for 1 min.",
+            jtClient.isTaskStarted(taskInfo));
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TTClient ttClient = getTTClientIns(taskInfo);
+    int counter = 0;
+    while (counter < 30) {
+      if (ttClient != null) {
+        break;
+      }else{
+         taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());  
+         ttClient = getTTClientIns(taskInfo); 
+      }
+      counter ++;
+    }
+    Assert.assertNotNull("TaskTracker has not been found",ttClient);
+    if (testName.equals("LTT1")) {
+        ttClient.kill();
+        waitForTTStop(ttClient);
+        UtilsForTests.waitFor(20000);
+        ttClient.start();
+        waitForTTStart(ttClient);
+    } else {
+       int index = 0 ;
+       while(index++ < 4 ) {
+           ttClient.kill();
+           waitForTTStop(ttClient);
+           UtilsForTests.waitFor(40000);
+           ttClient.start();
+           waitForTTStart(ttClient);
+           taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+           ttClient = getTTClientIns(taskInfo);
+           counter = 0;
+           while (counter < 30) {
+             if (ttClient != null) {
+               break;
+             }else{
+                taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());  
+                ttClient = getTTClientIns(taskInfo); 
+             }
+             counter ++;
+           }
+           Assert.assertNotNull("TaskTracker has not been found",ttClient);
+           LOG.info("Task killed attempts:" + 
+               taskInfo.numKilledAttempts());
+       }
+       Assert.assertEquals("Task killed attempts are not matched ",
+           4, taskInfo.numKilledAttempts());
+    }
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(jID);
+    }
+    return jInfo.getStatus();
+  }
+
+  private TTClient getTTClientIns(TaskInfo taskInfo) throws IOException{
+    String [] taskTrackers = taskInfo.getTaskTrackers();
+    int counter = 0;
+    TTClient ttClient = null;
+    while (counter < 60) {
+      if (taskTrackers.length != 0) {
+        break;
+      }
+      UtilsForTests.waitFor(100);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      taskTrackers = taskInfo.getTaskTrackers();
+      counter ++;
+    }
+    if ( taskTrackers.length != 0) {
+      String hostName = taskTrackers[0].split("_")[1];
+      hostName = hostName.split(":")[0];
+      ttClient = cluster.getTTClient(hostName);
+    }
+    return ttClient;
+  }
+  private void waitForTTStart(TTClient ttClient) throws 
+     IOException {
+    LOG.debug(ttClient.getHostName() + " is waiting to come up.");
+    while (true) { 
+      try {
+        ttClient.ping();
+        LOG.info("TaskTracker : " + ttClient.getHostName() + " is pinging...");
+        break;
+      } catch (Exception exp) {
+        LOG.debug(ttClient.getHostName() + " is waiting to come up.");
+        UtilsForTests.waitFor(10000);
+      }
+    }
+  }
+  
+  private void waitForTTStop(TTClient ttClient) throws 
+     IOException {
+    LOG.info("Waiting for Tasktracker:" + ttClient.getHostName() 
+        + " to stop.....");
+    while (true) {
+      try {
+        ttClient.ping();
+        LOG.debug(ttClient.getHostName() +" is waiting state to stop.");
+        UtilsForTests.waitFor(10000);
+      } catch (Exception exp) {
+        LOG.info("TaskTracker : " + ttClient.getHostName() + " is stopped...");
+        break;
+      } 
+    }
+  }
+  
+  private static void cleanup(Path dir, Configuration conf) throws 
+      IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+
+  private static void createInput(Path inDir, Configuration conf) throws 
+      IOException {
+    String input = "Hadoop is framework for data intensive distributed " 
+        + "applications.\nHadoop enables applications to" 
+        + " work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    file.writeBytes(input);
+    file.close();
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestNodeDecommissioning.java b/src/test/system/java/org/apache/hadoop/mapred/TestNodeDecommissioning.java
new file mode 100644
index 0000000..249061c
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestNodeDecommissioning.java
@@ -0,0 +1,199 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.Hashtable;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.test.system.process.HadoopDaemonRemoteCluster;
+import java.util.List;
+import java.io.IOException;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.common.RemoteExecution;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.Path;
+import java.net.InetAddress;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+/**
+ * Verify the task tracker node Decommission.
+ */
+
+public class TestNodeDecommissioning {
+
+  private static MRCluster cluster = null;
+  private static FileSystem dfs = null;
+  private static JobClient jobClient = null;
+  private static Configuration conf = null;
+  private static Path excludeHostPath = null;
+ 
+  static final Log LOG = LogFactory.
+      getLog(TestNodeDecommissioning.class);
+
+  public TestNodeDecommissioning() throws Exception {
+  }
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    cluster = MRCluster.createCluster(new Configuration());
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException"};
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    jobClient = cluster.getJTClient().getClient();
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    String confFile = "mapred-site.xml";
+    Hashtable<String,String> prop = new Hashtable<String,String>();
+    prop.put("mapred.hosts.exclude", "/tmp/mapred.exclude");
+    prop.put("mapreduce.cluster.administrators", " gridadmin,hadoop,users");
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    cluster.restart();
+    cluster.tearDown();
+  }
+
+  /**
+   * This tests whether a node is successfully able to be
+   * decommissioned or not.
+   * First a node is decommissioned and verified.
+   * Then it is removed from decommissoned and verified again.
+   * At last the node is started.
+   * @param none
+   * @return void
+   */
+  @Test
+  public void TestNodeDecommissioning() throws Exception {
+
+    JTProtocol remoteJTClientProxy = cluster.getJTClient().getProxy();
+
+    JTClient remoteJTClient = cluster.getJTClient();
+    String jtClientHostName = remoteJTClient.getHostName();
+    InetAddress localMachine = java.net.InetAddress.getLocalHost();
+    String testRunningHostName = localMachine.getHostName(); 
+    LOG.info("Hostname of local machine: " + testRunningHostName);
+
+    List<TTClient> ttClients = cluster.getTTClients();
+
+    //One slave is got
+    TTClient ttClient = (TTClient)ttClients.get(0);
+    String ttClientHostName = ttClient.getHostName();
+
+    //Hadoop Conf directory is got
+    String hadoopConfDir = cluster.getConf().get(
+        HadoopDaemonRemoteCluster.CONF_HADOOPCONFDIR);
+
+    LOG.info("hadoopConfDir is:" + hadoopConfDir);
+
+    //Hadoop Home is got
+    String hadoopHomeDir = cluster.getConf().get(
+        HadoopDaemonRemoteCluster.CONF_HADOOPHOME);
+
+    LOG.info("hadoopHomeDir is:" + hadoopHomeDir);
+
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    //"mapred.hosts.exclude" path is got
+    String excludeHostPathString = (String) conf.get("mapred.hosts.exclude");
+    String keytabForHadoopqaUser = 
+        "/homes/hadoopqa/hadoopqa.dev.headless.keytab hadoopqa"; 
+    excludeHostPath = new Path(excludeHostPathString);
+    LOG.info("exclude Host pathString is :" + excludeHostPathString);
+
+    //One sleep job is submitted
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    conf = job.setupJobConf(1, 0, 100, 100, 100, 100);
+    JobConf jconf = new JobConf(conf);
+    RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+
+    //username who submitted the job is got.
+    String userName = null;
+    try {
+      JobStatus[] jobStatus = cluster.getJTClient().getClient().getAllJobs();
+      userName = jobStatus[0].getUsername();
+    } catch(Exception ex) {
+      LOG.error("Failed to get user name");
+      boolean status = false;
+      Assert.assertTrue("Failed to get the userName", status);
+    }
+
+    //The client which needs to be decommissioned is put in the exclude path. 
+    String command = "echo " + ttClientHostName + " > " + excludeHostPath;
+ 
+    LOG.info("command is : " + command);
+    RemoteExecution.executeCommand(jtClientHostName, userName, command);
+
+    //The refreshNode command is created and execute in Job Tracker Client.
+    String refreshNodeCommand = "export HADOOP_CONF_DIR=" + hadoopConfDir + 
+        "; export HADOOP_HOME=" + hadoopHomeDir + ";cd " + hadoopHomeDir + 
+        ";kinit -k -t " + keytabForHadoopqaUser + 
+        ";bin/hadoop mradmin -refreshNodes;"; 
+    LOG.info("refreshNodeCommand is : " + refreshNodeCommand);
+    try {
+      RemoteExecution.executeCommand(testRunningHostName, userName, 
+          refreshNodeCommand);
+    } catch (Exception e) { e.printStackTrace();}
+
+    //Checked whether the node is really decommissioned.
+    boolean nodeDecommissionedOrNot = false;
+    nodeDecommissionedOrNot = remoteJTClientProxy.
+        isNodeDecommissioned(ttClientHostName); 
+
+    //The TTClient host is removed from the exclude path
+    command = "rm " + excludeHostPath;
+
+    LOG.info("command is : " + command);
+    RemoteExecution.executeCommand(jtClientHostName, userName, command);
+
+    Assert.assertTrue("Node should be decommissioned", nodeDecommissionedOrNot);
+
+    //The refreshNode command is created and execute in Job Tracker Client.
+    RemoteExecution.executeCommand(jtClientHostName, userName, 
+        refreshNodeCommand);
+
+    //Checked whether the node is out of decommission.
+    nodeDecommissionedOrNot = false;
+    nodeDecommissionedOrNot = remoteJTClientProxy.
+        isNodeDecommissioned(ttClientHostName); 
+    Assert.assertFalse("present of not is", nodeDecommissionedOrNot);
+
+    //Starting that node
+    String ttClientStart = "export HADOOP_CONF_DIR=" + hadoopConfDir +
+        "; export HADOOP_HOME=" + hadoopHomeDir + ";cd " + hadoopHomeDir +
+        ";kinit -k -t " + keytabForHadoopqaUser + 
+        ";bin/hadoop-daemons.sh start tasktracker;";
+    LOG.info("ttClientStart is : " + ttClientStart);
+    RemoteExecution.executeCommand(jtClientHostName, userName,
+        ttClientStart);
+  }
+}
+
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestPushConfig.java b/src/test/system/java/org/apache/hadoop/mapred/TestPushConfig.java
index 157c0a7..8c0f69f 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestPushConfig.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestPushConfig.java
@@ -1,12 +1,29 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.hadoop.mapred;
-import java.io.File;
-import java.io.FileOutputStream;
+
+
+import java.util.Hashtable;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapreduce.test.system.MRCluster;
-import org.apache.hadoop.test.system.AbstractDaemonClient;
-import org.apache.hadoop.test.system.process.HadoopDaemonRemoteCluster;
 import org.junit.AfterClass;
 import org.junit.Assert;
 import org.junit.BeforeClass;
@@ -14,7 +31,6 @@ import org.junit.Test;
 
 public class TestPushConfig {
   private static MRCluster cluster;
-  private String localConfDir = "localconf";
   private static final Log LOG = LogFactory.getLog(
       TestPushConfig.class.getName());
   
@@ -40,106 +56,24 @@ public class TestPushConfig {
    * in local input directory and pushes all the files from the local to the 
    * remote conf directory. This functionality is required is change the config
    * on the fly and restart the cluster which will be used by other test cases
-   * @throws Exception 
+   * @throws Exception if an I/O error occurs.
    */
   @Test
   public void testPushConfig() throws Exception {
     final String DUMMY_CONFIG_STRING = "mapred.newdummy.conf";
-    final String DUMMY_CONFIG_STRING_VALUE = "HerriotTestRules";
-    Configuration origconf = new Configuration(cluster.getConf());
-    origconf.set(DUMMY_CONFIG_STRING, DUMMY_CONFIG_STRING_VALUE);
-    String localDir = HadoopDaemonRemoteCluster.getDeployedHadoopConfDir() + 
-        File.separator + localConfDir;
-    File lFile = new File(localDir);
-    if(!lFile.exists()){
-      lFile.mkdir();
-    }
-    String mapredConf = localDir + File.separator + "mapred-site.xml";
-    File file = new File(mapredConf);
-    origconf.writeXml(new FileOutputStream(file));    
+    String confFile = "mapred-site.xml";
+    Hashtable<String,Long> prop = new Hashtable<String,Long>();
+    prop.put(DUMMY_CONFIG_STRING, 1L);
     Configuration daemonConf =  cluster.getJTClient().getProxy().getDaemonConf();
     Assert.assertTrue("Dummy varialble is expected to be null before restart.",
         daemonConf.get(DUMMY_CONFIG_STRING) == null);
-    String newDir = cluster.getClusterManager().pushConfig(localDir);
-    cluster.stop();
-    AbstractDaemonClient cli = cluster.getJTClient();
-    waitForClusterStop(cli);
-    // make sure the cluster has actually stopped
-    cluster.getClusterManager().start(newDir);
-    cli = cluster.getJTClient();
-    waitForClusterStart(cli);
-    // make sure the cluster has actually started
+    cluster.restartClusterWithNewConfig(prop, confFile);    
     Configuration newconf = cluster.getJTClient().getProxy().getDaemonConf();
     Assert.assertTrue("Extra varialble is expected to be set",
-        newconf.get(DUMMY_CONFIG_STRING).equals(DUMMY_CONFIG_STRING_VALUE));
-    cluster.getClusterManager().stop(newDir);
-    cli = cluster.getJTClient();
-    // make sure the cluster has actually stopped
-    waitForClusterStop(cli);
-    // start the daemons with original conf dir
-    cluster.getClusterManager().start();
-    cli = cluster.getJTClient();    
-    waitForClusterStart(cli);  
-    daemonConf =  cluster.getJTClient().getProxy().getDaemonConf();
+        newconf.get(DUMMY_CONFIG_STRING).equals("1"));
+    cluster.restart();
+    daemonConf = cluster.getJTClient().getProxy().getDaemonConf();
     Assert.assertTrue("Dummy variable is expected to be null after restart.",
-        daemonConf.get(DUMMY_CONFIG_STRING) == null);
-    lFile.delete();
-  }
-  
-  private void waitForClusterStop(AbstractDaemonClient cli) throws Exception {
-    int i=1;
-    while (i < 40) {
-      try {
-        cli.ping();
-        Thread.sleep(1000);
-        i++;
-      } catch (Exception e) {
-        break;
-      }
-    }
-    for (AbstractDaemonClient tcli : cluster.getTTClients()) {
-      i = 1;
-      while (i < 40) {
-        try {
-          tcli.ping();
-          Thread.sleep(1000);
-          i++;
-        } catch (Exception e) {
-          break;
-        }
-      }
-      if (i >= 40) {
-        Assert.fail("TT on " + tcli.getHostName() + " Should have been down.");
-      }
-    }
-  }
-  
-  private void waitForClusterStart(AbstractDaemonClient cli) throws Exception {
-    int i=1;
-    while (i < 40) {
-      try {
-        cli.ping();
-        break;
-      } catch (Exception e) {
-        i++;
-        Thread.sleep(1000);
-        LOG.info("Waiting for Jobtracker on host : "
-            + cli.getHostName() + " to come up.");
-      }
-    }
-    for (AbstractDaemonClient tcli : cluster.getTTClients()) {
-      i = 1;
-      while (i < 40) {
-        try {
-          tcli.ping();
-          break;
-        } catch (Exception e) {
-          i++;
-          Thread.sleep(1000);
-          LOG.info("Waiting for Tasktracker on host : "
-              + tcli.getHostName() + " to come up.");
-        }
-      }
-    }
-  }
-}
+        daemonConf.get(DUMMY_CONFIG_STRING) == null);   
+  }  
+ }
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestRetiredJobs.java b/src/test/system/java/org/apache/hadoop/mapred/TestRetiredJobs.java
new file mode 100644
index 0000000..a10fa23
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestRetiredJobs.java
@@ -0,0 +1,181 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.util.Collection;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.examples.SleepJob;
+
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+/**
+ * This tests when successful and failed jobs are retired,
+ * their jobInProgress object are removed properly. 
+ */
+
+public class TestRetiredJobs {
+
+  private static MRCluster cluster = null;
+  private static JobClient jobClient = null;
+  private static int retiredJobInterval = 0;
+  static final Log LOG = LogFactory.getLog(TestRetiredJobs.class);
+
+  public TestRetiredJobs() throws Exception {
+  }
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setUp();
+    jobClient = cluster.getJTClient().getClient();
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    cluster.tearDown();
+  }
+
+  @Test
+  /**
+   * This tests when successful and failed jobs are retired, 
+   * their jobInProgress object are removed properly.
+   * This is verified by checking whether getJobInfo
+   * method returns a JobInfo object when running,
+   * and whether getJobInfo method returns null
+   * after job is retired. 
+   * @param none
+   * @return void
+   */
+  public void testRetiredJobsSuccessful() throws Exception {
+    Configuration conf = new Configuration(cluster.getConf());
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    JTProtocol remoteJTClient = cluster.getJTClient().getProxy();
+    int testLoopCount = 0;
+
+    //First run a successful job and verify if JobInProgress
+    //object is removed by checking the getJobInfo. In the
+    //second iteration, verify if a killed job JobInProgress
+    //is removed.
+    do {
+      testLoopCount++;
+      SleepJob job = new SleepJob();
+      job.setConf(conf);
+      conf = job.setupJobConf(5, 1, 100, 100, 100, 100);
+      //Get the value of mapred.jobtracker.retirejob.check. If not
+      //found then use 60000 milliseconds, which is the application default.
+      retiredJobInterval =
+        conf.getInt("mapred.jobtracker.retirejob.check", 60000);
+      //Assert if retiredJobInterval is 0
+      if ( retiredJobInterval == 0 ) {
+        Assert.fail("mapred.jobtracker.retirejob.check is 0");
+      }
+
+      JobConf jconf = new JobConf(conf);
+      //Controls the job till all verification is done 
+      FinishTaskControlAction.configureControlActionForJob(conf);
+      //Submitting the job
+      RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+
+      JobID jobID = rJob.getID();
+
+      JobInfo jInfo = remoteJTClient.getJobInfo(jobID);
+      LOG.info("jInfo is :" + jInfo);
+
+      boolean jobStarted = cluster.getJTClient().isJobStarted(jobID);
+
+      Assert.assertTrue("Job has not started even after a minute", 
+          jobStarted );
+
+      LOG.info("job id is :" + jobID.toString());
+
+      TaskInfo[] taskInfos = cluster.getJTClient().getProxy()
+          .getTaskInfo(jobID);
+
+      // getJobInfo method should
+      // return a JobInProgress object when running,
+      JobInfo jobInfo = cluster.getJTClient().getProxy()
+          .getJobInfo(jobID);
+
+      Assert.assertNotNull("The Job information is not present ", jobInfo); 
+
+      //Allow the job to continue through MR control job.
+      for (TaskInfo taskInfoRemaining : taskInfos) {
+        FinishTaskControlAction action = new FinishTaskControlAction(TaskID
+            .downgrade(taskInfoRemaining.getTaskID()));
+        Collection<TTClient> tts = cluster.getTTClients();
+        for (TTClient cli : tts) {
+          cli.getProxy().sendAction(action);
+        }
+      }
+
+      //Killing this job will happen only in the second iteration.
+      if (testLoopCount == 2) {
+        //Killing the job because all the verification needed
+        //for this testcase is completed.
+        rJob.killJob();
+      }
+
+      //Making sure that the job is complete.
+      int count = 0;
+      while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
+        UtilsForTests.waitFor(10000);
+        count++;
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+        //If the count goes more than 100 seconds, then fail; This is to
+        //avoid infinite loop
+        if (count > 10) {
+          Assert.fail("Since the job has not completed even after" +
+              " 100 seconds, failing at this point");
+        }
+      }
+
+      //Waiting for a specific period of time for retire thread
+      //to be called taking into consideration the network issues
+      if (retiredJobInterval > 40000) {
+        UtilsForTests.waitFor(retiredJobInterval * 2);
+      } else {
+        UtilsForTests.waitFor(retiredJobInterval * 4);
+      }
+
+      jobInfo = null; 
+      // getJobInfo method should return null
+      // after job is retired. JobInProgress
+      // object should not be present.
+      jobInfo = cluster.getJTClient().getProxy()
+          .getJobInfo(jobID);
+
+      Assert.assertNull("Job information is still available " + 
+          "after retirement of job ", jobInfo);
+
+    } while (testLoopCount < 2);
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestTaskChildsKilling.java b/src/test/system/java/org/apache/hadoop/mapred/TestTaskChildsKilling.java
new file mode 100644
index 0000000..c6d7fbb
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestTaskChildsKilling.java
@@ -0,0 +1,551 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.TTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTTaskInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
+import org.apache.hadoop.mapred.JobClient.NetworkedJob;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.util.Tool;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Assert;
+import org.junit.Test;
+import java.io.IOException;
+import java.io.DataOutputStream;
+import java.util.Collection;
+import testjar.GenerateTaskChildProcess;
+import java.util.Hashtable;
+
+/**
+ * Submit a job which would spawn child processes and 
+ * verify whether the task child processes are cleaned up 
+ * or not after either job killed or task killed or task failed.
+ */
+public class TestTaskChildsKilling {
+  private static final Log LOG = LogFactory
+      .getLog(TestTaskChildsKilling.class);
+  private static MRCluster cluster;
+  private static Path inputDir = new Path("input");
+  private static Path outputDir = new Path("output");
+  private static Configuration conf = new Configuration();
+  private static String confFile = "mapred-site.xml";
+
+  @BeforeClass
+  public static void before() throws Exception {
+    Hashtable<String,Object> prop = new Hashtable<String,Object>();
+    prop.put("mapred.map.max.attempts", 1L);
+    prop.put("mapreduce.job.complete.cancel.delegation.tokens",false);
+    String [] expExcludeList = {"java.net.ConnectException",
+        "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    cluster.restartClusterWithNewConfig(prop, confFile);
+    UtilsForTests.waitFor(1000);
+    conf = cluster.getJTClient().getProxy().getDaemonConf();
+    createInput(inputDir, conf);
+  }
+
+  @AfterClass
+  public static void after() throws Exception {
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+    cluster.tearDown();
+    cluster.restart();
+    UtilsForTests.waitFor(1000);
+  }
+
+  /**
+   * Verifying the process tree cleanup of a task after task is killed
+   * by using -kill-task option.
+   */
+  @Test
+  public void testProcessTreeCleanupOfKilledTask1() throws 
+      Exception {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrDisplayMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    jobConf.setMaxMapAttempts(1);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+    
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null",jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+
+
+    Assert.assertTrue("Map process tree is not alive before task kills.", 
+        ttIns.isProcessTreeAlive(pid));
+
+    String args[] = new String[] { "-kill-task", tAttID.toString() };
+    int exitCode = runTool(jobConf, client, args);
+    Assert.assertEquals("Exit Code:", 0, exitCode);
+    
+    LOG.info("Waiting till the task is killed...");
+    counter = 0;
+    while (counter < 30) {
+      if (taskInfo.getTaskStatus().length > 0) {
+        if (taskInfo.getTaskStatus()[0].getRunState() == 
+            TaskStatus.State.KILLED) {
+          break;
+        }
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      counter ++;
+    }
+
+    runJob.killJob();
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().isJobComplete()) {
+        break;
+      } 
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(id);
+      counter ++;
+    }
+    Assert.assertTrue("Job has not  been completed for 1 min.", 
+        counter != 60 );
+    ttIns = ttClientIns.getProxy();
+    UtilsForTests.waitFor(1000);
+    Assert.assertTrue("Map process is still alive after task has been killed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  /**
+   * Verifying the process tree cleanup of a particular task 
+   * after task is killed.
+   */
+  @Test
+  public void testProcessTreeCleanupOfKilledTask2() throws 
+      IOException {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TaskAttemptID taskAttID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrDisplayMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+ 
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);    
+    Assert.assertNotNull("Job information is null", jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    taskAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+
+
+    Assert.assertTrue("Map process is not alive before task kills.", 
+        ttIns.isProcessTreeAlive(pid));
+
+    NetworkedJob networkJob = client.new NetworkedJob(jInfo.getStatus());
+    networkJob.killTask(taskAttID, false);
+
+    LOG.info("Waiting till the task is killed...");
+    taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+    counter = 0;
+    while (counter < 30) {
+      if (taskInfo.getTaskStatus()[0].getRunState() == 
+              TaskStatus.State.KILLED) {
+        break;
+      } 
+      UtilsForTests.waitFor(1000);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      counter ++;
+    }
+    runJob.killJob();
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(id);
+      counter ++;
+    }
+    Assert.assertTrue("Job has not been completed for 1 min.", 
+        counter != 60);
+    UtilsForTests.waitFor(2000);
+    ttIns = ttClientIns.getProxy();    
+    Assert.assertTrue("Map process is still alive after task has been killed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  /**
+   * Verifying the child process tree clean up of a task which fails due
+   * to an exception. 
+   */
+  @Test
+  public void testProcessTreeCleanupOfFailedTask1() throws IOException {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.FailedMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null", jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());    
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+    
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+
+    Assert.assertTrue("Map process is not alive before task fails.", 
+            ttIns.isProcessTreeAlive(pid));
+
+    LOG.info("Waiting till the task is failed...");
+    counter = 0;
+    while (counter < 60) {
+      if (taskInfo.getTaskStatus().length > 0) {
+        if (taskInfo.getTaskStatus()[0].getRunState() == 
+            TaskStatus.State.FAILED) {
+          break;
+        }
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      counter++;
+    }
+
+    LOG.info("Waiting till the job is completed...");
+    counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(id);
+      counter ++;
+    }
+    Assert.assertTrue("Job has not been completed for 1 min.", 
+        counter != 60);
+    ttIns = ttClientIns.getProxy();
+    UtilsForTests.waitFor(2000);
+    Assert.assertTrue("Map process is still alive after task has been failed.", 
+            !ttIns.isProcessTreeAlive(pid));
+  }
+
+  /**
+   * Verifying the process tree cleanup of a task after task is failed
+   * by using -fail-task option.
+   */
+  @Test
+  public void testProcessTreeCleanupOfFailedTask2() throws 
+      Exception {
+    TaskInfo taskInfo = null;
+    TaskID tID = null;
+    TTTaskInfo [] ttTaskinfo = null;
+    String pid = null;
+    TTProtocol ttIns = null;
+    TTClient ttClientIns = null;
+    int counter = 0;
+    
+    JobConf jobConf = new JobConf(conf);
+    jobConf.setJobName("Message Display");
+    jobConf.setJarByClass(GenerateTaskChildProcess.class);
+    jobConf.setMapperClass(GenerateTaskChildProcess.StrDisplayMapper.class);
+    jobConf.setNumMapTasks(1);
+    jobConf.setNumReduceTasks(0);
+    cleanup(outputDir, conf);
+    FileInputFormat.setInputPaths(jobConf, inputDir);
+    FileOutputFormat.setOutputPath(jobConf, outputDir);
+
+    JTClient jtClient = cluster.getJTClient();
+    JobClient client = jtClient.getClient();
+    JTProtocol wovenClient = cluster.getJTClient().getProxy();
+    RunningJob runJob = client.submitJob(jobConf);
+    JobID id = runJob.getID();
+    JobInfo jInfo = wovenClient.getJobInfo(id);
+    Assert.assertNotNull("Job information is null", jInfo);
+
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(id));
+
+    TaskInfo[] taskInfos = wovenClient.getTaskInfo(id);
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.",
+        jtClient.isTaskStarted(taskInfo));
+
+    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID tAttID = new TaskAttemptID(tID,0);
+    FinishTaskControlAction action = new FinishTaskControlAction(tID);
+
+    Collection<TTClient> ttClients = cluster.getTTClients();
+    for (TTClient ttClient : ttClients) {
+      TTProtocol tt = ttClient.getProxy();
+      tt.sendAction(action);
+      ttTaskinfo = tt.getTasks();
+      for (TTTaskInfo tttInfo : ttTaskinfo) {
+        if (!tttInfo.isTaskCleanupTask()) {
+          pid = tttInfo.getPid();
+          ttClientIns = ttClient;
+          ttIns = tt;
+          break;
+        }
+      }
+      if (ttClientIns != null) {
+        break;
+      }
+    }
+
+
+    Assert.assertTrue("Map process is not alive before task fails.", 
+        ttIns.isProcessTreeAlive(pid));
+
+    String args[] = new String[] { "-fail-task", tAttID.toString() };
+    int exitCode = runTool(jobConf, client, args);
+    Assert.assertEquals("Exit Code:", 0, exitCode);
+
+    LOG.info("Waiting till the task is failed...");
+    taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+    counter = 0;
+    while (counter < 60) {
+      if (taskInfo.getTaskStatus().length > 0) {
+        if (taskInfo.getTaskStatus()[0].getRunState() ==
+            TaskStatus.State.FAILED) {
+          break;
+        } 
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      counter ++;
+    }
+    counter = 0;
+    LOG.info("Waiting till the job is completed...");
+    while (counter < 60) {
+      if (jInfo.getStatus().isJobComplete()) {
+        break;
+      }
+      UtilsForTests.waitFor(1000);
+      jInfo = wovenClient.getJobInfo(id);
+      counter ++;
+    }
+
+    Assert.assertTrue("Job has not been completed for 1 min",
+        counter != 60);
+    ttIns = ttClientIns.getProxy();
+    UtilsForTests.waitFor(1000);
+    Assert.assertTrue("Map process is still alive after task has been failed.", 
+        !ttIns.isProcessTreeAlive(pid));
+  }
+
+  private int runTool(Configuration job, Tool tool, 
+      String[] jobArgs) throws Exception {
+    int returnStatus = ToolRunner.run(job, tool, jobArgs);
+    return returnStatus;
+  }
+
+  private static void cleanup(Path dir, Configuration conf) throws 
+      IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+  private static void createInput(Path inDir, Configuration conf) throws 
+      IOException {
+    String input = "Hadoop is framework for data intensive distributed " 
+        + "applications.\n Hadoop enables applications" 
+        + " to work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:" 
+          + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL, 
+        FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while(i < 10) {
+      file.writeBytes(input);
+      i++;
+    }
+    file.close();
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestTaskController.java b/src/test/system/java/org/apache/hadoop/mapred/TestTaskController.java
new file mode 100644
index 0000000..d608876
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestTaskController.java
@@ -0,0 +1,84 @@
+package org.apache.hadoop.mapred;
+
+import org.apache.commons.logging.Log;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.examples.SleepJob;
+import org.junit.Before;
+import org.junit.After;
+import org.junit.Test;
+import org.junit.Assert;
+
+/**
+ * Set the invalid configuration to task controller and verify the
+ * job status.
+ */
+public class TestTaskController {
+  private static final Log LOG = LogFactory.getLog(TestTaskController.class);
+  private static Configuration conf = new Configuration();
+  private static MRCluster cluster;
+  private static JTProtocol remoteJTClient;
+  private static JTClient jtClient;
+  
+  @Before
+  public void before() throws Exception {
+    String [] expExcludeList = {"java.net.ConnectException",
+                                "java.io.IOException"};
+    cluster = MRCluster.createCluster(conf);
+    cluster.setExcludeExpList(expExcludeList);
+    cluster.setUp();
+    jtClient = cluster.getJTClient();
+    remoteJTClient = jtClient.getProxy();
+  }
+
+  @After
+  public void after() throws Exception {
+    cluster.tearDown();
+  }
+  
+  /**
+   * Set the invalid mapred local directory location and run the job.
+   * Verify the job status. 
+   * @throws Exception - if an error occurs.
+   */
+  @Test
+  public void testJobStatusForInvalidTaskControllerConf() 
+      throws Exception {
+    conf = remoteJTClient.getDaemonConf();
+    if (conf.get("mapred.task.tracker.task-controller").
+            equals("org.apache.hadoop.mapred.LinuxTaskController")) {
+      StringBuffer mapredLocalDir = new StringBuffer();
+      LOG.info("JobConf.MAPRED_LOCAL_DIR_PROPERTY:" + conf.get(JobConf.MAPRED_LOCAL_DIR_PROPERTY));
+      mapredLocalDir.append(conf.get(JobConf.MAPRED_LOCAL_DIR_PROPERTY));
+      mapredLocalDir.append(",");
+      mapredLocalDir.append("/mapred/local");
+      String jobArgs []= {"-D","mapred.local.dir=" + mapredLocalDir.toString(),
+                         "-m", "1", 
+                         "-r", "1", 
+                         "-mt", "1000", 
+                         "-rt", "1000",
+                         "-recordt","100"};
+      SleepJob job = new SleepJob();
+      JobConf jobConf = new JobConf(conf); 
+      int exitStatus = ToolRunner.run(jobConf, job, jobArgs);
+      Assert.assertEquals("Exit Code:", 0, exitStatus);
+      UtilsForTests.waitFor(100);
+      JobClient jobClient = jtClient.getClient();
+      JobID jobId =jobClient.getAllJobs()[0].getJobID();
+      LOG.info("JobId:" + jobId);
+      if (jobId != null) {
+        JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+        Assert.assertEquals("Job has not been succeeded", 
+            jInfo.getStatus().getRunState(), JobStatus.SUCCEEDED);
+       }
+    } else {
+       Assert.assertTrue("Linux Task controller not found.", false);
+    }
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestTaskKilling.java b/src/test/system/java/org/apache/hadoop/mapred/TestTaskKilling.java
index 6614514..f61c098 100644
--- a/src/test/system/java/org/apache/hadoop/mapred/TestTaskKilling.java
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestTaskKilling.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.mapreduce.test.system.JTProtocol;
 import org.apache.hadoop.mapreduce.test.system.JobInfo;
 import org.apache.hadoop.mapreduce.test.system.TaskInfo;
 import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JTClient;
 import org.apache.hadoop.mapreduce.test.system.FinishTaskControlAction;
 import org.apache.hadoop.mapred.JobClient.NetworkedJob;
 import org.apache.hadoop.io.NullWritable;
@@ -43,6 +44,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.mapred.UtilsForTests;
 
 /**
  * A System test for verifying the status after killing the 
@@ -52,18 +54,17 @@ public class TestTaskKilling {
   private static final Log LOG = LogFactory.getLog(TestTaskKilling.class);
   private static MRCluster cluster;
   private static JobClient jobClient = null;
+  private static JTClient jtClient = null;
   private static JTProtocol remoteJTClient = null;
-
-  public TestTaskKilling() {
-  }
+  private static Configuration conf = new Configuration();
 
   @BeforeClass
-  public static void before() throws Exception {
-    Configuration conf = new Configuration();
+  public static void before() throws Exception {    
     cluster = MRCluster.createCluster(conf);
     cluster.setUp();
-    jobClient = cluster.getJTClient().getClient();
-    remoteJTClient = cluster.getJTClient().getProxy();
+    jtClient = cluster.getJTClient();
+    jobClient = jtClient.getClient();
+    remoteJTClient = jtClient.getProxy();
   }
 
   @AfterClass
@@ -78,64 +79,39 @@ public class TestTaskKilling {
   @Test
   public void testFailedTaskJobStatus() throws IOException, 
           InterruptedException {
-    Configuration conf = new Configuration(cluster.getConf());
+    conf = remoteJTClient.getDaemonConf();
     TaskInfo taskInfo = null;
     SleepJob job = new SleepJob();
     job.setConf(conf);
-    conf = job.setupJobConf(3, 1, 4000, 4000, 100, 100);
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setMaxMapAttempts(20);
-    jobConf.setMaxReduceAttempts(20);
+    JobConf jobConf = job.setupJobConf(1, 1, 10000, 4000, 100, 100);
     RunningJob runJob = jobClient.submitJob(jobConf);
-    JobID id = runJob.getID();
-    JobInfo jInfo = remoteJTClient.getJobInfo(id);
-    int counter = 0;
-    while (counter < 60) {
-      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
-        break;
-      } else {
-        UtilsForTests.waitFor(1000);
-        jInfo = remoteJTClient.getJobInfo(id);
-      }
-      counter ++;
-    }
-    Assert.assertTrue("Job has not been started for 1 min.", counter != 60);
-
-    TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(id);
+    JobID jobId = runJob.getID();
+    JobInfo jInfo = remoteJTClient.getJobInfo(jobId);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        jtClient.isJobStarted(jobId));
+    TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(jobId);
     for (TaskInfo taskinfo : taskInfos) {
-      if (!taskinfo.isSetupOrCleanup()) {
+      if (!taskinfo.isSetupOrCleanup() && taskinfo.getTaskID().isMap()) {
         taskInfo = taskinfo;
+        break;
       }
     }
+    Assert.assertTrue("Task has not been started for 1 min.", 
+        jtClient.isTaskStarted(taskInfo));
 
-    counter = 0;
-    taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
-    while (counter < 60) {
-      if (taskInfo.getTaskStatus().length > 0) {
-        if (taskInfo.getTaskStatus()[0].getRunState() 
-                == TaskStatus.State.RUNNING) {
-          break;
-        }
-      }
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
-      counter++;
-    }
-    Assert.assertTrue("Task has not been started for 1 min.", counter != 60);
-
+    // Fail the running task.
     NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());
     TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
     TaskAttemptID taskAttID = new TaskAttemptID(tID , 0);
-    networkJob.killTask(taskAttID, false);
+    networkJob.killTask(taskAttID, true);
 
     LOG.info("Waiting till the job is completed...");
     while (!jInfo.getStatus().isJobComplete()) {
       UtilsForTests.waitFor(100);
-      jInfo = remoteJTClient.getJobInfo(id);
+      jInfo = remoteJTClient.getJobInfo(jobId);
     }
-
-    Assert.assertEquals("JobStatus", jInfo.getStatus().getRunState(), 
-            JobStatus.SUCCEEDED);
+    Assert.assertEquals("JobStatus", JobStatus.SUCCEEDED, 
+       jInfo.getStatus().getRunState());
   }
 
 
@@ -150,7 +126,6 @@ public class TestTaskKilling {
     boolean isTempFolderExists = false;
     String localTaskDir = null;
     TTClient ttClient = null;
-    TaskID tID = null;
     FileStatus filesStatus [] = null;
     Path inputDir = new Path("input");
     Path outputDir = new Path("output");
@@ -163,8 +138,6 @@ public class TestTaskKilling {
     jconf.setReducerClass(WordCount.Reduce.class);
     jconf.setNumMapTasks(1);
     jconf.setNumReduceTasks(1);
-    jconf.setMaxMapAttempts(20);
-    jconf.setMaxReduceAttempts(20);
     jconf.setOutputKeyClass(Text.class);
     jconf.setOutputValueClass(IntWritable.class);
 
@@ -176,61 +149,46 @@ public class TestTaskKilling {
     RunningJob runJob = jobClient.submitJob(jconf);
     JobID id = runJob.getID();
     JobInfo jInfo = remoteJTClient.getJobInfo(id);
-    int counter = 0;
-    while (counter < 60) {
-      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
-        break;
-      } else {
-        UtilsForTests.waitFor(1000);
-        jInfo = remoteJTClient.getJobInfo(id);
-      }
-      counter ++;
-    }
-    Assert.assertTrue("Job has not been started for 1 min.", counter != 60);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+       jtClient.isJobStarted(id));
 
     JobStatus[] jobStatus = jobClient.getAllJobs();
     String userName = jobStatus[0].getUsername();
     TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(id);
     for (TaskInfo taskinfo : taskInfos) {
-      if (!taskinfo.isSetupOrCleanup()) {
+      if (!taskinfo.isSetupOrCleanup() && taskinfo.getTaskID().isMap()) {
         taskInfo = taskinfo;
         break;
       }
     }
 
-    counter = 0;
-    while (counter < 30) {
-      if (taskInfo.getTaskStatus().length > 0) {
-        if (taskInfo.getTaskStatus()[0].getRunState() 
-                == TaskStatus.State.RUNNING) {
-          break;
-        }
-      }
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
-      counter ++;
-    }
-    Assert.assertTrue("Task has not been started for 30 sec.", 
-            counter != 30);
+    Assert.assertTrue("Task has not been started for 1 min.", 
+       jtClient.isTaskStarted(taskInfo));
 
-    tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
     FinishTaskControlAction action = new FinishTaskControlAction(tID);
 
     String[] taskTrackers = taskInfo.getTaskTrackers();
-    counter = 0;
-    while (counter < 30) {
-      if (taskTrackers.length != 0) {
+    int counter = 0;
+    TaskInfo prvTaskInfo = taskInfo;
+    while (counter++ < 30) {
+      if (taskTrackers.length > 0) {
         break;
+      } else {
+        UtilsForTests.waitFor(100);
+        taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+        if (taskInfo == null) {
+          taskInfo = prvTaskInfo;
+        } else {
+          prvTaskInfo = taskInfo;
+        }
+        taskTrackers = taskInfo.getTaskTrackers();
       }
-      UtilsForTests.waitFor(100);
-      taskTrackers = taskInfo.getTaskTrackers();
-      counter ++;
     }
-
+    Assert.assertTrue("TaskTracker is not found.", taskTrackers.length > 0);
     String hostName = taskTrackers[0].split("_")[1];
     hostName = hostName.split(":")[0];
-    ttClient = cluster.getTTClient(hostName);
-    ttClient.getProxy().sendAction(action);
+    ttClient = cluster.getTTClient(hostName);    
     String localDirs[] = ttClient.getMapredLocalDirs();
     TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);
     for (String localDir : localDirs) {
@@ -240,40 +198,49 @@ public class TestTaskKilling {
       filesStatus = ttClient.listStatus(localTaskDir, true);
       if (filesStatus.length > 0) {
         isTempFolderExists = true;
-        NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());
-        networkJob.killTask(taskAttID, false);
         break;
       }
     }
-
+    
     Assert.assertTrue("Task Attempt directory " + 
             taskAttID + " has not been found while task was running.", 
                     isTempFolderExists);
+    
+    NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());
+    networkJob.killTask(taskAttID, false);
+    ttClient.getProxy().sendAction(action);
+    taskInfo = remoteJTClient.getTaskInfo(tID);
+    while(taskInfo.getTaskStatus()[0].getRunState() == 
+       TaskStatus.State.RUNNING) {
+    UtilsForTests.waitFor(1000);
+    taskInfo = remoteJTClient.getTaskInfo(tID);
+    }
+    UtilsForTests.waitFor(1000);
+    taskInfo = remoteJTClient.getTaskInfo(tID);
+    Assert.assertTrue("Task status has not been changed to KILLED.", 
+       (TaskStatus.State.KILLED == 
+       taskInfo.getTaskStatus()[0].getRunState()
+       || TaskStatus.State.KILLED_UNCLEAN == 
+       taskInfo.getTaskStatus()[0].getRunState()));
     taskInfo = remoteJTClient.getTaskInfo(tID);
-
     counter = 0;
-    while (counter < 60) {
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(tID);
+    while (counter++ < 60) {
       filesStatus = ttClient.listStatus(localTaskDir, true);
       if (filesStatus.length == 0) {
         break;
+      } else {
+        UtilsForTests.waitFor(100);
       }
-      counter ++;
     }
-
     Assert.assertTrue("Task attempt temporary folder has not been cleaned.", 
             isTempFolderExists && filesStatus.length == 0);
-    counter = 0;
-    while (counter < 30) {
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(tID);
-      counter ++;
+    UtilsForTests.waitFor(1000);
+    jInfo = remoteJTClient.getJobInfo(id);
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = remoteJTClient.getJobInfo(id);
     }
-    taskInfo = remoteJTClient.getTaskInfo(tID);
-    Assert.assertEquals("Task status has not been changed to KILLED.", 
-            TaskStatus.State.KILLED, 
-                    taskInfo.getTaskStatus()[0].getRunState());
   }
 
   private void cleanup(Path dir, Configuration conf) throws 
@@ -316,81 +283,51 @@ public class TestTaskKilling {
     TaskInfo taskInfo = null;
     TaskID tID = null;
     boolean isTempFolderExists = false;
-    Path inputDir = new Path("input");
-    Path outputDir = new Path("output");
-    Configuration conf = new Configuration(cluster.getConf());
-    JobConf jconf = new JobConf(conf);
-    jconf.setJobName("Task Failed job");
-    jconf.setJarByClass(UtilsForTests.class);
-    jconf.setMapperClass(FailedMapperClass.class);
-    jconf.setNumMapTasks(1);
-    jconf.setNumReduceTasks(0);
-    jconf.setMaxMapAttempts(1);
-    cleanup(inputDir, conf);
-    cleanup(outputDir, conf);
-    createInput(inputDir, conf);
-    FileInputFormat.setInputPaths(jconf, inputDir);
-    FileOutputFormat.setOutputPath(jconf, outputDir);
-    RunningJob runJob = jobClient.submitJob(jconf);
+    conf = remoteJTClient.getDaemonConf();
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    JobConf jobConf = job.setupJobConf(1, 0, 10000,100, 10, 10);
+    RunningJob runJob = jobClient.submitJob(jobConf);
     JobID id = runJob.getID();
     JobInfo jInfo = remoteJTClient.getJobInfo(id);
-    
-    int counter = 0;
-    while (counter < 60) {
-      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
-        break;
-      } else {
-        UtilsForTests.waitFor(1000);
-        jInfo = remoteJTClient.getJobInfo(id);
-      }
-      counter ++;
-    }
-    Assert.assertTrue("Job has not been started for 1 min.", counter != 60);
+    Assert.assertTrue("Job has not been started for 1 min.", 
+       jtClient.isJobStarted(id));
 
     JobStatus[] jobStatus = jobClient.getAllJobs();
     String userName = jobStatus[0].getUsername();
     TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(id);
     for (TaskInfo taskinfo : taskInfos) {
-      if (!taskinfo.isSetupOrCleanup()) {
+      if (!taskinfo.isSetupOrCleanup() && taskinfo.getTaskID().isMap()) {
         taskInfo = taskinfo;
         break;
       }
     }
-
+    Assert.assertTrue("Task has not been started for 1 min.", 
+       jtClient.isTaskStarted(taskInfo));
+    
     tID = TaskID.downgrade(taskInfo.getTaskID());
     FinishTaskControlAction action = new FinishTaskControlAction(tID);
     String[] taskTrackers = taskInfo.getTaskTrackers();
-    counter = 0;
-    while (counter < 30) {
-      if (taskTrackers.length != 0) {
+    int counter = 0;
+    TaskInfo prvTaskInfo = taskInfo;
+    while (counter++ < 30) {
+      if (taskTrackers.length > 0) {
         break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+        if (taskInfo == null) {
+          taskInfo = prvTaskInfo;
+        } else {
+          prvTaskInfo = taskInfo;
+        }
+        taskTrackers = taskInfo.getTaskTrackers();
       }
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
-      taskTrackers = taskInfo.getTaskTrackers();
-      counter ++;
     }
-    Assert.assertTrue("Task tracker not found.", taskTrackers.length != 0);
+    Assert.assertTrue("Task tracker not found.", taskTrackers.length > 0);
     String hostName = taskTrackers[0].split("_")[1];
     hostName = hostName.split(":")[0];
     ttClient = cluster.getTTClient(hostName);
-    ttClient.getProxy().sendAction(action);
-
-    counter = 0;
-    while(counter < 60) {
-      if (taskInfo.getTaskStatus().length > 0) {
-        if (taskInfo.getTaskStatus()[0].getRunState() 
-                == TaskStatus.State.RUNNING) {
-          break;
-        }
-      }
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
-      counter ++;
-    }
-    Assert.assertTrue("Task has not been started for 1 min.", 
-            counter != 60);
-
     String localDirs[] = ttClient.getMapredLocalDirs();
     TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);
     for (String localDir : localDirs) {
@@ -402,48 +339,49 @@ public class TestTaskKilling {
         isTempFolderExists = true;
         break;
       }
-    }
-
-    taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+    }    
+    
     Assert.assertTrue("Task Attempt directory " + 
             taskAttID + " has not been found while task was running.", 
                     isTempFolderExists);
-    counter = 0;
-    while (counter < 30) {
-      UtilsForTests.waitFor(1000);
-      taskInfo = remoteJTClient.getTaskInfo(tID);
-      counter ++;
-    }
-
-    Assert.assertEquals("Task status has not been changed to FAILED.", 
-            taskInfo.getTaskStatus()[0].getRunState(), 
-                    TaskStatus.State.FAILED);
-
+    boolean isFailTask = false;
+    JobInfo jobInfo = remoteJTClient.getJobInfo(id);
+    int MAX_MAP_TASK_ATTEMPTS = Integer.parseInt(
+       jobConf.get("mapred.map.max.attempts"));
+    if (!isFailTask) {        
+        TaskID taskId = TaskID.downgrade(taskInfo.getTaskID());
+        TaskAttemptID tAttID = new TaskAttemptID(taskId, 
+            taskInfo.numFailedAttempts());
+        while(taskInfo.numFailedAttempts() < MAX_MAP_TASK_ATTEMPTS) {
+          NetworkedJob networkJob = jtClient.getClient().
+             new NetworkedJob(jobInfo.getStatus());
+          networkJob.killTask(taskAttID, true);
+          taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+          taskAttID = new TaskAttemptID(taskId, taskInfo.numFailedAttempts());
+        }
+        isFailTask=true;
+      }
+    
+    ttClient.getProxy().sendAction(action);
+    taskInfo = remoteJTClient.getTaskInfo(tID);
+    Assert.assertTrue("Task status has not been changed to FAILED.", 
+       TaskStatus.State.FAILED == 
+       taskInfo.getTaskStatus()[0].getRunState() 
+       || TaskStatus.State.FAILED_UNCLEAN ==
+       taskInfo.getTaskStatus()[0].getRunState());
+    UtilsForTests.waitFor(1000);
     filesStatus = ttClient.listStatus(localTaskDir, true);
     Assert.assertTrue("Temporary folder has not been cleanup.", 
             filesStatus.length == 0);
-  }
-
-  public static class FailedMapperClass implements 
-          Mapper<NullWritable, NullWritable, NullWritable, NullWritable> {
-    public void configure(JobConf job) {
-    }
-    public void map(NullWritable key, NullWritable value, 
-            OutputCollector<NullWritable, NullWritable> output, 
-                    Reporter reporter) throws IOException {
-      int counter = 0;
-      while (counter < 240) {
-        UtilsForTests.waitFor(1000);
-        counter ++;
-      }
-      if (counter == 240) {
-        throw new IOException();
-      }
-    }
-    public void close() {
+    UtilsForTests.waitFor(1000);
+    jInfo = remoteJTClient.getJobInfo(id);
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(100);
+      jInfo = remoteJTClient.getJobInfo(id);
     }
   }
-  
+
   @Test
   /**
    * This tests verification of job killing by killing of all task 
@@ -458,7 +396,7 @@ public class TestTaskKilling {
 
     SleepJob job = new SleepJob();
     job.setConf(conf);
-    conf = job.setupJobConf(3, 1, 40000, 1000, 100, 100);
+    conf = job.setupJobConf(2, 1, 40000, 1000, 100, 100);
     JobConf jconf = new JobConf(conf);
 
     //Submitting the job
@@ -553,6 +491,8 @@ public class TestTaskKilling {
           } else {
             if (taskIdKilled.equals(taskid.toString())) {
               taskAttemptID = new TaskAttemptID(taskid, i);
+              //Make sure that task is midway and then kill
+              UtilsForTests.waitFor(20000);
               LOG.info("taskAttemptid going to be killed is : " +
                   taskAttemptID);
               (jobClient.new NetworkedJob(jInfo.getStatus())).
@@ -599,6 +539,10 @@ public class TestTaskKilling {
       TaskCompletionEvent[] taskCompletionEvents =  jobClient.new
         NetworkedJob(jInfo.getStatus()).getTaskCompletionEvents(0);
       for (TaskCompletionEvent taskCompletionEvent : taskCompletionEvents) {
+        LOG.info("taskCompletionEvent.getTaskAttemptId().toString() is : " + 
+          taskCompletionEvent.getTaskAttemptId().toString());
+        LOG.info("compared to taskAttemptID.toString() :" + 
+          taskAttemptID.toString());
         if ((taskCompletionEvent.getTaskAttemptId().toString()).
             equals(taskAttemptID.toString())){
           match = true;
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoSuccessfulFailedJobs.java b/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoSuccessfulFailedJobs.java
new file mode 100644
index 0000000..4f8a6ec
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoSuccessfulFailedJobs.java
@@ -0,0 +1,631 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.UtilsForTests;
+import org.apache.hadoop.mapred.JobClient.NetworkedJob;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.fs.permission.FsAction;
+import org.apache.hadoop.fs.Path;
+import testjar.GenerateTaskChildProcess;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+/**
+ * Verify the Task Tracker Info functionality.
+ */
+
+public class TestTaskTrackerInfoSuccessfulFailedJobs {
+
+  private static MRCluster cluster = null;
+  private static JobClient client = null;
+  static final Log LOG = LogFactory.
+                          getLog(TestTaskTrackerInfoSuccessfulFailedJobs.class);
+  private static Configuration conf = null;
+  private static JTProtocol remoteJTClient = null;
+
+  StatisticsCollectionHandler statisticsCollectionHandler = null;
+  int taskTrackerHeartBeatInterval = 0;
+
+  public TestTaskTrackerInfoSuccessfulFailedJobs() throws Exception {
+  }
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setUp();
+    conf = new Configuration(cluster.getConf());
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    remoteJTClient = cluster.getJTClient().getProxy();
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    cluster.tearDown();
+  }
+
+  @Test
+  /**
+   * This tests Task tracker summary information for
+   * since start - total tasks, successful tasks
+   * Last Hour - total tasks, successful tasks
+   * Last Day - total tasks, successful tasks
+   * It is checked for multiple job submissions. 
+   * @param none
+   * @return void
+   */
+  public void testTaskTrackerInfoAll() throws Exception {
+    
+    //This boolean will decide whether to run job again
+    boolean continueLoop = true;
+
+    //counter for job Loop
+    int countLoop = 0;
+
+    String jobTrackerUserName = remoteJTClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
+    //This counter will check for count of a loop,
+    //which might become infinite.
+    int count = 0;
+
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    int totalMapTasks = 5;
+    int totalReduceTasks = 1;
+    conf = job.setupJobConf(totalMapTasks, totalReduceTasks, 
+      100, 100, 100, 100);
+    JobConf jconf = new JobConf(conf);
+
+    count = 0;
+    //The last hour and last day are given 60 seconds and 120 seconds
+    //recreate values rate, replacing one hour and 1 day. Waiting for 
+    //them to be ona  just created stage when testacse starts.
+    while (remoteJTClient.getInfoFromAllClients("last_day","total_tasks") 
+        != 0) {
+      count++;
+      UtilsForTests.waitFor(1000);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will
+      //anyway fail later.
+      if (count > 180) {
+        Assert.fail("Since this value has not reached 0" +
+          "in more than 180 seconds. Failing at this point");
+      }
+    }
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler = remoteJTClient.
+        getInfoFromAllClientsForAllTaskType();
+
+    int totalTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    //Submitting the job
+    RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+
+    JobInfo jInfo = remoteJTClient.getJobInfo(rJob.getID());
+    LOG.info("jInfo is :" + jInfo);
+
+    //Assert if jobInfo is null
+    Assert.assertNotNull("jobInfo is null", jInfo);
+
+    count = 0;
+    LOG.info("Waiting till the job is completed...");
+    while (jInfo != null && !jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(1000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will 
+      //anyway fail later.
+      if (count > 40) {
+        Assert.fail("job has not reached completed state for more" +
+          " than 400 seconds. Failing at this point");
+      }
+    }
+
+    //Waiting for 20 seconds to make sure that all the completed tasks
+    //are reflected in their corresponding Tasktracker boxes.
+    taskTrackerHeartBeatInterval =  remoteJTClient.
+        getTaskTrackerHeartbeatInterval();
+
+    //Waiting for 6 times the Task tracker heart beat interval to
+    //account for network slowness, job tracker processing time
+    //after receiving the tasktracker updates etc.
+    UtilsForTests.waitFor(taskTrackerHeartBeatInterval * 6);
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler =
+      remoteJTClient.getInfoFromAllClientsForAllTaskType();
+    int totalTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    int totalTasksForJob = (totalMapTasks + totalReduceTasks + 2); 
+
+    Assert.assertEquals("The number of total tasks, since start" +
+        " dont match", 
+        (totalTasksSinceStartBeforeJob + totalTasksForJob), 
+        totalTasksSinceStartAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksSinceStartBeforeJob + totalTasksForJob), 
+        succeededTasksSinceStartAfterJob);
+
+    Assert.assertEquals("The number of total tasks, last hour" +
+        " dont match", 
+        (totalTasksLastHourBeforeJob + totalTasksForJob), 
+        totalTasksLastHourAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "last hour dont match", 
+        (succeededTasksLastHourBeforeJob + totalTasksForJob), 
+        succeededTasksLastHourAfterJob);
+
+    Assert.assertEquals("The number of total tasks, last day" +
+        " dont match", 
+        (totalTasksLastDayBeforeJob + totalTasksForJob), 
+        totalTasksLastDayAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksLastDayBeforeJob + totalTasksForJob), 
+        succeededTasksLastDayAfterJob);
+  }
+
+  @Test
+  /**
+   * This tests Task tracker task killed 
+   * summary information for
+   * since start - total tasks, successful tasks
+   * Last Hour - total tasks, successful tasks
+   * Last Day - total tasks, successful tasks
+   * It is checked for multiple job submissions. 
+   * @param none
+   * @return void
+   */
+  public void testTaskTrackerInfoKilled() throws Exception {
+    
+    //This boolean will decide whether to run job again
+    boolean continueLoop = true;
+
+    //counter for job Loop
+    int countLoop = 0;
+
+    TaskInfo taskInfo = null;
+
+    String jobTrackerUserName = remoteJTClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
+    //This counter will check for count of a loop,
+    //which might become infinite.
+    int count = 0;
+
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    int totalMapTasks = 5;
+    int totalReduceTasks = 1;
+    conf = job.setupJobConf(totalMapTasks, totalReduceTasks, 
+      100, 100, 100, 100);
+    JobConf jconf = new JobConf(conf);
+
+    count = 0;
+    //The last hour and last day are given 60 seconds and 120 seconds
+    //recreate values rate, replacing one hour and 1 day. Waiting for 
+    //them to be ona  just created stage when testacse starts.
+    while (remoteJTClient.getInfoFromAllClients("last_day","total_tasks") 
+        != 0) {
+      count++;
+      UtilsForTests.waitFor(1000);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will
+      //anyway fail later.
+      if (count > 140) {
+        Assert.fail("Since this value has not reached 0" +
+          "in more than 140 seconds. Failing at this point");
+      }
+    }
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler = remoteJTClient.
+        getInfoFromAllClientsForAllTaskType();
+
+    int totalTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+ 
+    //Submitting the job
+    RunningJob rJob = cluster.getJTClient().getClient().
+        submitJob(jconf);
+
+    JobInfo jInfo = remoteJTClient.getJobInfo(rJob.getID());
+    LOG.info("jInfo is :" + jInfo);
+
+    count = 0;
+    while (count < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      }
+      count++;
+    }
+    Assert.assertTrue("Job has not been started for 1 min.", 
+        count != 60);
+
+    //Assert if jobInfo is null
+    Assert.assertNotNull("jobInfo is null", jInfo);
+
+    TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(rJob.getID());
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+      }
+    }
+
+    count = 0;
+    taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+    while (count < 60) {
+      if (taskInfo.getTaskStatus().length > 0) {
+        if (taskInfo.getTaskStatus()[0].getRunState()
+              == TaskStatus.State.RUNNING) {
+          break;
+        }
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+      count++;
+    }
+
+    Assert.assertTrue("Task has not been started for 1 min.", 
+      count != 60);
+
+    NetworkedJob networkJob = (cluster.getJTClient().getClient()).new 
+      NetworkedJob(jInfo.getStatus());
+    TaskID tID = TaskID.downgrade(taskInfo.getTaskID());
+    TaskAttemptID taskAttID = new TaskAttemptID(tID , 0);
+    networkJob.killTask(taskAttID, false);
+
+    count = 0;
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(1000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will 
+      //anyway fail later.
+      if (count > 40) {
+        Assert.fail("job has not reached completed state for more" +
+          " than 400 seconds. Failing at this point");
+      }
+    }
+   
+    //Waiting for 20 seconds to make sure that all the completed tasks
+    //are reflected in their corresponding Tasktracker boxes.
+    taskTrackerHeartBeatInterval =  remoteJTClient.
+        getTaskTrackerHeartbeatInterval();
+
+    //Waiting for 6 times the Task tracker heart beat interval to
+    //account for network slowness, job tracker processing time
+    //after receiving the tasktracker updates etc.
+    UtilsForTests.waitFor(taskTrackerHeartBeatInterval * 6);
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler =
+      remoteJTClient.getInfoFromAllClientsForAllTaskType();
+    int totalTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    //Total tasks expected is setup, Cleanup + totalMapTasks
+    //+ totalReduceTasks
+    int totalTasksForJob = (totalMapTasks + totalReduceTasks + 2); 
+
+    //The total tasks will be equal to the totalTasksSinceStartBeforeJob
+    // + totalTasksFor present Job + 1 more task which was killed.
+    //This kiled task will be re-attempted by the job tracker and would have
+    //rerun in another tasktracker and would have completed successfully,
+    //which is captured in totalTasksForJob
+    Assert.assertEquals("The number of total tasks, since start" +
+         " dont match", 
+        (totalTasksSinceStartBeforeJob + totalTasksForJob + 1), 
+        totalTasksSinceStartAfterJob );
+    Assert.assertEquals("The number of succeeded tasks, " +
+         "since start dont match", 
+        (succeededTasksSinceStartBeforeJob + totalTasksForJob), 
+             succeededTasksSinceStartAfterJob); 
+
+    Assert.assertEquals("The number of total tasks, last hour" +
+        " dont match", 
+        (totalTasksLastHourBeforeJob + totalTasksForJob + 1), 
+        totalTasksLastHourAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "last hour dont match", 
+        (succeededTasksLastHourBeforeJob + totalTasksForJob),  
+        succeededTasksLastHourAfterJob); 
+
+    Assert.assertEquals("The number of total tasks, last day" +
+        " dont match", 
+        (totalTasksLastDayBeforeJob + totalTasksForJob + 1), 
+        totalTasksLastDayAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksLastDayBeforeJob + totalTasksForJob), 
+        succeededTasksLastDayAfterJob); 
+
+  }
+
+  @Test
+  /**
+   * This tests Task tracker task failure 
+   * summary information for
+   * since start - total tasks, successful tasks
+   * Last Hour - total tasks, successful tasks
+   * Last Day - total tasks, successful tasks
+   * @param none
+   * @return void
+   */
+  public void testTaskTrackerInfoTaskFailure() throws Exception {
+    
+    //This boolean will decide whether to run job again
+    boolean continueLoop = true;
+
+    //counter for job Loop
+    int countLoop = 0;
+
+    TaskInfo taskInfo = null;
+
+    String jobTrackerUserName = remoteJTClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
+    //This counter will check for count of a loop,
+    //which might become infinite.
+    int count = 0;
+
+    Configuration conf = new Configuration(cluster.getConf());
+    conf.setBoolean("mapreduce.map.output.compress", false); 
+    conf.set("mapred.map.output.compression.codec", 
+        "org.apache.hadoop.io.compress.DefaultCodec");
+    JobConf jconf = new JobConf(conf);
+    Path inputDir = new Path("input");
+    Path outputDir = new Path("output");
+    cleanup(inputDir, conf);
+    cleanup(outputDir, conf);
+
+    createInput(inputDir, conf);
+    jconf.setJobName("Task Failed job");
+    jconf.setJarByClass(UtilsForTests.class);
+    jconf.setMapperClass(GenerateTaskChildProcess.FailedMapper.class);
+    jconf.setNumMapTasks(1);
+    jconf.setNumReduceTasks(0);
+    jconf.setMaxMapAttempts(1);
+    FileInputFormat.setInputPaths(jconf, inputDir);
+    FileOutputFormat.setOutputPath(jconf, outputDir);
+
+    count = 0;
+    //The last hour and last day are given 60 seconds and 120 seconds
+    //recreate values rate, replacing one hour and 1 day. Waiting for 
+    //them to be ona  just created stage when testacse starts.
+    while (remoteJTClient.getInfoFromAllClients("last_day","total_tasks") 
+        != 0) {
+      count++;
+      UtilsForTests.waitFor(1000);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will
+      //anyway fail later.
+      if (count > 140) {
+        Assert.fail("Since this value has not reached 0" +
+          "in more than 140 seconds. Failing at this point");
+      }
+    }
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler = remoteJTClient.
+        getInfoFromAllClientsForAllTaskType();
+
+    int totalTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    RunningJob rJob = cluster.getJTClient().getClient().submitJob(jconf);
+    JobID id = rJob.getID();
+    JobInfo jInfo = remoteJTClient.getJobInfo(id);
+
+    LOG.info("jInfo is :" + jInfo);
+
+    count = 0;
+    while (count < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      }
+      count++;
+    }
+    Assert.assertTrue("Job has not been started for 1 min.", 
+      count != 60);
+
+    //Assert if jobInfo is null
+    Assert.assertNotNull("jobInfo is null", jInfo);
+
+    count = 0;
+    LOG.info("Waiting till the job is completed...");
+    while ( jInfo != null && (!jInfo.getStatus().isJobComplete())) {
+      UtilsForTests.waitFor(1000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(id);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will 
+      //anyway fail later.
+      if (count > 40) {
+        Assert.fail("job has not reached completed state for more" +
+          " than 400 seconds. Failing at this point");
+      }
+    }
+
+    //Waiting for 20 seconds to make sure that all the completed tasks
+    //are reflected in their corresponding Tasktracker boxes.
+    taskTrackerHeartBeatInterval =  remoteJTClient.
+        getTaskTrackerHeartbeatInterval();
+
+    //Waiting for 6 times the Task tracker heart beat interval to
+    //account for network slowness, job tracker processing time
+    //after receiving the tasktracker updates etc.
+    UtilsForTests.waitFor(taskTrackerHeartBeatInterval * 6);
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler =
+      remoteJTClient.getInfoFromAllClientsForAllTaskType();
+    int totalTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    //1 map running 4 times before failure, plus sometimes two failures 
+    //which are not captured in Job summary, but caught in 
+    //tasktracker summary. 
+    //0 reduces, setup and cleanup
+    int totalTasksForJob = 4; 
+
+    Assert.assertTrue("The number of total tasks, since start" +
+         " dont match", (totalTasksSinceStartAfterJob >= 
+         totalTasksSinceStartBeforeJob + totalTasksForJob)); 
+        
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksSinceStartAfterJob  >= 
+        succeededTasksSinceStartBeforeJob));
+      
+    Assert.assertTrue("The number of total tasks, last hour" +
+        " dont match", (totalTasksLastHourAfterJob >= 
+         totalTasksLastHourBeforeJob + totalTasksForJob));
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "last hour dont match", (succeededTasksLastHourAfterJob >= 
+        succeededTasksLastHourBeforeJob));  
+    
+    Assert.assertTrue("The number of total tasks, last day" +
+        " dont match", totalTasksLastDayAfterJob >= 
+        totalTasksLastDayBeforeJob + totalTasksForJob); 
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "since start dont match", succeededTasksLastDayAfterJob >= 
+        succeededTasksLastDayBeforeJob);
+  }
+
+  //This creates the input directories in the dfs
+  private void createInput(Path inDir, Configuration conf) throws
+          IOException {
+    String input = "Hadoop is framework for data intensive distributed "
+            + "applications.\n"
+            + "Hadoop enables applications to work with thousands of nodes.";
+    FileSystem fs = inDir.getFileSystem(conf);
+    if (!fs.mkdirs(inDir)) {
+      throw new IOException("Failed to create the input directory:"
+            + inDir.toString());
+    }
+    fs.setPermission(inDir, new FsPermission(FsAction.ALL,
+            FsAction.ALL, FsAction.ALL));
+    DataOutputStream file = fs.create(new Path(inDir, "data.txt"));
+    int i = 0;
+    while(i < 1000 * 3000) {
+      file.writeBytes(input);
+      i++;
+    }
+    file.close();
+  }
+
+  //This cleans up the specified directories in the dfs
+  private void cleanup(Path dir, Configuration conf) throws
+          IOException {
+    FileSystem fs = dir.getFileSystem(conf);
+    fs.delete(dir, true);
+  }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoTTProcess.java b/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoTTProcess.java
new file mode 100644
index 0000000..4617d1c
--- /dev/null
+++ b/src/test/system/java/org/apache/hadoop/mapred/TestTaskTrackerInfoTTProcess.java
@@ -0,0 +1,401 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.IOException;
+import java.util.List;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapreduce.test.system.JTProtocol;
+import org.apache.hadoop.mapreduce.test.system.TTClient;
+import org.apache.hadoop.mapreduce.test.system.JobInfo;
+import org.apache.hadoop.mapreduce.test.system.TaskInfo;
+import org.apache.hadoop.mapreduce.test.system.MRCluster;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.UtilsForTests;
+import org.apache.hadoop.examples.SleepJob;
+import org.apache.hadoop.mapred.StatisticsCollectionHandler;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.AfterClass;
+import org.junit.Test;
+
+/**
+ * Verify the Task Tracker Info functionality.
+ */
+
+public class TestTaskTrackerInfoTTProcess {
+
+  private static MRCluster cluster = null;
+  private static JobClient client = null;
+  static final Log LOG = LogFactory.
+                           getLog(TestTaskTrackerInfoTTProcess.class);
+  private static Configuration conf = null;
+  private static JTProtocol remoteJTClient = null;
+  private static String confFile = "mapred-site.xml";
+  int taskTrackerHeartBeatInterval;
+  StatisticsCollectionHandler statisticsCollectionHandler = null;
+
+  public TestTaskTrackerInfoTTProcess() throws Exception {
+  }
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    cluster = MRCluster.createCluster(new Configuration());
+    cluster.setUp();
+    conf = new Configuration(cluster.getConf());
+    conf.setBoolean("mapreduce.job.complete.cancel.delegation.tokens", false);
+    remoteJTClient = cluster.getJTClient().getProxy();
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    cluster.tearDown();
+  }
+
+  @Test
+  /**
+   * This tests Task tracker info when a job with 0 maps and 0 reduces run 
+   * summary information for
+   * since start - total tasks, successful tasks
+   * Last Hour - total tasks, successful tasks
+   * Last Day - total tasks, successful tasks
+   * It is checked for multiple job submissions. 
+   * @param none
+   * @return void
+   */
+  public void testTaskTrackerInfoZeroMapsZeroReduces() throws Exception {
+    
+    TaskInfo taskInfo = null;
+
+    String jobTrackerUserName = remoteJTClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
+    int count = 0;
+
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    int totalMapTasks = 0;
+    int totalReduceTasks = 0;
+    conf = job.setupJobConf(totalMapTasks, totalReduceTasks, 
+        100, 100, 100, 100);
+    JobConf jconf = new JobConf(conf);
+
+    count = 0;
+    //The last hour and last day are given 60 seconds and 120 seconds
+    //recreate values rate, replacing one hour and 1 day. Waiting for 
+    //them to be ona  just created stage when testacse starts.
+    while (remoteJTClient.getInfoFromAllClients("last_day","total_tasks") 
+        != 0) {
+      count++;
+      UtilsForTests.waitFor(1000);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will
+      //anyway fail later.
+      if (count > 140) {
+        Assert.fail("Since this value has not reached 0" +
+          "in more than 140 seconds. Failing at this point");
+      }
+    }
+
+    statisticsCollectionHandler = remoteJTClient.
+        getInfoFromAllClientsForAllTaskType();
+
+    int totalTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+ 
+    //Submitting the job
+    RunningJob rJob = cluster.getJTClient().getClient().
+        submitJob(jconf);
+
+    JobInfo jInfo = remoteJTClient.getJobInfo(rJob.getID());
+    LOG.info("jInfo is :" + jInfo);
+
+    count = 0;
+    while (count < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      }
+      count++;
+    }
+    Assert.assertTrue("Job has not been started for 1 min.", 
+      count != 60);
+
+    //Assert if jobInfo is null
+    Assert.assertNotNull("jobInfo is null", jInfo);
+
+    count = 0;
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(1000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will 
+      //anyway fail later.
+      if (count > 40) {
+        Assert.fail("job has not reached completed state for more" +
+          " than 400 seconds. Failing at this point");
+      }
+    }
+
+    //Waiting for 20 seconds to make sure that all the completed tasks 
+    //are reflected in their corresponding Tasktracker boxes. 
+    taskTrackerHeartBeatInterval =  remoteJTClient.
+        getTaskTrackerHeartbeatInterval();
+
+    //Waiting for 6 times the Task tracker heart beat interval to
+    //account for network slowness, job tracker processing time
+    //after receiving the tasktracker updates etc.
+    UtilsForTests.waitFor(taskTrackerHeartBeatInterval * 6);
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler =
+      remoteJTClient.getInfoFromAllClientsForAllTaskType();
+    int totalTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    int totalTasksForJob = (totalMapTasks + totalReduceTasks + 2); 
+
+    Assert.assertEquals("The number of total tasks, since start" +
+         " dont match", 
+        (totalTasksSinceStartBeforeJob + totalTasksForJob), 
+          totalTasksSinceStartAfterJob );
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksSinceStartBeforeJob + totalTasksForJob), 
+        succeededTasksSinceStartAfterJob); 
+
+    Assert.assertEquals("The number of total tasks, last hour" +
+        " dont match", 
+        (totalTasksLastHourBeforeJob + totalTasksForJob), 
+        totalTasksLastHourAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "last hour dont match", 
+        (succeededTasksLastHourBeforeJob + totalTasksForJob),  
+        succeededTasksLastHourAfterJob); 
+
+    Assert.assertEquals("The number of total tasks, last day" +
+        " dont match", 
+        (totalTasksLastDayBeforeJob + totalTasksForJob), 
+        totalTasksLastDayAfterJob);
+    Assert.assertEquals("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksLastDayBeforeJob + totalTasksForJob), 
+        succeededTasksLastDayAfterJob); 
+  }
+
+  @Test
+  /**
+   * This tests Task tracker info when tasktracker is suspended/killed 
+   * and then the process comes alive again 
+   * summary information for
+   * since start - total tasks, successful tasks
+   * Last Hour - total tasks, successful tasks
+   * Last Day - total tasks, successful tasks
+   * It is checked for multiple job submissions. 
+   * @param none
+   * @return void
+   */
+  public void testTaskTrackerInfoTaskTrackerSuspend() throws Exception {
+    
+    TaskInfo taskInfo = null;
+
+    String jobTrackerUserName = remoteJTClient.getDaemonUser();
+
+    LOG.info("jobTrackerUserName is :" + jobTrackerUserName);
+
+    int count = 0;
+
+    SleepJob job = new SleepJob();
+    job.setConf(conf);
+    int totalMapTasks = 5;
+    int totalReduceTasks = 1;
+    conf = job.setupJobConf(totalMapTasks, totalReduceTasks, 
+        100, 100, 100, 100);
+    JobConf jconf = new JobConf(conf);
+
+    //The last hour and last day are given 60 seconds and 120 seconds
+    //recreate values rate, replacing one hour and 1 day. Waiting for 
+    //them to be ona  just created stage when testacse starts.
+    while (remoteJTClient.getInfoFromAllClients("last_day","total_tasks") 
+        != 0) {
+      count++;
+      UtilsForTests.waitFor(1000);
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will
+      //anyway fail later.
+      if (count > 140) {
+        Assert.fail("Since this value has not reached 0" +
+          "in more than 140 seconds. Failing at this point");
+      }
+    }
+
+    statisticsCollectionHandler = remoteJTClient.
+        getInfoFromAllClientsForAllTaskType();
+
+    int totalTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartBeforeJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourBeforeJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayBeforeJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    //Submitting the job
+    RunningJob rJob = cluster.getJTClient().getClient().
+        submitJob(jconf);
+
+    JobInfo jInfo = remoteJTClient.getJobInfo(rJob.getID());
+    LOG.info("jInfo is :" + jInfo);
+
+    count = 0;
+    while (count < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      }
+      count++;
+    }
+    Assert.assertTrue("Job has not been started for 1 min.", 
+      count != 60);
+
+    //Assert if jobInfo is null
+    Assert.assertNotNull("jobInfo is null", jInfo);
+
+    TaskInfo[] taskInfos = remoteJTClient.getTaskInfo(rJob.getID());
+    for (TaskInfo taskinfo : taskInfos) {
+      if (!taskinfo.isSetupOrCleanup()) {
+        taskInfo = taskinfo;
+        break;
+      }
+    }
+
+    TTClient ttClient = cluster.getTTClientInstance(taskInfo);
+    String pid = null;
+    ttClient.kill();
+    ttClient.waitForTTStop();
+    ttClient.start();
+    ttClient.waitForTTStart();
+
+    count = 0;
+    LOG.info("Waiting till the job is completed...");
+    while (!jInfo.getStatus().isJobComplete()) {
+      UtilsForTests.waitFor(1000);
+      count++;
+      jInfo = remoteJTClient.getJobInfo(rJob.getID());
+      //If the count goes beyond a point, then break; This is to avoid
+      //infinite loop under unforeseen circumstances. Testcase will 
+      //anyway fail later.
+      if (count > 40) {
+        Assert.fail("job has not reached completed state for more" +
+          " than 400 seconds. Failing at this point");
+      }
+    }
+
+    //Waiting for 20 seconds to make sure that all the completed tasks
+    //are reflected in their corresponding Tasktracker boxes.
+    taskTrackerHeartBeatInterval =  remoteJTClient.
+        getTaskTrackerHeartbeatInterval();
+
+    //Waiting for 6 times the Task tracker heart beat interval to
+    //account for network slowness, job tracker processing time
+    //after receiving the tasktracker updates etc.
+    UtilsForTests.waitFor(taskTrackerHeartBeatInterval * 6);
+
+    statisticsCollectionHandler = null;
+    statisticsCollectionHandler =
+      remoteJTClient.getInfoFromAllClientsForAllTaskType();
+    int totalTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartTotalTasks();
+    int succeededTasksSinceStartAfterJob = statisticsCollectionHandler.
+        getSinceStartSucceededTasks();
+    int totalTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourTotalTasks();
+    int succeededTasksLastHourAfterJob = statisticsCollectionHandler.
+        getLastHourSucceededTasks();
+    int totalTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDayTotalTasks();
+    int succeededTasksLastDayAfterJob = statisticsCollectionHandler.
+        getLastDaySucceededTasks();
+
+    int totalTasksForJob = (totalMapTasks + totalReduceTasks + 2); 
+
+    Assert.assertTrue("The number of total tasks, since start" +
+         " dont match",  
+        (totalTasksSinceStartAfterJob >= succeededTasksSinceStartBeforeJob 
+        + totalTasksForJob));
+
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "since start dont match",  
+        (succeededTasksSinceStartAfterJob >= succeededTasksSinceStartBeforeJob
+        + totalTasksForJob)); 
+      
+    Assert.assertTrue("The number of total tasks, last hour" +
+        " dont match", 
+        ( totalTasksLastHourAfterJob >= totalTasksLastHourBeforeJob + 
+        totalTasksForJob)); 
+
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "last hour dont match", 
+        (succeededTasksLastHourAfterJob >= succeededTasksLastHourBeforeJob + 
+        totalTasksForJob));  
+
+    Assert.assertTrue("The number of total tasks, last day" +
+        " dont match", 
+        (totalTasksLastDayAfterJob >= totalTasksLastDayBeforeJob + 
+        totalTasksForJob));
+
+    Assert.assertTrue("The number of succeeded tasks, " +
+        "since start dont match", 
+        (succeededTasksLastDayAfterJob >= succeededTasksLastDayBeforeJob + 
+        totalTasksForJob));
+   }
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTClient.java b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTClient.java
index d263d37..ae3cd58 100644
--- a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTClient.java
+++ b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTClient.java
@@ -18,10 +18,18 @@
 
 package org.apache.hadoop.mapreduce.test.system;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.StringTokenizer;
 
 import junit.framework.Assert;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
@@ -30,13 +38,10 @@ import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobStatus;
 import org.apache.hadoop.mapred.JobTracker;
 import org.apache.hadoop.mapred.RunningJob;
+import org.apache.hadoop.mapred.TaskStatus;
+import org.apache.hadoop.mapred.UtilsForTests;
 import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.test.system.process.RemoteProcess;
-import org.apache.hadoop.mapred.TaskStatus;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.mapreduce.test.system.TaskInfo;
-import static org.junit.Assert.*;
 
 /**
  * JobTracker client for system tests.
@@ -164,17 +169,6 @@ public class JTClient extends MRDaemonClient<JTProtocol> {
           org.apache.hadoop.mapred.JobID.downgrade(id));
     }
     verifyJobDetails(id);
-    JobInfo jobInfo = getJobInfo(id);
-    if(jobInfo != null) {
-      while(!jobInfo.isHistoryFileCopied()) {
-        Thread.sleep(1000);
-        LOG.info(id+" waiting for history file to copied");
-        jobInfo = getJobInfo(id);
-        if(jobInfo == null) {
-          break;
-        }
-      }
-    }
     verifyJobHistory(id);
   }
 
@@ -326,4 +320,97 @@ public class JTClient extends MRDaemonClient<JTProtocol> {
     }
     LOG.info("Verified the job history for the jobId : " + jobId);
   }
+
+  /**
+   * The method provides the information on the job has stopped or not
+   * @return indicates true if the job has stopped false otherwise.
+   * @param job id has the information of the running job.
+   * @throw IOException is thrown if the job info cannot be fetched.   
+   */
+  public boolean isJobStopped(JobID id) throws IOException{
+    int counter = 0;
+    JobInfo jInfo = getProxy().getJobInfo(id);
+    if(jInfo != null ) {
+      while (counter < 60) {
+        if (jInfo.getStatus().isJobComplete()) {
+          break;
+        }
+        UtilsForTests.waitFor(1000);
+        jInfo = getProxy().getJobInfo(id);
+        counter ++;
+      }
+    }
+    return (counter != 60)? true : false;
+  }
+
+  /**
+   * It uses to check whether job is started or not.
+   * @param id job id
+   * @return true if job is running.
+   * @throws IOException if an I/O error occurs.
+   */
+  public boolean isJobStarted(JobID id) throws IOException {
+    JobInfo jInfo = getJobInfo(id);
+    int counter = 0;
+    while (counter < 60) {
+      if (jInfo.getStatus().getRunState() == JobStatus.RUNNING) {
+        break;
+      } else {
+        UtilsForTests.waitFor(1000);
+        jInfo = getJobInfo(jInfo.getID());
+        Assert.assertNotNull("Job information is null",jInfo);
+      }
+      counter++;
+    }
+    return (counter != 60)? true : false ;
+  }
+
+  /**
+   * It uses to check whether task is started or not.
+   * @param taskInfo task information
+   * @return true if task is running.
+   * @throws IOException if an I/O error occurs.
+   */
+  public boolean isTaskStarted(TaskInfo taskInfo) throws IOException { 
+    JTProtocol wovenClient = getProxy();
+    int counter = 0;
+    while (counter < 60) {
+      if (taskInfo.getTaskStatus().length > 0) {
+        if (taskInfo.getTaskStatus()[0].getRunState() == 
+            TaskStatus.State.RUNNING) {
+          break;
+        }
+      }
+      UtilsForTests.waitFor(1000);
+      taskInfo = wovenClient.getTaskInfo(taskInfo.getTaskID());
+      counter++;
+    }
+    return (counter != 60)? true : false;
+  }
+  /**
+   * Get the jobtracker log files as pattern.
+   * @return String - Jobtracker log file pattern.
+   * @throws IOException - if I/O error occurs.
+   */
+  public String getJobTrackerLogFilePattern() throws IOException  {
+    return getProxy().getFilePattern();
+  }
+
+  /**
+   * It uses to get the job summary details of given job id. .
+   * @param jobID - job id
+   * @return HashMap -the job summary details as map.
+   * @throws IOException if any I/O error occurs.
+   */
+  public HashMap<String,String> getJobSummary(JobID jobID)
+      throws IOException {
+    String output = getProxy().getJobSummaryInfo(jobID);
+    StringTokenizer strToken = new StringTokenizer(output,",");
+    HashMap<String,String> mapcollect = new HashMap<String,String>();
+    while(strToken.hasMoreTokens()) {
+      String keypair = strToken.nextToken();
+      mapcollect.put(keypair.split("=")[0], keypair.split("=")[1]);
+    }
+    return mapcollect;
+  }
 }
diff --git a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTProtocol.java b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTProtocol.java
index 4e0f3c8..5314dd4 100644
--- a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTProtocol.java
+++ b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JTProtocol.java
@@ -23,6 +23,8 @@ import java.io.IOException;
 import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.TaskID;
 import org.apache.hadoop.test.system.DaemonProtocol;
+import org.apache.hadoop.mapred.TaskTrackerStatus;
+import org.apache.hadoop.mapred.StatisticsCollectionHandler;
 
 /**
  * Client side API's exposed from JobTracker.
@@ -118,4 +120,88 @@ public interface JTProtocol extends DaemonProtocol {
    * @throws IOException
    */
   String getJobHistoryLocationForRetiredJob(JobID jobID) throws IOException;
+  
+  /**
+   * This directly calls the JobTracker public with no modifications
+   * @param trackerID uniquely indentifies the task tracker
+   * @return
+   * @throws IOException is thrown in case of RPC error
+   */
+  public boolean isBlackListed(String trackerID) throws IOException;
+
+  /**
+   * Get the job summary details from the jobtracker log files.
+   * @param jobID - job id
+   * @param filePattern - jobtracker log file pattern.
+   * @return String - the job summary details
+   * @throws IOException if any I/O error occurs.
+   */
+  public String getJobSummaryFromLog(JobID jobId, String filePattern)
+    throws IOException;
+
+  /**
+   * Get the job summary information of given job id.
+   * @param jobID - job id
+   * @return String - the job summary details as map.
+   * @throws IOException if any I/O error occurs.
+   */
+   public String getJobSummaryInfo(JobID jobId) throws IOException;
+
+
+   /**
+    * This gets the value of one task tracker window in the tasktracker page.
+    *
+    * @param TaskTrackerStatus,
+    * timePeriod and totalTasksOrSucceededTasks, which are requried to
+    * identify the window
+    * @return value of one task in a single Job tracker window
+    */
+   public int getTaskTrackerLevelStatistics(TaskTrackerStatus ttStatus,
+       String timePeriod, String totalTasksOrSucceededTasks)
+       throws IOException;
+
+   /**
+    * This gets the value of all task trackers windows in the tasktracker page.
+    *
+    * @param none,
+    * return a object which returns all the tasktracker info
+    */
+   public StatisticsCollectionHandler getInfoFromAllClientsForAllTaskType()
+     throws Exception;
+
+   /**
+    * Get Information for Time Period and TaskType box
+    * from all tasktrackers
+    * @param
+    * timePeriod and totalTasksOrSucceededTasks, which are requried to
+    * identify the window
+    * @return The total number of tasks info for a particular column in
+    * tasktracker page.
+    */
+   public int getInfoFromAllClients(String timePeriod,
+       String totalTasksOrSucceededTasks) throws IOException;
+
+   /**
+    * This gets the value of all task trackers windows in the tasktracker page.
+    */
+   public int getTaskTrackerHeartbeatInterval() throws Exception;
+   
+
+   
+   /**
+    * The method with access the history data in JobTracker, it will only do a 
+    * read on the data structure none is returned, this is used to verify a
+    * bug with simultaneously accessing history data
+    * @param jobId
+    * @throws Exception
+    */
+   public void accessHistoryData(JobID jobId) throws Exception;
+
+
+  /**
+   * Finds out if the given Task tracker client is decommissioned or not
+   */
+  public boolean isNodeDecommissioned(String ttClientHostName) 
+      throws IOException;
+
 }
diff --git a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JobInfo.java b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JobInfo.java
index b5f2f92..36d67f6 100644
--- a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JobInfo.java
+++ b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/JobInfo.java
@@ -130,10 +130,23 @@ public interface JobInfo extends Writable {
   List<String> getBlackListedTrackers();
   
   /**
-   * Gets if the history file of the job is copied to the done 
-   * location <br/>
-   * 
-   * @return true if history file copied.
+   * Get the launch time of a job.
+   * @return long - launch time for a job.
+   */
+  long getLaunchTime();
+  /**
+   * Get the finish time of a job
+   * @return long - finish time for a job
+   */
+  long getFinishTime();
+  /**
+   * Get the number of slots per map.
+   * @return int - number of slots per map.
+   */
+  int getNumSlotsPerMap();
+  /**
+   * Get the number of slots per reduce.
+   * @return int - number of slots per reduce.
    */
-  boolean isHistoryFileCopied();
-}
\ No newline at end of file
+  int getNumSlotsPerReduce();
+}
diff --git a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/MRCluster.java b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/MRCluster.java
index 34ddff5..954a955 100644
--- a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/MRCluster.java
+++ b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/MRCluster.java
@@ -34,6 +34,10 @@ import org.apache.hadoop.test.system.process.HadoopDaemonRemoteCluster;
 import org.apache.hadoop.test.system.process.MultiUserHadoopDaemonRemoteCluster;
 import org.apache.hadoop.test.system.process.RemoteProcess;
 import org.apache.hadoop.test.system.process.HadoopDaemonRemoteCluster.HadoopDaemonInfo;
+import org.apache.hadoop.mapred.JobID;
+import org.apache.hadoop.mapred.TaskID;
+import java.util.Collection;
+import org.apache.hadoop.mapred.UtilsForTests;
 
 /**
  * Concrete AbstractDaemonCluster representing a Map-Reduce cluster.
@@ -57,7 +61,7 @@ public class MRCluster extends AbstractDaemonCluster {
   private static String TT_hostFileName;
   private static String jtHostName;
 
-  protected enum Role {JT, TT};
+  public enum Role {JT, TT};
 
   static{
     Configuration.addDefaultResource("mapred-default.xml");
@@ -131,6 +135,19 @@ public class MRCluster extends AbstractDaemonCluster {
     }
     return null;
   }
+  
+  /**
+   * This function will give access to one of many TTClient present
+   * @return an Instance of TTclient 
+   */
+  public TTClient getTTClient() {
+    for (TTClient c: getTTClients()) {
+      if (c != null){
+        return c;
+      }
+    }
+    return null;
+  }
 
   @Override
   public void ensureClean() throws IOException {
@@ -143,6 +160,26 @@ public class MRCluster extends AbstractDaemonCluster {
           org.apache.hadoop.mapred.JobID.downgrade(job.getID()));
     }
   }
+  /**
+    * Allow the job to continue through MR control job.
+    * @param id of the job. 
+    * @throws IOException when failed to get task info. 
+    */
+  public void signalAllTasks(JobID id) throws IOException{
+    TaskInfo[] taskInfos = getJTClient().getProxy().getTaskInfo(id);
+    if(taskInfos !=null) {
+      for (TaskInfo taskInfoRemaining : taskInfos) {
+        if(taskInfoRemaining != null) {
+          FinishTaskControlAction action = new FinishTaskControlAction(TaskID
+              .downgrade(taskInfoRemaining.getTaskID()));
+          Collection<TTClient> tts = getTTClients();
+          for (TTClient cli : tts) {
+            cli.getProxy().sendAction(action);
+          }
+        }
+      }  
+    }
+  }
 
   @Override
   protected AbstractDaemonClient createClient(
@@ -167,4 +204,34 @@ public class MRCluster extends AbstractDaemonCluster {
       super(mrDaemonInfos);
     }
   }
+
+  /**
+   * Get a TTClient Instance from a running task <br/>
+   * @param Task Information of the running task
+   * @return TTClient instance
+   * @throws IOException
+   */
+  public TTClient getTTClientInstance(TaskInfo taskInfo)
+      throws IOException {
+    JTProtocol remoteJTClient = getJTClient().getProxy();
+    String [] taskTrackers = taskInfo.getTaskTrackers();
+    int counter = 0;
+    TTClient ttClient = null;
+    while (counter < 60) {
+      if (taskTrackers.length != 0) {
+        break;
+      }
+      UtilsForTests.waitFor(100);
+      taskInfo = remoteJTClient.getTaskInfo(taskInfo.getTaskID());
+      taskTrackers = taskInfo.getTaskTrackers();
+      counter ++;
+    }
+    if ( taskTrackers.length != 0 ) {
+      String hostName = taskTrackers[0].split("_")[1];
+      hostName = hostName.split(":")[0];
+      ttClient = getTTClient(hostName);
+    }
+    return ttClient;
+  }
+
 }
diff --git a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/TTClient.java b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/TTClient.java
index aa71363..7d06197 100644
--- a/src/test/system/java/org/apache/hadoop/mapreduce/test/system/TTClient.java
+++ b/src/test/system/java/org/apache/hadoop/mapreduce/test/system/TTClient.java
@@ -27,6 +27,11 @@ import org.apache.hadoop.mapred.JobTracker;
 import org.apache.hadoop.mapred.TaskTrackerStatus;
 import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.test.system.process.RemoteProcess;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import org.apache.hadoop.mapred.TaskID;
+import org.apache.hadoop.mapred.TaskStatus;
+import org.apache.hadoop.mapred.UtilsForTests;
 
 /**
  * TaskTracker client for system tests. Assumption of the class is that the
@@ -37,6 +42,7 @@ import org.apache.hadoop.test.system.process.RemoteProcess;
 public class TTClient extends MRDaemonClient<TTProtocol> {
 
   TTProtocol proxy;
+  static final Log LOG = LogFactory.getLog(TTClient.class);
 
   public TTClient(Configuration conf, RemoteProcess daemon) 
       throws IOException {
@@ -86,5 +92,70 @@ public class TTClient extends MRDaemonClient<TTProtocol> {
   public TaskTrackerStatus getStatus() throws IOException {
     return getProxy().getStatus();
   }
+  
+  /**
+   * This methods provides the information on the particular task managed
+   * by a task tracker has stopped or not. 
+   * @param TaskID is id of the task to get the status.
+   * @throws IOException if there is an error. 
+   * @return true is stopped. 
+   */
+  public boolean isTaskStopped(TaskID tID) throws IOException {
+    int counter = 0;
+    if(tID != null && proxy.getTask(tID) != null) {
+      TaskStatus.State tState= proxy.getTask(tID).getTaskStatus().getRunState();
+      while ( counter < 60) {
+        if(tState != TaskStatus.State.RUNNING && 
+            tState != TaskStatus.State.UNASSIGNED) {
+          break;
+        }
+        UtilsForTests.waitFor(1000);
+        tState= proxy.getTask(tID).getTaskStatus().getRunState();
+        counter++;
+      }      
+    }
+    return (counter != 60)? true : false;
+  }
 
+  /**
+   * Waits till this Tasktracker daemon process is stopped <br/>
+   *
+   * @return void
+   * @throws IOException
+   */
+  public void waitForTTStop() throws IOException {
+    LOG.info("Waiting for Tasktracker:" + getHostName()
+        + " to stop.....");
+    while (true) {
+      try {
+        ping();
+        LOG.debug(getHostName() +" is waiting state to stop.");
+        UtilsForTests.waitFor(10000);
+      } catch (Exception exp) {
+        LOG.info("TaskTracker : " + getHostName() + " is stopped...");
+        break;
+      }
+    }
+  }
+
+  /**
+   * Waits till this Tasktracker daemon process is started <br/>
+   *
+   * @return void
+   * @throws IOException
+   */
+  public void waitForTTStart() throws
+     IOException {
+    LOG.debug("Waiting for Tasktracker:" + getHostName() + " to come up.");
+    while (true) {
+      try {
+        ping();
+        LOG.debug("TaskTracker : " + getHostName() + " is pinging...");
+        break;
+      } catch (Exception exp) {
+        LOG.info(getHostName() + " is waiting to come up.");
+        UtilsForTests.waitFor(10000);
+      }
+    }
+  }
 }
diff --git a/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonClient.java b/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonClient.java
index 7706a6f..6b20eb4 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonClient.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonClient.java
@@ -27,6 +27,8 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.test.system.process.RemoteProcess;
 /**
  * Abstract class which encapsulates the DaemonClient which is used in the 
@@ -47,7 +49,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * @param conf client to be used by proxy to connect to Daemon.
    * @param process the Daemon process to manage the particular daemon.
    * 
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public AbstractDaemonClient(Configuration conf, RemoteProcess process) 
       throws IOException {
@@ -71,7 +73,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
   /**
    * Create an RPC proxy to the daemon <br/>
    * 
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public abstract void connect() throws IOException;
 
@@ -110,7 +112,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * Gets if the Daemon is ready to accept RPC connections. <br/>
    * 
    * @return true if daemon is ready.
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public boolean isReady() throws IOException {
     return getProxy().isReady();
@@ -118,7 +120,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
 
   /**
    * Kills the Daemon process <br/>
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public void kill() throws IOException {
     process.kill();
@@ -126,8 +128,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
 
   /**
    * Checks if the Daemon process is alive or not <br/>
-   * 
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public void ping() throws IOException {
     getProxy().ping();
@@ -135,7 +136,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
 
   /**
    * Start up the Daemon process. <br/>
-   * @throws IOException
+   * @throws IOException on RPC error
    */
   public void start() throws IOException {
     process.start();
@@ -146,7 +147,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * 
    * @return returns system level view of the Daemon process.
    * 
-   * @throws IOException
+   * @throws IOException on RPC error. 
    */
   public ProcessInfo getProcessInfo() throws IOException {
     return getProxy().getProcessInfo();
@@ -167,6 +168,56 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
   }
 
   /**
+   * Create a file with full permissions in a file system.
+   * @param path - source path where the file has to create.
+   * @param fileName - file name
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void createFile(String path, String fileName, 
+      boolean local) throws IOException {
+    getProxy().createFile(path, fileName, null, local);
+  }
+
+  /**
+   * Create a file with given permissions in a file system.
+   * @param path - source path where the file has to create.
+   * @param fileName - file name.
+   * @param permission - file permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void createFile(String path, String fileName, 
+     FsPermission permission,  boolean local) throws IOException {
+    getProxy().createFile(path, fileName, permission, local);
+  }
+
+  /**
+   * Create a folder with default permissions in a file system.
+   * @param path - source path where the file has to be creating.
+   * @param folderName - folder name.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs. 
+   */
+  public void createFolder(String path, String folderName, 
+     boolean local) throws IOException {
+    getProxy().createFolder(path, folderName, null, local);
+  }
+
+  /**
+   * Create a folder with given permissions in a file system.
+   * @param path - source path where the file has to be creating.
+   * @param folderName - folder name.
+   * @param permission - folder permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void createFolder(String path, String folderName, 
+     FsPermission permission,  boolean local) throws IOException {
+    getProxy().createFolder(path, folderName, permission, local);
+  }
+
+  /**
    * List the statuses of the files/directories in the given path if the path is
    * a directory.
    * 
@@ -175,7 +226,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * @param local
    *          whether the path is local or not
    * @return the statuses of the files/directories in the given patch
-   * @throws IOException
+   * @throws IOException on RPC error. 
    */
   public FileStatus[] listStatus(String path, boolean local) 
     throws IOException {
@@ -193,7 +244,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * @param recursive 
    *          whether to recursively get the status
    * @return the statuses of the files/directories in the given patch
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   public FileStatus[] listStatus(String f, boolean local, boolean recursive) 
     throws IOException {
@@ -239,7 +290,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * Pattern used for searching is ERROR. <br/>
    * @param excludeExpList list of exception to exclude 
    * @return number of occurrence of error message.
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   public int getNumberOfErrorStatementsInLog(String[] excludeExpList) 
       throws IOException {
@@ -254,7 +305,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * Pattern used for searching is WARN. <br/>
    * @param excludeExpList list of exception to exclude 
    * @return number of occurrence of warning message.
-   * @throws IOException
+   * @throws IOException thrown on RPC error. 
    */
   public int getNumberOfWarnStatementsInLog(String[] excludeExpList) 
       throws IOException {
@@ -269,7 +320,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * @param e exception class.
    * @param excludeExpList list of exceptions to exclude. 
    * @return number of exceptions in log
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   public int getNumberOfExceptionsInLog(Exception e,
       String[] excludeExpList) throws IOException {
@@ -283,7 +334,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * <br/>
    * @param excludeExpList list of exceptions to exclude.
    * @return number of times exception in log file.
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   public int getNumberOfConcurrentModificationExceptionsInLog(
       String[] excludeExpList) throws IOException {
@@ -299,7 +350,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * Populate the initial exception counts to be used to assert once a testcase
    * is done there was no exception in the daemon when testcase was run.
    * @param excludeExpList list of exceptions to exclude
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   protected void populateExceptionCount(String [] excludeExpList) 
       throws IOException {
@@ -320,7 +371,7 @@ public abstract class AbstractDaemonClient<PROXY extends DaemonProtocol> {
    * Pre-req for the method is that populateExceptionCount() has 
    * to be called before calling this method.</b></i>
    * @param excludeExpList list of exceptions to exclude
-   * @throws IOException
+   * @throws IOException is thrown on RPC error. 
    */
   protected void assertNoExceptionsOccurred(String [] excludeExpList) 
       throws IOException {
diff --git a/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonCluster.java b/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonCluster.java
index e4277bd..7fc10d2 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonCluster.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/AbstractDaemonCluster.java
@@ -19,10 +19,16 @@
 package org.apache.hadoop.test.system;
 
 import java.io.IOException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.FileInputStream;
+import java.io.DataInputStream;
 import java.util.ArrayList;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Enumeration;
+import java.util.Hashtable;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -41,6 +47,13 @@ public abstract class AbstractDaemonCluster {
   protected ClusterProcessManager clusterManager;
   private Map<Enum<?>, List<AbstractDaemonClient>> daemons = 
     new LinkedHashMap<Enum<?>, List<AbstractDaemonClient>>();
+  private String newConfDir = null;  
+  File localFolderObj = null;
+  private static final  String CONF_HADOOP_LOCAL_DIR =
+      "test.system.hdrc.hadoop.local.confdir"; 
+  private static final  String CONF_HADOOP_MULTI_USER_LIST =
+      "test.system.hdrc.multi-user.list.path";
+  private final static Object waitLock = new Object();
   
   /**
    * Constructor to create a cluster client.<br/>
@@ -84,7 +97,7 @@ public abstract class AbstractDaemonCluster {
   /**
    * Method to create the daemon client.<br/>
    * 
-   * @param remoteprocess
+   * @param process
    *          to manage the daemon.
    * @return instance of the daemon client
    * 
@@ -232,8 +245,7 @@ public abstract class AbstractDaemonCluster {
    * that will be excluded.
    * @param excludeExpList list of exceptions to exclude
    */
-  public void setExcludeExpList(String [] excludeExpList)
-  {
+  public void setExcludeExpList(String [] excludeExpList) {
     this.excludeExpList = excludeExpList;
   }
   
@@ -289,5 +301,302 @@ public abstract class AbstractDaemonCluster {
       }
     }
   }
+
+  /**
+   * Get the multi users list.
+   * @return ArrayList - users list as a array list.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public ArrayList<String> getHadoopMultiUsersList() throws
+     IOException {
+    String hadoopUserListPath = conf.get(CONF_HADOOP_MULTI_USER_LIST);
+    if (hadoopUserListPath == null || hadoopUserListPath.isEmpty()) {
+      LOG.error("Proxy user list path has not been passed for "
+          + CONF_HADOOP_MULTI_USER_LIST);
+      throw new IllegalArgumentException(
+          "Proxy user list hasn't been provided.");
+    }
+    File fileObj = new File(hadoopUserListPath);
+    DataInputStream disObj = new DataInputStream(new FileInputStream(fileObj));
+    ArrayList<String> usersList = new ArrayList<String>();
+    String strLine = null;
+    while((strLine = disObj.readLine()) != null){
+      usersList.add(strLine.substring(0,strLine.indexOf(',')));
+    }
+    return usersList;
+  }
+  
+  /**
+   * Get the proxy userlist path.
+   * @return String - userlist path
+   */
+  public String getProxyUsersFilePath() {
+    return null;
+  }
+
+  /**
+   * It's a local folder where the config file stores temporarily
+   * while serializing the object.
+   * @return String temporary local folder path for configuration.
+   */
+  private String getHadoopLocalConfDir() {
+    String hadoopLocalConfDir = conf.get(CONF_HADOOP_LOCAL_DIR);
+    if (hadoopLocalConfDir == null || hadoopLocalConfDir.isEmpty()) {
+      LOG.error("No configuration "
+          + "for the CONF_HADOOP_LOCAL_DIR passed");
+      throw new IllegalArgumentException(
+          "No Configuration passed for hadoop conf local directory");
+    }
+    return hadoopLocalConfDir;
+  }
+
+  /**
+   * It uses to restart the cluster with new configuration at runtime.<br/>
+   * @param props attributes for new configuration.
+   * @param configFile configuration file.
+   * @throws IOException if an I/O error occurs.
+   */
+  public void restartClusterWithNewConfig(Hashtable<String,?> props, 
+      String configFile) throws IOException {
+    String mapredConf = null;
+    String localDirPath = null; 
+    Configuration initConf = new Configuration(getConf());
+    Enumeration<String> e = props.keys();
+    while (e.hasMoreElements()) {
+      String propKey = e.nextElement();
+      Object propValue = props.get(propKey);
+      initConf.set(propKey,propValue.toString());
+    }
+    localDirPath = getHadoopLocalConfDir(); 
+    writeConfToFile(configFile,localDirPath,initConf);
+    newConfDir = clusterManager.pushConfig(localDirPath);
+    stop();
+    waitForClusterToStop();
+    clusterManager.start(newConfDir);
+    waitForClusterToStart();
+    localFolderObj.delete();
+  }
+  
+  /**
+   * It uses to delete the new configuration folder.
+   * @param path - configuration directory path.
+   * @throws IOException if an I/O error occurs.
+   */
+  public void cleanupNewConf(String path) throws IOException {
+    File file = new File(path);
+    file.delete();
+  }
+  
+  /**
+   * The method is used to only restart one daemon with new config
+   * and this gives a great flexibility in choosing which daemon the
+   * test wants to restarting instead of changing the config in all the 
+   * daemons. 
+   * @param client points to the daemon that will restarted.
+   * @param configFile the name of the config file
+   * @param conf the Configuration object
+   * @param role of remote process such as JT,TT,NN,DN
+   * @throws IOException thrown in case of any error 
+   */
+  public void restartDaemonWithNewConfig(AbstractDaemonClient client, 
+      String configFile, Configuration conf, Enum<?> role) throws IOException {    
+    String localDirPath = getHadoopLocalConfDir();
+    writeConfToFile(configFile,localDirPath,conf);
+    RemoteProcess daemon=clusterManager.getDaemonProcess(client.getHostName(), 
+        role);    
+    newConfDir = daemon.pushConfig(localDirPath);
+    daemon.kill();
+    waitForDaemonToStop(client);
+    daemon.start(newConfDir);
+    waitForDaemonToStart(client);    
+    localFolderObj.delete();
+  }
+  
+  private void writeConfToFile(String configFile, String localDirPath,
+      Configuration conf) throws IOException{      
+    localFolderObj = new File(localDirPath);
+    if (!localFolderObj.exists()) {
+      localFolderObj.mkdir();
+    }
+    String confXMLFile = localDirPath + File.separator + configFile;
+    File xmlFileObj = new File(confXMLFile);
+    conf.writeXml(new FileOutputStream(xmlFileObj));
+  }
+  
+  /**
+   * It uses to restart the cluster with default configuration.<br/>
+   * @throws IOException if an I/O error occurs.
+   */
+  public void restart() throws 
+      IOException {
+    stop();
+    waitForClusterToStop();
+    start();
+    waitForClusterToStart();
+    cleanupNewConf(newConfDir);
+  }
+  
+  /**
+   * Restart only one daemon as opposed to all the daemons
+   * @param client points to the daemon that will restarted. 
+   * @param role  of remote process such as JT,TT,NN,DN 
+   * @throws IOException is thrown when restart fails
+   */
+  public void restart(AbstractDaemonClient client,Enum<?> role) throws IOException {
+    RemoteProcess daemon=clusterManager.getDaemonProcess(client.getHostName(), 
+        role);
+    daemon.kill();
+    waitForDaemonToStop(client);
+    daemon.start();
+    waitForDaemonToStart(client);
+  }
+  
+  /**
+   * It uses to wait until the cluster is stopped.<br/>
+   * @throws IOException if an I/O error occurs.
+   */
+  public void waitForClusterToStop() throws 
+      IOException {
+    List<Thread> chkDaemonStop = new ArrayList<Thread>();
+    for (List<AbstractDaemonClient> set : daemons.values()) {	  
+      for (AbstractDaemonClient daemon : set) {
+        DaemonStopThread dmStop = new DaemonStopThread(daemon);
+        chkDaemonStop.add(dmStop);
+        dmStop.start();
+      }
+    }
+    
+    for (Thread daemonThread : chkDaemonStop){
+      try {
+        daemonThread.join();
+      } catch(InterruptedException intExp) {
+         LOG.warn("Interrupted while thread is joining." + intExp.getMessage());
+      }
+    }
+  }
+ 
+  /**
+   * It uses to wait until the cluster is started.<br/>
+   * @throws IOException if an I/O error occurs.
+   */
+  public void  waitForClusterToStart() throws 
+      IOException {
+    List<Thread> chkDaemonStart = new ArrayList<Thread>();
+    for (List<AbstractDaemonClient> set : daemons.values()) {
+      for (AbstractDaemonClient daemon : set) {
+        DaemonStartThread dmStart = new DaemonStartThread(daemon);
+        chkDaemonStart.add(dmStart);;
+        dmStart.start();
+      }
+    }
+
+    for (Thread daemonThread : chkDaemonStart){
+      try {
+        daemonThread.join();
+      } catch(InterruptedException intExp) {
+        LOG.warn("Interrupted while thread is joining" + intExp.getMessage());
+      }
+    }
+  }
+  /**
+   * This will provide synchronization for the daemon to stop
+   * @param client identifies the daemon
+   * @throws IOException thrown if daemon does not stop 
+   */
+  public void waitForDaemonToStop(AbstractDaemonClient client) 
+      throws IOException {
+    while (true) {
+      try {
+        client.ping();
+        LOG.debug(client.getHostName() +" is waiting state to stop.");
+        waitFor(5000);
+      } catch (Exception exp) {
+        LOG.info("Daemon is : " + client.getHostName() + " stopped...");
+        break;
+      } 
+    }   
+  }
+  
+  /**
+   * This will provide synchronization for the daemon to start
+   * @param client identifies the daemon
+   * @throws IOException thrown if the daemon does not start
+   */
+  public void waitForDaemonToStart(AbstractDaemonClient client) 
+      throws IOException {
+    
+    while (true) {
+      try {
+        client.ping();
+        LOG.info("Daemon is : " + client.getHostName() + " pinging...");
+        break;
+      } catch (Exception exp) {
+        LOG.debug(client.getHostName() + " is waiting to come up.");
+        waitFor(5000);
+      }
+    }    
+  }
+
+  /**
+   * It waits for specified amount of time.
+   * @param duration time in milliseconds.
+   * @throws InterruptedException if any thread interrupted the current
+   * thread while it is waiting for a notification.
+   */
+  public void waitFor(long duration) {
+    try {
+      synchronized (waitLock) {
+        waitLock.wait(duration);
+      }
+    } catch (InterruptedException intExp) {
+       LOG.warn("Interrrupeted while thread is waiting" + intExp.getMessage());
+    }
+  }
+  
+  class DaemonStartThread extends Thread {
+    private AbstractDaemonClient daemon;
+
+    public DaemonStartThread(AbstractDaemonClient daemon) {
+      this.daemon = daemon;
+    }
+
+    public void run(){
+      LOG.info("Waiting for Daemon " + daemon.getHostName() 
+          + " to come up.....");
+      while (true) { 
+        try {
+          daemon.ping();
+          LOG.info("Daemon is : " + daemon.getHostName() + " pinging...");
+          break;
+        } catch (Exception exp) {
+          LOG.debug(daemon.getHostName() + " is waiting to come up.");
+          waitFor(60000);
+        }
+      }
+    }
+  }
+  
+  class DaemonStopThread extends Thread {
+    private AbstractDaemonClient daemon;
+
+    public DaemonStopThread(AbstractDaemonClient daemon) {
+      this.daemon = daemon;
+    }
+
+    public void run() {
+      LOG.info("Waiting for Daemon " + daemon.getHostName() 
+          + " to stop.....");
+      while (true) {
+        try {
+          daemon.ping();
+          LOG.debug(daemon.getHostName() +" is waiting state to stop.");
+          waitFor(60000);
+        } catch (Exception exp) {
+          LOG.info("Daemon is : " + daemon.getHostName() + " stopped...");
+          break;
+        } 
+      }
+    }
+  }
 }
 
diff --git a/src/test/system/java/org/apache/hadoop/test/system/DaemonProtocol.java b/src/test/system/java/org/apache/hadoop/test/system/DaemonProtocol.java
index a74292d..9e24fff 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/DaemonProtocol.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/DaemonProtocol.java
@@ -23,8 +23,10 @@ import java.io.IOException;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.ipc.VersionedProtocol;
+import org.apache.hadoop.fs.permission.FsPermission;
 
 /**
  * RPC interface of a given Daemon.
@@ -77,6 +79,28 @@ public interface DaemonProtocol extends VersionedProtocol{
   FileStatus getFileStatus(String path, boolean local) throws IOException;
 
   /**
+   * Create a file with given permissions in a file system.
+   * @param path - source path where the file has to create.
+   * @param fileName - file name.
+   * @param permission - file permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  void createFile(String path, String fileName, 
+      FsPermission permission, boolean local) throws IOException;
+   
+  /**
+   * Create a folder with given permissions in a file system.
+   * @param path - source path where the file has to be creating.
+   * @param folderName - folder name.
+   * @param permission - folder permissions.
+   * @param local - identifying the path whether its local or not.
+   * @throws IOException - if an I/O error occurs.
+   */
+  public void createFolder(String path, String folderName, 
+      FsPermission permission, boolean local) throws IOException;
+
+  /**
    * List the statuses of the files/directories in the given path if the path is
    * a directory.
    * 
@@ -147,12 +171,12 @@ public interface DaemonProtocol extends VersionedProtocol{
    * <b><i>Please note that search spans across all previous messages of
    * Daemon, so better practice is to get previous counts before an operation
    * and then re-check if the sequence of action has caused any problems</i></b>
-   * @param pattern to look for in the damon's log file
-   * @param List of exceptions to ignore
+   * @param pattern to look for in the daemon's log file
+   * @param list Exceptions that will be ignored from log file. 
    * @return number of times the pattern if found in log file.
    * @throws IOException in case of errors
    */
-  int getNumberOfMatchesInLogFile(String pattern,String[] list) 
+  int getNumberOfMatchesInLogFile(String pattern, String[] list) 
       throws IOException;
 
   /**
@@ -162,4 +186,21 @@ public interface DaemonProtocol extends VersionedProtocol{
    * @throws IOException in case of errors
    */
   String getDaemonUser() throws IOException;
+  
+  /**
+   * It uses for suspending the process.
+   * @param pid process id.
+   * @return true if the process is suspended otherwise false.
+   * @throws IOException if an I/O error occurs.
+   */
+  boolean suspendProcess(String pid) throws IOException;
+
+  /**
+   * It uses for resuming the suspended process.
+   * @param pid process id
+   * @return true if suspended process is resumed otherwise false.
+   * @throws IOException if an I/O error occurs.
+   */
+  boolean resumeProcess(String pid) throws IOException;
+
 }
diff --git a/src/test/system/java/org/apache/hadoop/test/system/process/ClusterProcessManager.java b/src/test/system/java/org/apache/hadoop/test/system/process/ClusterProcessManager.java
index 5f31dd2..231c618 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/process/ClusterProcessManager.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/process/ClusterProcessManager.java
@@ -46,6 +46,13 @@ public interface ClusterProcessManager {
    * Get the list of RemoteProcess handles of all the remote processes.
    */
   List<RemoteProcess> getAllProcesses();
+  
+  /**
+   * Get a RemoteProcess given the hostname
+   * @param The hostname for which the object instance needs to be obtained.
+   * @param The role of process example are JT,TT,DN,NN
+   */
+  RemoteProcess getDaemonProcess(String hostname, Enum<?> role);
 
   /**
    * Get all the roles this cluster's daemon processes have.
@@ -62,16 +69,15 @@ public interface ClusterProcessManager {
   /**
    * Starts the daemon from the user specified conf dir.
    * @param newConfLocation the dir where the new conf files reside.
-   * @throws IOException
+   * @throws IOException if start from new conf fails. 
    */
   void start(String newConfLocation) throws IOException;
 
   /**
    * Stops the daemon running from user specified conf dir.
    * 
-   * @param newConfLocation
-   *          the dir where ther new conf files reside.
-   * @throws IOException
+   * @param newConfLocation the dir where the new conf files reside.
+   * @throws IOException if stop from new conf fails. 
    */
   void stop(String newConfLocation) throws IOException;
 
@@ -86,7 +92,7 @@ public interface ClusterProcessManager {
    * Gets if multi-user support is enabled for this cluster. 
    * <br/>
    * @return true if multi-user support is enabled.
-   * @throws IOException
+   * @throws IOException if RPC returns error. 
    */
   boolean isMultiUserSupported() throws IOException;
 
@@ -94,7 +100,7 @@ public interface ClusterProcessManager {
    * The pushConfig is used to push a new config to the daemons.
    * @param localDir
    * @return is the remoteDir location where config will be pushed
-   * @throws IOException
+   * @throws IOException if pushConfig fails.
    */
   String pushConfig(String localDir) throws IOException;
 }
diff --git a/src/test/system/java/org/apache/hadoop/test/system/process/HadoopDaemonRemoteCluster.java b/src/test/system/java/org/apache/hadoop/test/system/process/HadoopDaemonRemoteCluster.java
index b6a5efc..21f83c2 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/process/HadoopDaemonRemoteCluster.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/process/HadoopDaemonRemoteCluster.java
@@ -141,6 +141,18 @@ public abstract class HadoopDaemonRemoteCluster
     }
     return hadoopNewConfDir;
   }
+  
+  @Override
+  public RemoteProcess getDaemonProcess(String hostname,Enum<?> role) {
+    RemoteProcess daemon=null;
+    for (RemoteProcess p : processes) {      
+      if(p.getHostName().equals(hostname) && p.getRole() == role){
+        daemon =p;
+        break;
+      }     
+    }
+    return daemon;
+  }
 
   public HadoopDaemonRemoteCluster(List<HadoopDaemonInfo> daemonInfos) {
     this.daemonInfos = daemonInfos;
@@ -324,9 +336,10 @@ public abstract class HadoopDaemonRemoteCluster
     }
 
     @Override
-    public void pushConfig(String localDir) throws IOException {
+    public String pushConfig(String localDir) throws IOException {
       createNewConfDir().execute();
       buildPushConfig(localDir, hadoopNewConfDir).execute();
+      return hadoopNewConfDir;
     }
 
     private ShellCommandExecutor buildCommandExecutor(String command,
diff --git a/src/test/system/java/org/apache/hadoop/test/system/process/RemoteProcess.java b/src/test/system/java/org/apache/hadoop/test/system/process/RemoteProcess.java
index b4e8477..401e518 100644
--- a/src/test/system/java/org/apache/hadoop/test/system/process/RemoteProcess.java
+++ b/src/test/system/java/org/apache/hadoop/test/system/process/RemoteProcess.java
@@ -41,7 +41,7 @@ public interface RemoteProcess {
   /**
    * Starts a daemon from user specified conf dir. 
    * @param newConfLocation is dir where new conf resides. 
-   * @throws IOException
+   * @throws IOException if start of process fails from new location.
    */
   void start(String newConfLocation) throws IOException;
   /**
@@ -54,7 +54,7 @@ public interface RemoteProcess {
   /**
    * Stops a given daemon running from user specified 
    * conf dir. </br>
-   * @throws IOException
+   * @throws IOException if kill fails from new conf location.
    * @param newconfLocation dir location where new conf resides. 
    */
    void kill(String newConfLocation) throws IOException;
@@ -67,8 +67,10 @@ public interface RemoteProcess {
   
   /**
    * Pushed the configuration to new configuration directory 
-   * @param localDir
-   * @throws IOException
+   * @param localDir The local directory which has config files that will be 
+   * pushed to the remote location
+   * @throws IOException is thrown if the pushConfig results in a error. 
+   * @return The newconfdir location will be returned
    */
-  void pushConfig(String localDir) throws IOException;
+  String pushConfig(String localDir) throws IOException;
 }
diff --git a/src/test/system/java/shared/org/apache/hadoop/common/RemoteExecution.java b/src/test/system/java/shared/org/apache/hadoop/common/RemoteExecution.java
new file mode 100644
index 0000000..dc9d95b
--- /dev/null
+++ b/src/test/system/java/shared/org/apache/hadoop/common/RemoteExecution.java
@@ -0,0 +1,97 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.common;
+
+import com.jcraft.jsch.*;
+import org.apache.commons.logging.LogFactory;
+import org.apache.commons.logging.Log;
+import java.io.InputStream;
+
+/**
+ * Remote Execution of commands or any other utility on a remote machine.
+ */
+
+public class RemoteExecution {
+
+  static final Log LOG = LogFactory.
+      getLog(RemoteExecution.class);
+
+  static String rsaOrDsaFileName = "id_dsa";
+
+  public RemoteExecution() throws Exception {
+  }
+
+  /**
+   * Execute command at remoteNode. 
+   * @param JotTracker Client HostName, username provided,
+   * command to be executed remotely.
+   * @return void
+   */
+  public static void executeCommand(String jtClientHostName, String user, 
+      String  command) throws Exception {
+
+    JSch jsch = new JSch();
+
+    Session session = jsch.getSession(user, jtClientHostName, 22);
+    jsch.setKnownHosts("/homes/" + user + "/.ssh/known_hosts");
+    jsch.addIdentity("/homes/" + user + "/.ssh/" + rsaOrDsaFileName);
+    java.util.Properties config = new java.util.Properties();
+    config.put("StrictHostKeyChecking", "no");
+    session.setConfig(config);
+
+    session.connect(30000);   // making a connection with timeout.
+
+    Channel channel=session.openChannel("exec");
+    ((ChannelExec)channel).setCommand(command);
+    channel.setInputStream(null);
+   
+    ((ChannelExec)channel).setErrStream(System.err);
+   
+    InputStream in = channel.getInputStream();
+    channel.connect();
+    byte[] tmp = new byte[1024];
+    while(true) {
+      while(in.available()>0){
+        int i=in.read(tmp, 0, 1024);
+        if(i<0)break;
+           System.out.print(new String(tmp, 0, i));
+           LOG.info(new String(tmp, 0, i));
+      }
+      if(channel.isClosed()){
+        System.out.println("exit-status: " + channel.getExitStatus());
+        break;
+      }
+      try{Thread.sleep(1000);}catch(Exception ee){ee.printStackTrace();}
+    }
+    channel.disconnect();
+    session.disconnect();
+  }
+
+  /**
+   * Execute command at remoteNode.
+   * @param JotTracker Client HostName, username provided,
+   * comamnd to be executed remotely, rsaOrDsaFileName to be found under .ssh.
+   * @return void
+   */
+  public static void executeCommand(String jtClientHostName, String user,
+      String  command, String rsaDsaFileName) throws Exception {
+      rsaOrDsaFileName = rsaDsaFileName;
+      executeCommand(jtClientHostName, user, command);
+  }
+}
diff --git a/src/test/system/java/shared/org/apache/hadoop/mapred/StatisticsCollectionHandler.java b/src/test/system/java/shared/org/apache/hadoop/mapred/StatisticsCollectionHandler.java
new file mode 100644
index 0000000..8012362
--- /dev/null
+++ b/src/test/system/java/shared/org/apache/hadoop/mapred/StatisticsCollectionHandler.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+/*
+ * Object which stores all the tasktracker info
+ */
+public class StatisticsCollectionHandler implements Writable{
+
+  private int sinceStartTotalTasks = 0;
+  private int sinceStartSucceededTasks = 0;
+  private int lastHourTotalTasks = 0;
+  private int lastHourSucceededTasks = 0;
+  private int lastDayTotalTasks = 0;
+  private int lastDaySucceededTasks = 0;
+
+  public int getSinceStartTotalTasks() {
+    return sinceStartTotalTasks;
+  }
+
+  public int getSinceStartSucceededTasks() {
+    return sinceStartSucceededTasks;
+  }
+
+  public int getLastHourTotalTasks() {
+    return lastHourTotalTasks;
+  }
+
+  public int getLastHourSucceededTasks() {
+    return lastHourSucceededTasks;
+  }
+
+  public int getLastDayTotalTasks() {
+    return lastDayTotalTasks;
+  }
+
+  public int getLastDaySucceededTasks() {
+    return lastDaySucceededTasks;
+  }
+
+  public void setSinceStartTotalTasks(int value) {
+    sinceStartTotalTasks = value;
+  }
+
+  public void setSinceStartSucceededTasks(int value) {
+    sinceStartSucceededTasks = value;
+  }
+
+  public void setLastHourTotalTasks(int value) {
+    lastHourTotalTasks = value;
+  }
+
+  public void setLastHourSucceededTasks(int value) {
+    lastHourSucceededTasks = value;
+  }
+
+  public void setLastDayTotalTasks(int value) {
+    lastDayTotalTasks = value;
+  }
+
+  public void setLastDaySucceededTasks(int value) {
+    lastDaySucceededTasks = value;
+  }
+  
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    sinceStartTotalTasks = WritableUtils.readVInt(in);
+    sinceStartSucceededTasks = WritableUtils.readVInt(in);
+    lastHourTotalTasks = WritableUtils.readVInt(in);
+    lastHourSucceededTasks = WritableUtils.readVInt(in);
+    lastDayTotalTasks = WritableUtils.readVInt(in);
+    lastDaySucceededTasks = WritableUtils.readVInt(in);
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    WritableUtils.writeVInt(out, sinceStartTotalTasks);
+    WritableUtils.writeVInt(out, sinceStartSucceededTasks);
+    WritableUtils.writeVInt(out, lastHourTotalTasks);
+    WritableUtils.writeVInt(out, lastHourSucceededTasks);
+    WritableUtils.writeVInt(out, lastDayTotalTasks);
+    WritableUtils.writeVInt(out, lastDaySucceededTasks);
+  }
+  
+}
+
diff --git a/src/test/system/scripts/healthScriptError b/src/test/system/scripts/healthScriptError
new file mode 100644
index 0000000..cd0f02b
--- /dev/null
+++ b/src/test/system/scripts/healthScriptError
@@ -0,0 +1,19 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#This will mark the task tracker unhealthy with the custom error message
+echo "ERROR Task Tracker status is fatal"
diff --git a/src/test/system/scripts/healthScriptTimeout b/src/test/system/scripts/healthScriptTimeout
new file mode 100644
index 0000000..d4a845c
--- /dev/null
+++ b/src/test/system/scripts/healthScriptTimeout
@@ -0,0 +1,19 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#This will let the script sleep for 2 seconds
+sleep 2
-- 
1.7.0.4

